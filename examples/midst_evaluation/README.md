# Running Quality and Privacy Evaluations on Synthetic Data

This example demonstrates running the full suite of evaluation metrics in the Evaluation module for synthetically
generated datasets, with the exception of the Membership Inference Attack Metrics, which require a different pipeline.
Provided paths to real data used to train the generating model, synthetic data generated by that model, and a holdout
set of real data that was not used to train the model, the configuration file allows for the selection and
configuration of the various quality and privacy metrics to run on the target datasets. Dataset preprocessing is
handled by the script before evaluating any data.

Note that this script is somewhat specific to datasets and structures used in the
[MIDST Models repository](https://github.com/VectorInstitute/MIDSTModels). That is, we assume the presence of a JSON
file containing meta-information about the datasets. For example, this file includes information about which columns
correspond to numerical and categorical variables. If running evaluations that require a target variable, such as
`MeanF1ScoreDifference`, this information should also be contained in that file.

Scores for each metric run will be logged and, depending on the configuration, written to a log file.

## Configuration

The provided configurations allow for flexible configuration of the evaluation pipeline, including hyper-parameters
associated with the evaluation metrics, which metrics to be applied, and where to write the results.

There are two exemplary configuration files corresponding to slightly different applications of the Evaluation module.
While the scripts are largely similar, there is one key difference that is important to highlight.

- In `config_berka.yaml`, `f1_score_diff` is set to False. This metric can only be run on binary or multi-class
target variables and this config considers datasets like
[Berka](https://webpages.charlotte.edu/mirsad/itcs6265/group1/domain.html), where the target variable corresponds to a
regression task. As such, the metric is turned off.

- In `config_adult`, `f1_score_diff` is set to True with a target column specified as "income." This configuration
can be used for the [Adult](https://archive.ics.uci.edu/dataset/2/adult) dataset, where the column labeled "income"
is a categorical target variable. As such, the `MeanF1ScoreDifference` metric can be applied.

## Preprocessing

Preprocessing can have a noticeable impact on the metrics. As such, it is important to handle such transformations with
care and to take into account the calculations being performed in the metrics themselves. For example, if computing
distance-based metrics with Euclidean norms, categorical variables should be one-hot encoded, rather than ordinally
encoded. The script handles proper preprocessing of the datasets before applying the various metrics in the pipeline.

In particular:

- When applying the `AlphaPrecision` metric, we follow the preprocessing procedure suggested in this [MIDST Models
evaluation script](https://github.com/VectorInstitute/MIDSTModels/blob/main/midst_models/single_table_TabDDPM/evaluate_synthetic_data.ipynb)
wherein categorical variables are one-hot encoded and numerical variables are left untouched.
- When applying the distance based metrics of `DistanceToClosestRecordScore`, `MedianDistanceToClosestRecordScore`,
and `NearestNeighborDistanceRatio`, we follow the preprocessing procedure suggested in this [MIDST models
evaluations script](https://github.com/VectorInstitute/MIDSTModels/blob/main/midst_models/single_table_TabDDPM/eval/eval_dcr.py).
Therein, categorical variables are one-hot encoded and numerical variables are normalized by their range such that
values are guaranteed to fall between [-1, 1].
- For all other metrics, which leverage the [SynthEval](https://github.com/schneiderkamplab/syntheval) library to
perform at least some portion of the computations, the standard SynthEval preprocessing pipeline is applied. This
process **ordinally encodes** categorical variables and min-max encodes numerical variables.

## Running the Script

Make sure to properly modify the configuration script to include the metrics you would like to run (and not run),
along with any hyper-parameter settings.

The `base_data_dir` and paths to the various dataset components under `data_paths` need to be modified to point to
the right files. If you would like to dump the metrics to a file, the `metric_report_path` also needs to correspond
to a valid path and file name.

Thereafter, from the top directory of the repository one runs

```bash
uv sync
source .venv/bin/activate
python -m examples.midst_evaluation.run_evaluation
```

To point to a configuration in a specific directory and file name, the last line would be changed to

```bash
python -m examples.midst_evaluation.run_evaluation --config-path=path/to/configs/ --config-name=config_berka
```
