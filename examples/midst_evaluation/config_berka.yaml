# Evaluation Example Configuration
# Base data directory (can be overridden from command line)
base_data_dir: base/data_dir/placeholder

# Data paths (relative to base_data_dir)
data_paths:
  meta_data_path: ${base_data_dir}/info.json # Information about the column types etc.
  synthetic_data_path: ${base_data_dir}/synthetic.csv # Synthetic data generated by a model
  real_train_data_path: ${base_data_dir}/train.csv  # Real data used to train the generating model
  real_test_data_path: ${base_data_dir}/test.csv # Real data NOT used to train the generating model

# Reporting Configuration
write_report: True
metric_report_path: report_path/placeholder.txt

# Privacy Evaluation Configurations
dcr:
  run: True
  norm: "l2"
  batch_size: 1000

median_dcr:
  run: True
  norm: "l2"
  batch_size: 1000

hitting_rate:
  run: True
  hitting_threshold: 0.03

eir:
  run: True
  norm: "gower"

nndr:
  run: True
  norm: "l2"
  batch_size: 1000

# Quality Evaluation Configurations
ks_tv:
  run: True
  significance_level: 0.05
  permutations: 1000

alpha_precision:
  run: False
  naive_only: False

ci_overlap:
  run: True
  confidence_level: 95

correlation_diff:
  run: True
  compute_mixed_correlations: True

mean_diff:
  run: True

f1_score_diff:
  run: False
  label_column: "balance"
  folds: 5
  f1_type: "macro"

hellinger:
  run: True
  include_numerical_columns: True

propensity_mse:
  run: True
  folds: 5
  max_iterations: 50
  solver: "liblinear"

mutual_information:
  run: True
  include_numerical_columns: False
