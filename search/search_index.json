{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MIDST Toolkit Repository","text":""},{"location":"#midst-toolkit","title":"MIDST Toolkit","text":"<p>A toolkit for facilitating MIA resiliency testing on diffusion-model-based synthetic tabular data. Many of the attacks included in this toolkit are based on the most success ones used in the 2025 SaTML MIDST Competition.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#top-level-module","title":"Top Level Module","text":""},{"location":"api/#midst_toolkit","title":"midst_toolkit","text":"<p>PLACEHOLDER.</p>"},{"location":"api/#midst_toolkit.attacks","title":"attacks","text":""},{"location":"api/#midst_toolkit.attacks.ensemble","title":"ensemble","text":""},{"location":"api/#midst_toolkit.attacks.ensemble.data_utils","title":"data_utils","text":""},{"location":"api/#midst_toolkit.attacks.ensemble.data_utils.save_dataframe","title":"save_dataframe","text":"<pre><code>save_dataframe(df, file_path, file_name)\n</code></pre> <p>Save a DataFrame to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to be saved.</p> required <code>file_path</code> <code>Path</code> <p>Path where the file will be saved.</p> required <code>file_name</code> <code>str</code> <p>Name of the file to save the DataFrame as.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/midst_toolkit/attacks/ensemble/data_utils.py</code> <pre><code>def save_dataframe(df: pd.DataFrame, file_path: Path, file_name: str) -&gt; None:\n    \"\"\"\n    Save a DataFrame to a CSV file.\n\n    Args:\n        df: DataFrame to be saved.\n        file_path: Path where the file will be saved.\n        file_name: Name of the file to save the DataFrame as.\n\n    Returns:\n        None\n    \"\"\"\n    assert Path.exists(file_path), f\"Path {file_path} does not exist.\"\n    if Path.exists(file_path / file_name):\n        log(\n            WARNING,\n            f\"File {file_path / file_name} already exists and will be overwritten.\",\n        )\n    df.to_csv(file_path / file_name, index=False)\n    log(INFO, f\"DataFrame saved to {file_path / file_name}\")\n</code></pre>"},{"location":"api/#midst_toolkit.attacks.ensemble.data_utils.load_dataframe","title":"load_dataframe","text":"<pre><code>load_dataframe(file_path, file_name)\n</code></pre> <p>Load a DataFrame from a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path where the file is stored.</p> required <code>file_name</code> <code>str</code> <p>Name of the file to load the DataFrame from.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Loaded dataframe.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> Source code in <code>src/midst_toolkit/attacks/ensemble/data_utils.py</code> <pre><code>def load_dataframe(file_path: Path, file_name: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Load a DataFrame from a CSV file.\n\n    Args:\n        file_path: Path where the file is stored.\n        file_name: Name of the file to load the DataFrame from.\n\n    Returns:\n        Loaded dataframe.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n    \"\"\"\n    full_path = Path(file_path / file_name)\n    if not Path.exists(full_path):\n        raise FileNotFoundError(f\"File {full_path} does not exist.\")\n    # Assert that the file is a CSV file\n    assert full_path.suffix == \".csv\", f\"File {file_name} is not a CSV file.\"\n    df = pd.read_csv(full_path)\n    log(INFO, f\"DataFrame loaded from {full_path}\")\n    return df\n</code></pre>"},{"location":"api/#midst_toolkit.attacks.ensemble.process_split_data","title":"process_split_data","text":""},{"location":"api/#midst_toolkit.attacks.ensemble.process_split_data.split_real_data","title":"split_real_data","text":"<pre><code>split_real_data(\n    df_real,\n    column_to_stratify=None,\n    proportion=None,\n    random_seed=None,\n)\n</code></pre> <p>Splits a real dataset into train, validation, and test sets, saves them as CSV files, and returns the splits.</p> <p>Parameters:</p> Name Type Description Default <code>df_real</code> <code>DataFrame</code> <p>The input real dataset to be split.</p> required <code>column_to_stratify</code> <code>str | None</code> <p>Column name to use for stratified splitting. If provided, the function ensures that the distribution of values in this column is preserved across the splits. If None, no stratification is applied. Defaults to None.</p> <code>None</code> <code>proportion</code> <code>dict[str, float] | None</code> <p>Proportions for train and validation splits. If None, defaults to {\"train\": 0.5, \"val\": 0.25}. The test set proportion will be inferred as 1 - (train + val). Defaults to None.</p> <code>None</code> <code>random_seed</code> <code>int | None</code> <p>Random seed for reproducibility. If None, you might get different splits each time. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame, DataFrame]</code> <p>A tuple containing the train, validation, and test dataframes.</p> Source code in <code>src/midst_toolkit/attacks/ensemble/process_split_data.py</code> <pre><code>def split_real_data(\n    df_real: pd.DataFrame,\n    column_to_stratify: str | None = None,\n    proportion: dict[str, float] | None = None,\n    random_seed: int | None = None,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Splits a real dataset into train, validation, and test sets, saves them as CSV files, and returns the splits.\n\n    Args:\n        df_real: The input real dataset to be split.\n        column_to_stratify: Column name to use for stratified splitting. If provided, the function\n            ensures that the distribution of values in this column is preserved across the splits.\n            If None, no stratification is applied. Defaults to None.\n        proportion: Proportions for train and validation splits. If None, defaults to {\"train\": 0.5, \"val\": 0.25}.\n            The test set proportion will be inferred as 1 - (train + val). Defaults to None.\n        random_seed: Random seed for reproducibility. If None, you might get different splits each time.\n            Defaults to None.\n\n    Returns:\n        A tuple containing the train, validation, and test dataframes.\n    \"\"\"\n    if proportion is None:\n        proportion = {\"train\": 0.5, \"val\": 0.25}\n    else:\n        # Sanity check for proportion values\n        assert \"train\" in proportion and \"val\" in proportion, \"Proportion must contain 'train' and 'val' keys.\"\n        assert 0 &lt; proportion[\"train\"] &lt; 1, \"Train proportion must be between 0 and 1.\"\n        assert 0 &lt; proportion[\"val\"] &lt; 1, \"Validation proportion must be between 0 and 1.\"\n        assert proportion[\"train\"] + proportion[\"val\"] &lt; 1, (\n            \"Sum of train and validation proportions must be less than 1.\"\n        )\n\n    # Split the real data into train and control\n    df_real_train, df_real_control = train_test_split(\n        df_real,\n        test_size=1 - proportion[\"train\"],\n        random_state=random_seed,\n        stratify=df_real[column_to_stratify],\n    )\n\n    # Further split the control into val and test set:\n    df_real_val, df_real_test = train_test_split(\n        df_real_control,\n        test_size=(1 - proportion[\"train\"] - proportion[\"val\"]) / (1 - proportion[\"train\"]),\n        random_state=random_seed,\n        stratify=df_real_control[column_to_stratify],\n    )\n\n    return (\n        df_real_train,\n        df_real_val,\n        df_real_test,\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.attacks.ensemble.process_split_data.generate_train_test_challenge_splits","title":"generate_train_test_challenge_splits","text":"<pre><code>generate_train_test_challenge_splits(\n    df_real_train,\n    df_real_control_val,\n    df_real_control_test,\n    stratify,\n    random_seed,\n)\n</code></pre> <p>Generates the validation and test sets with labels. The resulting validation and test sets are used for meta classifier training and evaluation, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>df_real_train</code> <code>DataFrame</code> <p>Real training data.</p> required <code>df_real_control_val</code> <code>DataFrame</code> <p>Real control data for validation.</p> required <code>df_real_control_test</code> <code>DataFrame</code> <p>Real control data for final evaluation.</p> required <code>stratify</code> <code>Series</code> <p>Series used to stratify the real training data. This column is added to read train data as \"stratify\" and is used for stratified splitting. This ensures that the distribution of values in this column is preserved across the splits.</p> required <code>random_seed</code> <code>int</code> <p>Random seed for reproducibility.</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, ndarray, DataFrame, ndarray]</code> <p>Features and labels for validation and test sets, respectively.</p> Source code in <code>src/midst_toolkit/attacks/ensemble/process_split_data.py</code> <pre><code>def generate_train_test_challenge_splits(\n    df_real_train: pd.DataFrame,\n    df_real_control_val: pd.DataFrame,\n    df_real_control_test: pd.DataFrame,\n    stratify: pd.Series,\n    random_seed: int,\n) -&gt; tuple[pd.DataFrame, np.ndarray, pd.DataFrame, np.ndarray]:\n    \"\"\"\n    Generates the validation and test sets with labels.\n    The resulting validation and test sets are used for meta classifier training and evaluation, respectively.\n\n    Args:\n        df_real_train: Real training data.\n        df_real_control_val: Real control data for validation.\n        df_real_control_test: Real control data for final evaluation.\n        stratify: Series used to stratify the real training data. This column is added to read train data\n            as \"stratify\" and is used for stratified splitting. This ensures that the distribution of values\n            in this column is preserved across the splits.\n        random_seed: Random seed for reproducibility.\n\n    Returns:\n        Features and labels for validation and test sets, respectively.\n    \"\"\"\n    df_real_train[\"stratify\"] = stratify\n\n    # Construct validation set for ensemble model\n    df_real_train_val, df_temp = train_test_split(\n        df_real_train,\n        train_size=len(df_real_control_val),\n        stratify=df_real_train[\"stratify\"],\n        random_state=random_seed,\n    )\n\n    #  Label 1 for real records used to generate 1st generation synthetic data and 0 for control\n    df_real_train_val[\"is_train\"] = 1\n    df_real_control_val[\"is_train\"] = 0\n\n    df_val = pd.concat(\n        [df_real_train_val.drop(columns=[\"stratify\"]), df_real_control_val],\n        axis=0,\n        ignore_index=True,\n    )\n\n    # Shuffle data\n    df_val = df_val.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n\n    y_val = df_val[\"is_train\"].values\n    df_val = df_val.drop(columns=[\"is_train\"])\n\n    # Test set\n    # `df_temp` will be assigned as our test set if it has the same size as `df_real_control_test`,\n    # otherwise, we further split `df_temp` to get a test set of the same size as `df_real_control_test`.\n    # This is because we want to take a train split of same size as `df_real_control_test` to ensure\n    # balanced classes in the final test set.\n    if len(df_temp) == len(df_real_control_test):\n        df_real_train_test = df_temp\n    else:\n        df_real_train_test, _ = train_test_split(\n            df_temp,\n            train_size=len(df_real_control_test),\n            stratify=df_temp[\"stratify\"],\n            random_state=random_seed,\n        )\n\n    df_real_train_test[\"is_train\"] = 1\n    df_real_control_test[\"is_train\"] = 0\n\n    df_test = pd.concat(\n        [df_real_train_test.drop(columns=[\"stratify\"]), df_real_control_test],\n        axis=0,\n        ignore_index=True,\n    )\n\n    df_test = df_test.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n\n    y_test = df_test[\"is_train\"].values\n    df_test = df_test.drop(columns=[\"is_train\"])\n\n    return df_val, y_val, df_test, y_test\n</code></pre>"},{"location":"api/#midst_toolkit.attacks.ensemble.process_split_data.process_split_data","title":"process_split_data","text":"<pre><code>process_split_data(\n    all_population_data,\n    processed_attack_data_path,\n    column_to_stratify,\n    num_total_samples=40000,\n    random_seed=42,\n)\n</code></pre> <p>Splits the data into train, validation, and test sets according to the attack design.</p> <p>Parameters:</p> Name Type Description Default <code>all_population_data</code> <code>DataFrame</code> <p>The total population data that the attacker has access to as a DataFrame.</p> required <code>processed_attack_data_path</code> <code>Path</code> <p>Path where the processed attack data will be saved.</p> required <code>column_to_stratify</code> <code>str</code> <p>Column name to use for stratified splitting.</p> required <code>num_total_samples</code> <code>int</code> <p>Number os samples that I randomly selected from the population. Defaults to 40000.</p> <code>40000</code> <code>random_seed</code> <code>int</code> <p>Random seed used for reproducibility. Defaults to 42.</p> <code>42</code> Source code in <code>src/midst_toolkit/attacks/ensemble/process_split_data.py</code> <pre><code>def process_split_data(\n    all_population_data: pd.DataFrame,\n    processed_attack_data_path: Path,\n    column_to_stratify: str,\n    num_total_samples: int = 40000,\n    random_seed: int = 42,\n) -&gt; None:\n    \"\"\"\n    Splits the data into train, validation, and test sets according to the attack design.\n\n    Args:\n        all_population_data: The total population data that the attacker has access to as a DataFrame.\n        processed_attack_data_path: Path where the processed attack data will be saved.\n        column_to_stratify: Column name to use for stratified splitting.\n        num_total_samples: Number os samples that I randomly selected from the population. Defaults to 40000.\n        random_seed: Random seed used for reproducibility. Defaults to 42.\n    \"\"\"\n    # Original Ensemble attack samples 40k data points to construct\n    # 1) the main population (real data) used for training the synthetic data generator model,\n    # 2) evaluation that is the meta train data (membership classification train dataset) used to train\n    #    the meta classifier,\n    # 3) test (membership classification test dataset) to evaluate the meta classifier.\n\n    df_real_data = all_population_data.sample(n=num_total_samples, random_state=random_seed)\n\n    # `df_real_train` is used for training the synthetic data generator model.\n    df_real_train, df_real_val, df_real_test = split_real_data(\n        df_real_data,\n        column_to_stratify=column_to_stratify,\n        random_seed=random_seed,\n    )\n    # Generate challenge datasets:\n    # `df_val` is used for training the meta classifier (membership classification train dataset).\n    # and `df_test` is used for meta classifier evaluation (membership classification test dataset).\n    # A part of the `df_real_train` will be assigned to `df_val` and a another part to `df_test` with\n    # their \"is_train\" column set to 1 meaning that these samples are in the models training corpus.\n    # Because `df_real_train` will be used to train a synthetic model, we're including some of it in\n    # `df_val` and `df_test` sets to create the challenges assuming the `df_real_val` and `df_real_test`\n    # data will not be part of the training data.\n    # This code makes sure `is_train` classes are balanced in the challenge datasets.\n    df_val, y_val, df_test, y_test = generate_train_test_challenge_splits(\n        df_real_train,\n        df_real_val,\n        df_real_test,\n        stratify=df_real_train[column_to_stratify],  # TODO: This value is not documented in the original codebase.\n        random_seed=random_seed,\n    )\n\n    df_real_train = df_real_train.drop(columns=[\"stratify\"])\n    df_real_val = df_real_val.drop(columns=[\"is_train\"])\n    df_real_test = df_real_test.drop(columns=[\"is_train\"])\n\n    save_dataframe(df_real_train, processed_attack_data_path, \"real_train.csv\")\n    save_dataframe(df_real_val, processed_attack_data_path, \"real_val.csv\")\n    save_dataframe(df_real_test, processed_attack_data_path, \"real_test.csv\")\n\n    save_dataframe(df_val, processed_attack_data_path, \"master_challenge_train.csv\")\n    np.save(\n        processed_attack_data_path / \"master_challenge_train_labels.npy\",\n        y_val,\n    )\n    save_dataframe(df_test, processed_attack_data_path, \"master_challenge_test.csv\")\n    np.save(\n        processed_attack_data_path / \"master_challenge_test_labels.npy\",\n        y_test,\n    )\n    log(INFO, f\"Data splits saved to {processed_attack_data_path}\")\n</code></pre>"},{"location":"api/#midst_toolkit.common","title":"common","text":""},{"location":"api/#midst_toolkit.common.enumerations","title":"enumerations","text":""},{"location":"api/#midst_toolkit.common.enumerations.TaskType","title":"TaskType","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>src/midst_toolkit/common/enumerations.py</code> <pre><code>class TaskType(Enum):\n    BINCLASS = \"binclass\"\n    MULTICLASS = \"multiclass\"\n    REGRESSION = \"regression\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string value of the enum.\"\"\"\n        return self.value\n</code></pre>"},{"location":"api/#midst_toolkit.common.enumerations.TaskType.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Return the string value of the enum.</p> Source code in <code>src/midst_toolkit/common/enumerations.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string value of the enum.\"\"\"\n    return self.value\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger","title":"logger","text":"<p>MIDST Toolkit Logger. Borrowed heavily from the Flower Labs logger.</p>"},{"location":"api/#midst_toolkit.common.logger.ConsoleHandler","title":"ConsoleHandler","text":"<p>               Bases: <code>StreamHandler</code></p> Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>class ConsoleHandler(StreamHandler):\n    def __init__(\n        self,\n        timestamps: bool = False,\n        json: bool = False,\n        colored: bool = True,\n        stream: TextIO | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Console handler that allows configurable formatting.\n\n        Args:\n            timestamps: Whether or not to include timestamps. Defaults to False.\n            json: Whether or not to accept json. Defaults to False.\n            colored: Whether or not to apply color to the logs. Defaults to True.\n            stream: To initialize the underlying StreamHandler. Defaults to None.\n        \"\"\"\n        super().__init__(stream)\n        self.timestamps = timestamps\n        self.json = json\n        self.colored = colored\n\n    def emit(self, record: LogRecord) -&gt; None:\n        \"\"\"\n        Console handler that emits the provided record.\n\n        Args:\n            record: Record to emit\n        \"\"\"\n        if self.json:\n            record.message = record.getMessage().replace(\"\\t\", \"\").strip()\n\n            # Check if the message is empty\n            if not record.message:\n                return\n\n        super().emit(record)\n\n    def format(self, record: LogRecord) -&gt; str:\n        \"\"\"\n        Format function that adds colors to log level.\n\n        Args:\n            record: Record to have color added\n\n        Returns:\n            String with color formatting corresponding to the log.\n        \"\"\"\n        seperator = \" \" * (8 - len(record.levelname))\n        if self.json:\n            log_fmt = \"{lvl='%(levelname)s', time='%(asctime)s', msg='%(message)s'}\"\n        else:\n            log_fmt = (\n                f\"{LOG_COLORS[record.levelname] if self.colored else ''}\"\n                f\"%(levelname)s {'%(asctime)s' if self.timestamps else ''}\"\n                f\"{LOG_COLORS['RESET'] if self.colored else ''}\"\n                f\": {seperator} %(message)s\"\n            )\n        formatter = logging.Formatter(log_fmt)\n        return formatter.format(record)\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.ConsoleHandler.__init__","title":"__init__","text":"<pre><code>__init__(\n    timestamps=False, json=False, colored=True, stream=None\n)\n</code></pre> <p>Console handler that allows configurable formatting.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>bool</code> <p>Whether or not to include timestamps. Defaults to False.</p> <code>False</code> <code>json</code> <code>bool</code> <p>Whether or not to accept json. Defaults to False.</p> <code>False</code> <code>colored</code> <code>bool</code> <p>Whether or not to apply color to the logs. Defaults to True.</p> <code>True</code> <code>stream</code> <code>TextIO | None</code> <p>To initialize the underlying StreamHandler. Defaults to None.</p> <code>None</code> Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def __init__(\n    self,\n    timestamps: bool = False,\n    json: bool = False,\n    colored: bool = True,\n    stream: TextIO | None = None,\n) -&gt; None:\n    \"\"\"\n    Console handler that allows configurable formatting.\n\n    Args:\n        timestamps: Whether or not to include timestamps. Defaults to False.\n        json: Whether or not to accept json. Defaults to False.\n        colored: Whether or not to apply color to the logs. Defaults to True.\n        stream: To initialize the underlying StreamHandler. Defaults to None.\n    \"\"\"\n    super().__init__(stream)\n    self.timestamps = timestamps\n    self.json = json\n    self.colored = colored\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.ConsoleHandler.emit","title":"emit","text":"<pre><code>emit(record)\n</code></pre> <p>Console handler that emits the provided record.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>LogRecord</code> <p>Record to emit</p> required Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def emit(self, record: LogRecord) -&gt; None:\n    \"\"\"\n    Console handler that emits the provided record.\n\n    Args:\n        record: Record to emit\n    \"\"\"\n    if self.json:\n        record.message = record.getMessage().replace(\"\\t\", \"\").strip()\n\n        # Check if the message is empty\n        if not record.message:\n            return\n\n    super().emit(record)\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.ConsoleHandler.format","title":"format","text":"<pre><code>format(record)\n</code></pre> <p>Format function that adds colors to log level.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>LogRecord</code> <p>Record to have color added</p> required <p>Returns:</p> Type Description <code>str</code> <p>String with color formatting corresponding to the log.</p> Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def format(self, record: LogRecord) -&gt; str:\n    \"\"\"\n    Format function that adds colors to log level.\n\n    Args:\n        record: Record to have color added\n\n    Returns:\n        String with color formatting corresponding to the log.\n    \"\"\"\n    seperator = \" \" * (8 - len(record.levelname))\n    if self.json:\n        log_fmt = \"{lvl='%(levelname)s', time='%(asctime)s', msg='%(message)s'}\"\n    else:\n        log_fmt = (\n            f\"{LOG_COLORS[record.levelname] if self.colored else ''}\"\n            f\"%(levelname)s {'%(asctime)s' if self.timestamps else ''}\"\n            f\"{LOG_COLORS['RESET'] if self.colored else ''}\"\n            f\": {seperator} %(message)s\"\n        )\n    formatter = logging.Formatter(log_fmt)\n    return formatter.format(record)\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.update_console_handler","title":"update_console_handler","text":"<pre><code>update_console_handler(\n    level=None, timestamps=None, colored=None\n)\n</code></pre> <p>Helper function for setting the proper logging.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int | str | None</code> <p>Level of the logger. Defaults to None.</p> <code>None</code> <code>timestamps</code> <code>bool | None</code> <p>Whether to include timestamps. Defaults to None.</p> <code>None</code> <code>colored</code> <code>bool | None</code> <p>Whether to apply color formatting. Defaults to None.</p> <code>None</code> Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def update_console_handler(\n    level: int | str | None = None,\n    timestamps: bool | None = None,\n    colored: bool | None = None,\n) -&gt; None:\n    \"\"\"\n    Helper function for setting the proper logging.\n\n    Args:\n        level: Level of the logger. Defaults to None.\n        timestamps: Whether to include timestamps. Defaults to None.\n        colored: Whether to apply color formatting. Defaults to None.\n    \"\"\"\n    for handler in logging.getLogger(LOGGER_NAME).handlers:\n        if isinstance(handler, ConsoleHandler):\n            if level is not None:\n                handler.setLevel(level)\n            if timestamps is not None:\n                handler.timestamps = timestamps\n            if colored is not None:\n                handler.colored = colored\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.configure","title":"configure","text":"<pre><code>configure(identifier, filename=None)\n</code></pre> <p>Configure logging to file.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>Identifier to front the logged string</p> required <code>filename</code> <code>str | None</code> <p>Name of the file producing the log, if desired. Defaults to None.</p> <code>None</code> Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def configure(identifier: str, filename: str | None = None) -&gt; None:\n    \"\"\"\n    Configure logging to file.\n\n    Args:\n        identifier: Identifier to front the logged string\n        filename: Name of the file producing the log, if desired. Defaults to None.\n    \"\"\"\n    # Create formatter\n    string_to_input = f\"{identifier} | %(levelname)s %(name)s %(asctime)s \"\n    string_to_input += \"| %(filename)s:%(lineno)d | %(message)s\"\n    formatter = logging.Formatter(string_to_input)\n\n    file_path = Path(filename) if filename else None\n\n    if file_path:\n        assert file_path.parent.exists(), \"Folder into which the logging file is to be inserted does not exist.\"\n        # Create file handler and log to disk\n        file_handler = logging.FileHandler(file_path)\n        file_handler.setLevel(logging.DEBUG)\n        file_handler.setFormatter(formatter)\n        TOOLKIT_LOGGER.addHandler(file_handler)\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.set_logger_propagation","title":"set_logger_propagation","text":"<pre><code>set_logger_propagation(child_logger, value=True)\n</code></pre> <p>Set the logger propagation attribute.</p> <p>Parameters:</p> Name Type Description Default <code>child_logger</code> <code>Logger</code> <p>Child logger object</p> required <code>value</code> <code>bool</code> <p>Boolean setting for propagation. If True, both parent and child logger display messages. Otherwise, only the child logger displays a message. This False setting prevents duplicate logs in Colab notebooks. Reference: https://stackoverflow.com/a/19561320. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Logger</code> <p>Child logger object with updated propagation setting</p> Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def set_logger_propagation(child_logger: logging.Logger, value: bool = True) -&gt; logging.Logger:\n    \"\"\"\n    Set the logger propagation attribute.\n\n    Args:\n        child_logger: Child logger object\n        value: Boolean setting for propagation. If True, both parent and child logger display messages. Otherwise,\n            only the child logger displays a message. This False setting prevents duplicate logs in Colab notebooks.\n            Reference: https://stackoverflow.com/a/19561320. Defaults to True.\n\n    Returns:\n        Child logger object with updated propagation setting\n    \"\"\"\n    child_logger.propagate = value\n    if not child_logger.propagate:\n        child_logger.log(logging.DEBUG, \"Logger propagate set to False\")\n    return child_logger\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.redirect_output","title":"redirect_output","text":"<pre><code>redirect_output(output_buffer)\n</code></pre> <p>Redirect stdout and stderr to text I/O buffer.</p> <p>Parameters:</p> Name Type Description Default <code>output_buffer</code> <code>StringIO</code> <p>output buffer to be directed to the I/O buffer</p> required Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def redirect_output(output_buffer: StringIO) -&gt; None:\n    \"\"\"\n    Redirect stdout and stderr to text I/O buffer.\n\n    Args:\n        output_buffer: output buffer to be directed to the I/O buffer\n    \"\"\"\n    sys.stdout = output_buffer\n    sys.stderr = output_buffer\n    console_handler.stream = sys.stdout\n</code></pre>"},{"location":"api/#midst_toolkit.common.random","title":"random","text":""},{"location":"api/#midst_toolkit.common.random.set_all_random_seeds","title":"set_all_random_seeds","text":"<pre><code>set_all_random_seeds(\n    seed=42,\n    use_deterministic_torch_algos=False,\n    disable_torch_benchmarking=False,\n)\n</code></pre> <p>Set seeds for python random, numpy random, and pytorch random. It also offers the option to force pytorch to use deterministic algorithm for certain methods and layers.</p> <p>See:</p> <p>https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html)</p> <p>for more details. Finally, it allows one to disable cuda benchmarking, which can also affect the determinism of pytorch training outside of random seeding. For more information on reproducibility in pytorch see:</p> <p>https://pytorch.org/docs/stable/notes/randomness.html</p> <p>NOTE: If the <code>use_deterministic_torch_algos</code> flag is set to True, you may need to set the environment variable <code>CUBLAS_WORKSPACE_CONFIG</code> to something like <code>:4096:8</code>, to avoid CUDA errors. Additional documentation may be found here:</p> <p>https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>The seed value to be used for random number generators. Default is 42. Seed setting will no-op if the seed is explicitly set to None.</p> <code>42</code> <code>use_deterministic_torch_algos</code> <code>bool</code> <p>Whether or not to set <code>torch.use_deterministic_algorithms</code> to True. Defaults to False.</p> <code>False</code> <code>disable_torch_benchmarking</code> <code>bool</code> <p>Whether to explicitly disable cuda benchmarking in torch processes. Defaults to False.</p> <code>False</code> Source code in <code>src/midst_toolkit/common/random.py</code> <pre><code>def set_all_random_seeds(\n    seed: int | None = 42, use_deterministic_torch_algos: bool = False, disable_torch_benchmarking: bool = False\n) -&gt; None:\n    \"\"\"\n    Set seeds for python random, numpy random, and pytorch random. It also offers the option to force pytorch to use\n    deterministic algorithm for certain methods and layers.\n\n    See:\n\n    https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html)\n\n    for more details. Finally, it allows one to disable cuda benchmarking, which can also affect the determinism of\n    pytorch training outside of random seeding. For more information on reproducibility in pytorch see:\n\n    https://pytorch.org/docs/stable/notes/randomness.html\n\n    **NOTE**: If the ``use_deterministic_torch_algos`` flag is set to True, you may need to set the environment\n    variable ``CUBLAS_WORKSPACE_CONFIG`` to something like ``:4096:8``, to avoid CUDA errors. Additional documentation\n    may be found here:\n\n    https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility\n\n    Args:\n        seed (int | None, optional): The seed value to be used for random number generators. Default is 42. Seed\n            setting will no-op if the seed is explicitly set to None.\n        use_deterministic_torch_algos (bool, optional): Whether or not to set ``torch.use_deterministic_algorithms`` to\n            True. Defaults to False.\n        disable_torch_benchmarking (bool, optional): Whether to explicitly disable cuda benchmarking in torch\n            processes. Defaults to False.\n    \"\"\"\n    if seed is None:\n        log(INFO, \"No seed provided. Using random seed.\")\n    else:\n        log(INFO, f\"Setting random seeds to {seed}.\")\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n    if use_deterministic_torch_algos:\n        log(INFO, \"Setting torch.use_deterministic_algorithms to True.\")\n        # warn_only is set to true so that layers and components without deterministic algorithms available will\n        # warn the user that they don't exist, but won't take down the process with an exception.\n        torch.use_deterministic_algorithms(True, warn_only=True)\n    if disable_torch_benchmarking:\n        log(INFO, \"Disabling CUDA algorithm benchmarking.\")\n        torch.backends.cudnn.benchmark = False\n</code></pre>"},{"location":"api/#midst_toolkit.common.random.unset_all_random_seeds","title":"unset_all_random_seeds","text":"<pre><code>unset_all_random_seeds()\n</code></pre> <p>Set random seeds for Python random, NumPy, and PyTorch to None. Running this function would undo the effects of <code>set_all_random_seeds</code>.</p> Source code in <code>src/midst_toolkit/common/random.py</code> <pre><code>def unset_all_random_seeds() -&gt; None:\n    \"\"\"\n    Set random seeds for Python random, NumPy, and PyTorch to None. Running this function would undo\n    the effects of ``set_all_random_seeds``.\n    \"\"\"\n    log(INFO, \"Setting all random seeds to None. Reverting torch determinism settings\")\n    random.seed(None)\n    np.random.seed(None)\n    torch.seed()\n    torch.use_deterministic_algorithms(False)\n</code></pre>"},{"location":"api/#midst_toolkit.common.random.save_random_state","title":"save_random_state","text":"<pre><code>save_random_state()\n</code></pre> <p>Save the state of the random number generators for Python, NumPy, and PyTorch. This will allow you to restore the state of the random number generators at a later time.</p> <p>Returns:</p> Type Description <code>tuple[Any, ...]</code> <p>tuple[tuple[Any, ...], dict[str, Any], torch.Tensor]: A tuple containing the state of the random number</p> <code>dict[str, Any]</code> <p>generators for Python, NumPy, and PyTorch.</p> Source code in <code>src/midst_toolkit/common/random.py</code> <pre><code>def save_random_state() -&gt; tuple[tuple[Any, ...], dict[str, Any], torch.Tensor]:\n    \"\"\"\n    Save the state of the random number generators for Python, NumPy, and PyTorch. This will allow you to restore the\n    state of the random number generators at a later time.\n\n    Returns:\n        tuple[tuple[Any, ...], dict[str, Any], torch.Tensor]: A tuple containing the state of the random number\n        generators for Python, NumPy, and PyTorch.\n    \"\"\"\n    log(INFO, \"Saving random state.\")\n    random_state = random.getstate()\n    numpy_state = np.random.get_state()\n    torch_state = torch.get_rng_state()\n    return random_state, numpy_state, torch_state\n</code></pre>"},{"location":"api/#midst_toolkit.common.random.restore_random_state","title":"restore_random_state","text":"<pre><code>restore_random_state(\n    random_state, numpy_state, torch_state\n)\n</code></pre> <p>Restore the state of the random number generators for Python, NumPy, and PyTorch. This will allow you to restore the state of the random number generators to a previously saved state.</p> <p>Parameters:</p> Name Type Description Default <code>random_state</code> <code>tuple[Any, ...]</code> <p>The state of the Python random number generator.</p> required <code>numpy_state</code> <code>dict[str, Any]</code> <p>The state of the NumPy random number generator.</p> required <code>torch_state</code> <code>Tensor</code> <p>The state of the PyTorch random number generator.</p> required Source code in <code>src/midst_toolkit/common/random.py</code> <pre><code>def restore_random_state(\n    random_state: tuple[Any, ...], numpy_state: dict[str, Any], torch_state: torch.Tensor\n) -&gt; None:\n    \"\"\"\n    Restore the state of the random number generators for Python, NumPy, and PyTorch. This will allow you to restore\n    the state of the random number generators to a previously saved state.\n\n    Args:\n        random_state (tuple[Any, ...]): The state of the Python random number generator.\n        numpy_state (dict[str, Any]): The state of the NumPy random number generator.\n        torch_state (torch.Tensor): The state of the PyTorch random number generator.\n    \"\"\"\n    log(INFO, \"Restoring random state.\")\n    random.setstate(random_state)\n    np.random.set_state(numpy_state)\n    torch.set_rng_state(torch_state)\n</code></pre>"},{"location":"api/#midst_toolkit.core","title":"core","text":"<p>PLACEHOLDER.</p>"},{"location":"api/#midst_toolkit.core.logger","title":"logger","text":"<p>Logger copied from OpenAI baselines to avoid extra RL-based dependencies.</p> <p>https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/logger.py</p>"},{"location":"api/#midst_toolkit.core.logger.TensorBoardOutputFormat","title":"TensorBoardOutputFormat","text":"<p>               Bases: <code>KVWriter</code></p> <p>Dumps key/value pairs into TensorBoard's numeric format.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>class TensorBoardOutputFormat(KVWriter):\n    \"\"\"Dumps key/value pairs into TensorBoard's numeric format.\"\"\"\n\n    def __init__(self, dir: str):\n        os.makedirs(dir, exist_ok=True)\n        self.dir = dir\n        self.step = 1\n        prefix = \"events\"\n        path = osp.join(osp.abspath(dir), prefix)\n        import tensorflow as tf\n        from tensorflow.core.util import event_pb2\n        from tensorflow.python import pywrap_tensorflow\n        from tensorflow.python.util import compat\n\n        self.tf = tf\n        self.event_pb2 = event_pb2\n        self.pywrap_tensorflow = pywrap_tensorflow\n        self.writer = pywrap_tensorflow.EventsWriter(compat.as_bytes(path))\n\n    def writekvs(self, kvs: dict[str, Any]) -&gt; None:\n        def summary_val(k: str, v: Any) -&gt; Any:\n            kwargs = {\"tag\": k, \"simple_value\": float(v)}\n            return self.tf.Summary.Value(**kwargs)\n\n        summary = self.tf.Summary(value=[summary_val(k, v) for k, v in kvs.items()])\n        event = self.event_pb2.Event(wall_time=time.time(), summary=summary)\n        event.step = self.step  # is there any reason why you'd want to specify the step?\n        self.writer.WriteEvent(event)\n        self.writer.Flush()\n        self.step += 1\n\n    def close(self) -&gt; None:\n        if self.writer:\n            self.writer.Close()\n            self.writer = None\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.logkv","title":"logkv","text":"<pre><code>logkv(key, val)\n</code></pre> <p>Log a value of some diagnostic.</p> <p>Call this once for each diagnostic quantity, each iteration If called many times, last value will be used.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def logkv(key: str, val: Any) -&gt; None:\n    \"\"\"\n    Log a value of some diagnostic.\n\n    Call this once for each diagnostic quantity, each iteration\n    If called many times, last value will be used.\n    \"\"\"\n    get_current().logkv(key, val)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.logkv_mean","title":"logkv_mean","text":"<pre><code>logkv_mean(key, val)\n</code></pre> <p>The same as logkv(), but if called many times, values averaged.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def logkv_mean(key: str, val: Any) -&gt; None:\n    \"\"\"The same as logkv(), but if called many times, values averaged.\"\"\"\n    get_current().logkv_mean(key, val)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.logkvs","title":"logkvs","text":"<pre><code>logkvs(d)\n</code></pre> <p>Log a dictionary of key-value pairs.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def logkvs(d: dict[str, Any]) -&gt; None:\n    \"\"\"Log a dictionary of key-value pairs.\"\"\"\n    for k, v in d.items():\n        logkv(k, v)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.dumpkvs","title":"dumpkvs","text":"<pre><code>dumpkvs()\n</code></pre> <p>Write all of the diagnostics from the current iteration.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def dumpkvs() -&gt; dict[str, Any]:\n    \"\"\"Write all of the diagnostics from the current iteration.\"\"\"\n    return get_current().dumpkvs()\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.log","title":"log","text":"<pre><code>log(*args, level=INFO)\n</code></pre> <p>Logs the args in the desired level.</p> <p>Write the sequence of args, with no separators, to the console and output files (if you've configured an output file).</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def log(*args: Iterable[Any], level: int = INFO) -&gt; None:\n    \"\"\"\n    Logs the args in the desired level.\n\n    Write the sequence of args, with no separators, to the console and output\n    files (if you've configured an output file).\n    \"\"\"\n    get_current().log(*args, level=level)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.set_level","title":"set_level","text":"<pre><code>set_level(level)\n</code></pre> <p>Set logging threshold on current logger.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def set_level(level: int) -&gt; None:\n    \"\"\"Set logging threshold on current logger.\"\"\"\n    get_current().set_level(level)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.get_dir","title":"get_dir","text":"<pre><code>get_dir()\n</code></pre> <p>Get directory that log files are being written to.</p> <p>will be None if there is no output directory (i.e., if you didn't call start)</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def get_dir() -&gt; str:\n    \"\"\"\n    Get directory that log files are being written to.\n\n    will be None if there is no output directory (i.e., if you didn't call start)\n    \"\"\"\n    return get_current().get_dir()\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.profile","title":"profile","text":"<pre><code>profile(n)\n</code></pre> <p>Usage.</p> <p>@profile(\"my_func\") def my_func(): code</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def profile(n):\n    \"\"\"\n    Usage.\n\n    @profile(\"my_func\")\n    def my_func(): code\n    \"\"\"\n\n    def decorator_with_name(func):\n        def func_wrapper(*args, **kwargs):\n            with profile_kv(n):\n                return func(*args, **kwargs)\n\n        return func_wrapper\n\n    return decorator_with_name\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.mpi_weighted_mean","title":"mpi_weighted_mean","text":"<pre><code>mpi_weighted_mean(comm, local_name2valcount)\n</code></pre> <p>Copied from below.</p> <p>https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/common/mpi_util.py#L110 Perform a weighted average over dicts that are each on a different node Input: local_name2valcount: dict mapping key -&gt; (value, count) Returns: key -&gt; mean</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def mpi_weighted_mean(comm: Any, local_name2valcount: dict[str, tuple[float, float]]) -&gt; dict[str, float]:\n    \"\"\"\n    Copied from below.\n\n    https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/common/mpi_util.py#L110\n    Perform a weighted average over dicts that are each on a different node\n    Input: local_name2valcount: dict mapping key -&gt; (value, count)\n    Returns: key -&gt; mean\n    \"\"\"\n    all_name2valcount = comm.gather(local_name2valcount)\n    if comm.rank == 0:\n        name2sum: defaultdict[str, float] = defaultdict(float)\n        name2count: defaultdict[str, float] = defaultdict(float)\n        for n2vc in all_name2valcount:\n            for name, (val, count) in n2vc.items():\n                try:\n                    val = float(val)\n                except ValueError:\n                    if comm.rank == 0:\n                        warnings.warn(\"WARNING: tried to compute mean on non-float {}={}\".format(name, val))\n                        # ruff: noqa: B028\n                else:\n                    name2sum[name] += val * count\n                    name2count[name] += count\n        return {name: name2sum[name] / name2count[name] for name in name2sum}\n    return {}\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.configure","title":"configure","text":"<pre><code>configure(\n    dir=None, format_strs=None, comm=None, log_suffix=\"\"\n)\n</code></pre> <p>If comm is provided, average all numerical stats across that comm.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def configure(\n    dir: str | None = None,\n    format_strs: list[str] | None = None,\n    comm: Any | None = None,\n    log_suffix: str = \"\",\n) -&gt; None:\n    \"\"\"If comm is provided, average all numerical stats across that comm.\"\"\"\n    if dir is None:\n        dir = os.getenv(\"OPENAI_LOGDIR\")\n    if dir is None:\n        dir = osp.join(\n            tempfile.gettempdir(),\n            datetime.datetime.now().strftime(\"openai-%Y-%m-%d-%H-%M-%S-%f\"),\n        )\n    assert isinstance(dir, str)\n    dir = os.path.expanduser(dir)\n    os.makedirs(os.path.expanduser(dir), exist_ok=True)\n\n    rank = get_rank_without_mpi_import()\n    if rank &gt; 0:\n        log_suffix = log_suffix + \"-rank%03i\" % rank\n\n    if format_strs is None:\n        if rank == 0:\n            format_strs = os.getenv(\"OPENAI_LOG_FORMAT\", \"stdout,log,csv\").split(\",\")\n        else:\n            format_strs = os.getenv(\"OPENAI_LOG_FORMAT_MPI\", \"log\").split(\",\")\n    format_strs_filter = filter(None, format_strs)\n    output_formats = [make_output_format(f, dir, log_suffix) for f in format_strs_filter]\n\n    Logger.CURRENT = Logger(dir=dir, output_formats=output_formats, comm=comm)  # type: ignore[assignment]\n    if output_formats:\n        log(\"Logging to %s\" % dir)\n</code></pre>"},{"location":"api/#midst_toolkit.data_processing","title":"data_processing","text":""},{"location":"api/#midst_toolkit.data_processing.midst_data_processing","title":"midst_data_processing","text":""},{"location":"api/#midst_toolkit.data_processing.midst_data_processing.process_midst_data_for_alpha_precision_evaluation","title":"process_midst_data_for_alpha_precision_evaluation","text":"<pre><code>process_midst_data_for_alpha_precision_evaluation(\n    numerical_real_data,\n    categorical_real_data,\n    numerical_synthetic_data,\n    categorical_synthetic_data,\n    dataset_name,\n    model,\n)\n</code></pre> <p>This function handles data preprocessing customized to some of the models and datasets used in the MIDST competition. The processing is drawn from https://github.com/VectorInstitute/MIDSTModels/blob/main/midst_models/single_table_TabDDPM/eval/eval_quality.py.</p> <p>It has special considerations for how the provided dataframes are processed into numpy arrays depending on the dataname and model provided in the arguments.</p> <p>Parameters:</p> Name Type Description Default <code>numerical_real_data</code> <code>DataFrame</code> <p>Real data with numerical values</p> required <code>categorical_real_data</code> <code>DataFrame</code> <p>Real data with categorical values</p> required <code>numerical_synthetic_data</code> <code>DataFrame</code> <p>Synthetically generated data with numerical values</p> required <code>categorical_synthetic_data</code> <code>DataFrame</code> <p>Synthetically generated data with numerical values</p> required <code>dataset_name</code> <code>str</code> <p>Name of the dataset to which the real data belongs. The way that the data is processed will depend on whether special treatment is required for the specified name.</p> required <code>model</code> <code>str</code> <p>Model that was used to generate the synthetic data. Specific model names require special postprocessing in order for quality evaluation</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A tuple of four Numpy arrays, one for each of the numerical and categorical collections of real and synthetic</p> <code>ndarray</code> <p>data after processing. The order is numerical and categorical data for the real data, followed by the same</p> <code>ndarray</code> <p>for the synthetic data.</p> Source code in <code>src/midst_toolkit/data_processing/midst_data_processing.py</code> <pre><code>def process_midst_data_for_alpha_precision_evaluation(\n    numerical_real_data: pd.DataFrame,\n    categorical_real_data: pd.DataFrame,\n    numerical_synthetic_data: pd.DataFrame,\n    categorical_synthetic_data: pd.DataFrame,\n    dataset_name: str,\n    model: str,\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    This function handles data preprocessing customized to some of the models and datasets used in the MIDST\n    competition. The processing is drawn from\n    https://github.com/VectorInstitute/MIDSTModels/blob/main/midst_models/single_table_TabDDPM/eval/eval_quality.py.\n\n    It has special considerations for how the provided dataframes are processed into numpy arrays depending on the\n    dataname and model provided in the arguments.\n\n    Args:\n        numerical_real_data: Real data with numerical values\n        categorical_real_data: Real data with categorical values\n        numerical_synthetic_data: Synthetically generated data with numerical values\n        categorical_synthetic_data: Synthetically generated data with numerical values\n        dataset_name: Name of the dataset to which the real data belongs. The way that the data is processed will\n            depend on whether special treatment is required for the specified name.\n        model: Model that was used to generate the synthetic data. Specific model names require special postprocessing\n            in order for quality evaluation\n\n    Returns:\n        A tuple of four Numpy arrays, one for each of the numerical and categorical collections of real and synthetic\n        data after processing. The order is numerical and categorical data for the real data, followed by the same\n        for the synthetic data.\n    \"\"\"\n    categorical_synthetic_numpy = categorical_synthetic_data.to_numpy().astype(\"str\")\n\n    # Perform some special data post-processing for specific datasets and models as specified in the script\n    # arguments\n\n    if dataset_name in CONVERSION_DATASETS and model.startswith(CONVERSION_MODEL_PREFIX):\n        # If using the default or news dataset and a model postfixed with \"codi,\" need to perform an int cast\n        categorical_synthetic_numpy = categorical_synthetic_data.astype(\"int\").to_numpy().astype(\"str\")\n    elif model.startswith(CLIPPING_MODEL_PREFIX):\n        if dataset_name in MAX_CLIPPING_DATASETS:\n            # Column reassignment\n            categorical_synthetic_numpy[:, 1] = categorical_synthetic_data[11].astype(\"int\").to_numpy().astype(\"str\")\n            categorical_synthetic_numpy[:, 2] = categorical_synthetic_data[12].astype(\"int\").to_numpy().astype(\"str\")\n            categorical_synthetic_numpy[:, 3] = categorical_synthetic_data[13].astype(\"int\").to_numpy().astype(\"str\")\n\n            # Clip the maximum value to reflect that of the real data\n            max_data = categorical_real_data[14].max()\n            categorical_synthetic_data.loc[categorical_synthetic_data[14] &gt; max_data, 14] = max_data\n\n            # Perform column reassignment\n            categorical_synthetic_numpy[:, 4] = categorical_synthetic_data[14].astype(\"int\").to_numpy().astype(\"str\")\n            categorical_synthetic_numpy[:, 4] = categorical_synthetic_data[14].astype(\"int\").to_numpy().astype(\"str\")\n\n        elif dataset_name in MIN_MAX_CLIPPING_DATASETS:\n            # Note that columns here are not contiguous, so we enumerate\n            columns = categorical_real_data.columns\n            for i, col in enumerate(columns):\n                if categorical_real_data[col].dtype == \"int\":\n                    max_data = categorical_real_data[col].max()\n                    min_data = categorical_real_data[col].min()\n\n                    # Perform clipping based on the real data on both sides (min and max)\n                    categorical_synthetic_data.loc[categorical_synthetic_data[col] &gt; max_data, col] = max_data\n                    categorical_synthetic_data.loc[categorical_synthetic_data[col] &lt; min_data, col] = min_data\n\n                    categorical_synthetic_numpy[:, i] = (\n                        categorical_synthetic_data[col].astype(\"int\").to_numpy().astype(\"str\")\n                    )\n    return (\n        numerical_real_data.to_numpy(),\n        categorical_real_data.to_numpy().astype(\"str\"),\n        numerical_synthetic_data.to_numpy(),\n        categorical_synthetic_numpy,\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.data_processing.midst_data_processing.load_midst_data","title":"load_midst_data","text":"<pre><code>load_midst_data(\n    real_data_path, synthetic_data_path, meta_info_path\n)\n</code></pre> <p>Helper function for loading data at the specified paths. These paths are constructed either by the user or with a particular set of defaults that were used in the original MIDST competition (see, for example, https://github.com/VectorInstitute/MIDSTModels/blob/main/midst_models/single_table_TabDDPM/eval/eval_quality.py).</p> <p>Parameters:</p> Name Type Description Default <code>real_data_path</code> <code>Path</code> <p>Path from which to load the real data to which the synthetic data will be compared. This should be a CSV file.</p> required <code>synthetic_data_path</code> <code>Path</code> <p>Path from which to load the synthetic data to which the real data will be compared. This should be a CSV file.</p> required <code>meta_info_path</code> <code>Path</code> <p>This should be a JSON file containing meta information about the data generation process. Specifically, it should contain information about which columns of the real and synthetic data should actually be compared. It must contain keys: 'num_col_idx', 'cat_col_idx', 'target_col_idx', and 'task_type'.</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame, dict[str, Any]]</code> <p>The loaded real data, synthetic data, and meta information json for further processing.</p> Source code in <code>src/midst_toolkit/data_processing/midst_data_processing.py</code> <pre><code>def load_midst_data(\n    real_data_path: Path, synthetic_data_path: Path, meta_info_path: Path\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, dict[str, Any]]:\n    \"\"\"\n    Helper function for loading data at the specified paths. These paths are constructed either by the user or with a\n    particular set of defaults that were used in the original MIDST competition (see, for example,\n    https://github.com/VectorInstitute/MIDSTModels/blob/main/midst_models/single_table_TabDDPM/eval/eval_quality.py).\n\n    Args:\n        real_data_path: Path from which to load the real data to which the synthetic data will be compared. This\n            should be a CSV file.\n        synthetic_data_path: Path from which to load the synthetic data to which the real data will be compared. This\n            should be a CSV file.\n        meta_info_path: This should be a JSON file containing meta information about the data generation process.\n            Specifically, it should contain information about which columns of the real and synthetic data should\n            actually be compared. It must contain keys: 'num_col_idx', 'cat_col_idx', 'target_col_idx', and\n            'task_type'.\n\n    Returns:\n        The loaded real data, synthetic data, and meta information json for further processing.\n    \"\"\"\n    real_data = pd.read_csv(real_data_path)\n    synthetic_data = pd.read_csv(synthetic_data_path)\n\n    with open(meta_info_path, \"r\") as f:\n        meta_info = json.load(f)\n\n    return real_data, synthetic_data, meta_info\n</code></pre>"},{"location":"api/#midst_toolkit.data_processing.midst_data_processing.load_midst_data_with_test","title":"load_midst_data_with_test","text":"<pre><code>load_midst_data_with_test(\n    real_data_path,\n    synthetic_data_path,\n    meta_info_path,\n    real_data_test_path,\n)\n</code></pre> <p>Helper function for loading data at the specified paths. These paths are constructed either by the user or with a particular set of defaults that were used in the original MIDST competition (see, for example, https://github.com/VectorInstitute/MIDSTModels/blob/main/midst_models/single_table_TabDDPM/eval/eval_quality.py).</p> <p>Parameters:</p> Name Type Description Default <code>real_data_path</code> <code>Path</code> <p>Path from which to load the real data to which the synthetic data will be compared. This should be a CSV file.</p> required <code>synthetic_data_path</code> <code>Path</code> <p>Path from which to load the synthetic data to which the real data will be compared. This should be a CSV file.</p> required <code>real_data_test_path</code> <code>Path</code> <p>Path from which to load the real data to which the synthetic data will be compared. This should be a CSV file. This data should NOT have been used to train the model that generated the synthetic data. If None, then it will not be returned.</p> required <code>meta_info_path</code> <code>Path</code> <p>This should be a JSON file containing meta information about the data generation process. Specifically, it should contain information about which columns of the real and synthetic data should actually be compared. It must contain keys: 'num_col_idx', 'cat_col_idx', 'target_col_idx', and 'task_type'.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The loaded real data train split, synthetic data, real data test split, and meta information json for</p> <code>DataFrame</code> <p>further processing.</p> Source code in <code>src/midst_toolkit/data_processing/midst_data_processing.py</code> <pre><code>def load_midst_data_with_test(\n    real_data_path: Path, synthetic_data_path: Path, meta_info_path: Path, real_data_test_path: Path\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, dict[str, Any]]:\n    \"\"\"\n    Helper function for loading data at the specified paths. These paths are constructed either by the user or with a\n    particular set of defaults that were used in the original MIDST competition (see, for example,\n    https://github.com/VectorInstitute/MIDSTModels/blob/main/midst_models/single_table_TabDDPM/eval/eval_quality.py).\n\n    Args:\n        real_data_path: Path from which to load the real data to which the synthetic data will be compared. This\n            should be a CSV file.\n        synthetic_data_path: Path from which to load the synthetic data to which the real data will be compared. This\n            should be a CSV file.\n        real_data_test_path: Path from which to load the real data to which the synthetic data will be compared. This\n            should be a CSV file. This data should NOT have been used to train the model that generated the synthetic\n            data. If None, then it will not be returned.\n        meta_info_path: This should be a JSON file containing meta information about the data generation process.\n            Specifically, it should contain information about which columns of the real and synthetic data should\n            actually be compared. It must contain keys: 'num_col_idx', 'cat_col_idx', 'target_col_idx', and\n            'task_type'.\n\n    Returns:\n        The loaded real data train split, synthetic data, real data test split, and meta information json for\n        further processing.\n    \"\"\"\n    real_data, synthetic_data, meta_info = load_midst_data(real_data_path, synthetic_data_path, meta_info_path)\n    real_data_test = pd.read_csv(real_data_test_path)\n\n    return real_data, synthetic_data, real_data_test, meta_info\n</code></pre>"},{"location":"api/#midst_toolkit.data_processing.utils","title":"utils","text":"<p>Code is heavily inspired by SynthEvals approach to preprocessing https://github.com/schneiderkamplab/syntheval/tree/main/src/syntheval/utils.</p> <p>Note: There are a number of reasons why we don't directly import Syntheval's preprocessing pipeline directly. A few examples are:</p> <p>1) Syntheval's preprocessor holds the data in place and can only be used to transform the dataframes provided in the constructor. The one implemented below is more flexible. 2) Syntheval's get_cat_variables (our get_categorical_columns) has not been heavily documented which makes it difficult to use in practice. 3) For Syntheval's metrics, the preprocessing enforces a hardcoded threshold on categorical variable detection that cannot be changed and made assumptions about what your numerical and categorical column name lists implied. We make different assumptions here.</p>"},{"location":"api/#midst_toolkit.data_processing.utils.SynthEvalDataframeEncoding","title":"SynthEvalDataframeEncoding","text":"Source code in <code>src/midst_toolkit/data_processing/utils.py</code> <pre><code>class SynthEvalDataframeEncoding:\n    def __init__(\n        self,\n        real_data: pd.DataFrame,\n        synthetic_data: pd.DataFrame,\n        categorical_columns: list[str] | None,\n        numerical_columns: list[str] | None,\n        holdout_data: pd.DataFrame = None,\n    ) -&gt; None:\n        \"\"\"\n        A class responsible for fitting encoders and scalers for categorical and numerical columns of dataframes,\n        respectively, These transformations are fitted against the concatenation of the real, synthetic, and any held\n        out data that is provided to this class. The categorical and numerical columns lists should correspond to\n        the column names for each of the dataframes provided. When using encode and decode, these columns must also\n        be present in the provided dataframes to be transformed. Any extant columns in the dataframe not specified\n        in these lists will be unmodified. Currently, categorical values are transformed using an Ordinal Encoder and\n        numerical values are scaled using a MinMax Scaler.\n\n        NOTE: Upon initialization, the transformations are only fitted against the provided dataframes. These\n        dataframes are NOT transformed.\n\n        Args:\n            real_data: Dataframe of real data. To be combined with synthetic data to fit the transforms.\n            synthetic_data: Dataframe of synthetic data. To be combined with real data to fit the transforms.\n            categorical_columns: Column names associated with the categorical variables to be used to fit the\n                Ordinal Encoder.\n            numerical_columns: Column names associated with the numerical variables to be used to fit the\n                MinMax Scaler.\n            holdout_data: Any holdout or otherwise auxiliary dataframe to be included as part of the transformation\n                fitting process. If none, only the real and synthetic dataframes are used. Defaults to None.\n        \"\"\"\n        assert not is_none_or_empty(categorical_columns) or not is_none_or_empty(numerical_columns), (\n            \"Either categorical or numerical columns must be provided.\"\n        )\n\n        joint_dataframe = pd.concat((real_data.reset_index(), synthetic_data.reset_index()), axis=0)\n        if holdout_data is not None:\n            joint_dataframe = pd.concat((joint_dataframe.reset_index(), holdout_data.reset_index()), axis=0)\n\n        self.categorical_columns = None\n        if not is_none_or_empty(categorical_columns):\n            self.categorical_columns = categorical_columns\n            self.ordinal_encoder = OrdinalEncoder().fit(joint_dataframe[self.categorical_columns])\n\n        self.numerical_columns = None\n        if not is_none_or_empty(numerical_columns):\n            self.numerical_columns = numerical_columns\n            self.numerical_encoder = MinMaxScaler().fit(joint_dataframe[self.numerical_columns])\n\n    def encode(self, data_to_encode: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Use the fitted categorical and numerical column transformations to encode/scale the data in the provided\n        dataframe. This assumes that the ``data_to_encode`` shares the same columns used to fit the respective\n        transforms in ``self.categorical_columns`` and ``self.numerical_columns``.\n\n        NOTE: This is an immutable function. It is NOT an in-place operation.\n\n        Args:\n            data_to_encode: Dataframe to transform with the fitted transformations.\n\n        Returns:\n            New dataframe with the columns encoded/scaled.\n        \"\"\"\n        # Deep copy the dataframe\n        encoded_data = data_to_encode.copy()\n        if self.categorical_columns is not None:\n            encoded_data[self.categorical_columns] = self.ordinal_encoder.transform(\n                encoded_data[self.categorical_columns]\n            ).astype(\"int\")\n        if self.numerical_columns is not None:\n            encoded_data[self.numerical_columns] = self.numerical_encoder.transform(\n                encoded_data[self.numerical_columns]\n            )\n        return encoded_data\n\n    def decode(self, data_to_decode: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Assumes that the provided dataframe has been previously transformed by this class or has the appropriate\n        columns and values to facilitate inverting the transformation. That is, for example, taking a categorical\n        value of 1 and mapping it back to \"Cat,\" the original categorical. This assumes that the ``data_to_decode``\n        shares  the same columns used to fit the respective transforms in ``self.categorical_columns`` and\n        ``self.numerical_columns``.\n\n        NOTE: This is an immutable function. It is NOT an in-place operation.\n\n        Args:\n            data_to_decode: Dataframe containing columns that need to be mapped back to their \"original\" values.\n\n        Returns:\n            Dataframe with the inverse mapping/scaling applied to the specified columns.\n        \"\"\"\n        encoded_data = data_to_decode.copy()\n        if self.categorical_columns is not None:\n            encoded_data[self.categorical_columns] = self.ordinal_encoder.inverse_transform(\n                encoded_data[self.categorical_columns]\n            )\n        if self.numerical_columns is not None:\n            encoded_data[self.numerical_columns] = self.numerical_encoder.inverse_transform(\n                encoded_data[self.numerical_columns]\n            )\n        return encoded_data\n</code></pre>"},{"location":"api/#midst_toolkit.data_processing.utils.SynthEvalDataframeEncoding.__init__","title":"__init__","text":"<pre><code>__init__(\n    real_data,\n    synthetic_data,\n    categorical_columns,\n    numerical_columns,\n    holdout_data=None,\n)\n</code></pre> <p>A class responsible for fitting encoders and scalers for categorical and numerical columns of dataframes, respectively, These transformations are fitted against the concatenation of the real, synthetic, and any held out data that is provided to this class. The categorical and numerical columns lists should correspond to the column names for each of the dataframes provided. When using encode and decode, these columns must also be present in the provided dataframes to be transformed. Any extant columns in the dataframe not specified in these lists will be unmodified. Currently, categorical values are transformed using an Ordinal Encoder and numerical values are scaled using a MinMax Scaler.</p> <p>NOTE: Upon initialization, the transformations are only fitted against the provided dataframes. These dataframes are NOT transformed.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>DataFrame</code> <p>Dataframe of real data. To be combined with synthetic data to fit the transforms.</p> required <code>synthetic_data</code> <code>DataFrame</code> <p>Dataframe of synthetic data. To be combined with real data to fit the transforms.</p> required <code>categorical_columns</code> <code>list[str] | None</code> <p>Column names associated with the categorical variables to be used to fit the Ordinal Encoder.</p> required <code>numerical_columns</code> <code>list[str] | None</code> <p>Column names associated with the numerical variables to be used to fit the MinMax Scaler.</p> required <code>holdout_data</code> <code>DataFrame</code> <p>Any holdout or otherwise auxiliary dataframe to be included as part of the transformation fitting process. If none, only the real and synthetic dataframes are used. Defaults to None.</p> <code>None</code> Source code in <code>src/midst_toolkit/data_processing/utils.py</code> <pre><code>def __init__(\n    self,\n    real_data: pd.DataFrame,\n    synthetic_data: pd.DataFrame,\n    categorical_columns: list[str] | None,\n    numerical_columns: list[str] | None,\n    holdout_data: pd.DataFrame = None,\n) -&gt; None:\n    \"\"\"\n    A class responsible for fitting encoders and scalers for categorical and numerical columns of dataframes,\n    respectively, These transformations are fitted against the concatenation of the real, synthetic, and any held\n    out data that is provided to this class. The categorical and numerical columns lists should correspond to\n    the column names for each of the dataframes provided. When using encode and decode, these columns must also\n    be present in the provided dataframes to be transformed. Any extant columns in the dataframe not specified\n    in these lists will be unmodified. Currently, categorical values are transformed using an Ordinal Encoder and\n    numerical values are scaled using a MinMax Scaler.\n\n    NOTE: Upon initialization, the transformations are only fitted against the provided dataframes. These\n    dataframes are NOT transformed.\n\n    Args:\n        real_data: Dataframe of real data. To be combined with synthetic data to fit the transforms.\n        synthetic_data: Dataframe of synthetic data. To be combined with real data to fit the transforms.\n        categorical_columns: Column names associated with the categorical variables to be used to fit the\n            Ordinal Encoder.\n        numerical_columns: Column names associated with the numerical variables to be used to fit the\n            MinMax Scaler.\n        holdout_data: Any holdout or otherwise auxiliary dataframe to be included as part of the transformation\n            fitting process. If none, only the real and synthetic dataframes are used. Defaults to None.\n    \"\"\"\n    assert not is_none_or_empty(categorical_columns) or not is_none_or_empty(numerical_columns), (\n        \"Either categorical or numerical columns must be provided.\"\n    )\n\n    joint_dataframe = pd.concat((real_data.reset_index(), synthetic_data.reset_index()), axis=0)\n    if holdout_data is not None:\n        joint_dataframe = pd.concat((joint_dataframe.reset_index(), holdout_data.reset_index()), axis=0)\n\n    self.categorical_columns = None\n    if not is_none_or_empty(categorical_columns):\n        self.categorical_columns = categorical_columns\n        self.ordinal_encoder = OrdinalEncoder().fit(joint_dataframe[self.categorical_columns])\n\n    self.numerical_columns = None\n    if not is_none_or_empty(numerical_columns):\n        self.numerical_columns = numerical_columns\n        self.numerical_encoder = MinMaxScaler().fit(joint_dataframe[self.numerical_columns])\n</code></pre>"},{"location":"api/#midst_toolkit.data_processing.utils.SynthEvalDataframeEncoding.encode","title":"encode","text":"<pre><code>encode(data_to_encode)\n</code></pre> <p>Use the fitted categorical and numerical column transformations to encode/scale the data in the provided dataframe. This assumes that the <code>data_to_encode</code> shares the same columns used to fit the respective transforms in <code>self.categorical_columns</code> and <code>self.numerical_columns</code>.</p> <p>NOTE: This is an immutable function. It is NOT an in-place operation.</p> <p>Parameters:</p> Name Type Description Default <code>data_to_encode</code> <code>DataFrame</code> <p>Dataframe to transform with the fitted transformations.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>New dataframe with the columns encoded/scaled.</p> Source code in <code>src/midst_toolkit/data_processing/utils.py</code> <pre><code>def encode(self, data_to_encode: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Use the fitted categorical and numerical column transformations to encode/scale the data in the provided\n    dataframe. This assumes that the ``data_to_encode`` shares the same columns used to fit the respective\n    transforms in ``self.categorical_columns`` and ``self.numerical_columns``.\n\n    NOTE: This is an immutable function. It is NOT an in-place operation.\n\n    Args:\n        data_to_encode: Dataframe to transform with the fitted transformations.\n\n    Returns:\n        New dataframe with the columns encoded/scaled.\n    \"\"\"\n    # Deep copy the dataframe\n    encoded_data = data_to_encode.copy()\n    if self.categorical_columns is not None:\n        encoded_data[self.categorical_columns] = self.ordinal_encoder.transform(\n            encoded_data[self.categorical_columns]\n        ).astype(\"int\")\n    if self.numerical_columns is not None:\n        encoded_data[self.numerical_columns] = self.numerical_encoder.transform(\n            encoded_data[self.numerical_columns]\n        )\n    return encoded_data\n</code></pre>"},{"location":"api/#midst_toolkit.data_processing.utils.SynthEvalDataframeEncoding.decode","title":"decode","text":"<pre><code>decode(data_to_decode)\n</code></pre> <p>Assumes that the provided dataframe has been previously transformed by this class or has the appropriate columns and values to facilitate inverting the transformation. That is, for example, taking a categorical value of 1 and mapping it back to \"Cat,\" the original categorical. This assumes that the <code>data_to_decode</code> shares  the same columns used to fit the respective transforms in <code>self.categorical_columns</code> and <code>self.numerical_columns</code>.</p> <p>NOTE: This is an immutable function. It is NOT an in-place operation.</p> <p>Parameters:</p> Name Type Description Default <code>data_to_decode</code> <code>DataFrame</code> <p>Dataframe containing columns that need to be mapped back to their \"original\" values.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe with the inverse mapping/scaling applied to the specified columns.</p> Source code in <code>src/midst_toolkit/data_processing/utils.py</code> <pre><code>def decode(self, data_to_decode: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Assumes that the provided dataframe has been previously transformed by this class or has the appropriate\n    columns and values to facilitate inverting the transformation. That is, for example, taking a categorical\n    value of 1 and mapping it back to \"Cat,\" the original categorical. This assumes that the ``data_to_decode``\n    shares  the same columns used to fit the respective transforms in ``self.categorical_columns`` and\n    ``self.numerical_columns``.\n\n    NOTE: This is an immutable function. It is NOT an in-place operation.\n\n    Args:\n        data_to_decode: Dataframe containing columns that need to be mapped back to their \"original\" values.\n\n    Returns:\n        Dataframe with the inverse mapping/scaling applied to the specified columns.\n    \"\"\"\n    encoded_data = data_to_decode.copy()\n    if self.categorical_columns is not None:\n        encoded_data[self.categorical_columns] = self.ordinal_encoder.inverse_transform(\n            encoded_data[self.categorical_columns]\n        )\n    if self.numerical_columns is not None:\n        encoded_data[self.numerical_columns] = self.numerical_encoder.inverse_transform(\n            encoded_data[self.numerical_columns]\n        )\n    return encoded_data\n</code></pre>"},{"location":"api/#midst_toolkit.data_processing.utils.is_none_or_empty","title":"is_none_or_empty","text":"<pre><code>is_none_or_empty(list_to_check)\n</code></pre> <p>Function to check with the provided argument is None or is an empty list.</p> <p>Parameters:</p> Name Type Description Default <code>list_to_check</code> <code>list[str] | None</code> <p>List to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the list provided is empty or the variable is None</p> Source code in <code>src/midst_toolkit/data_processing/utils.py</code> <pre><code>def is_none_or_empty(list_to_check: list[str] | None) -&gt; bool:\n    \"\"\"\n    Function to check with the provided argument is None or is an empty list.\n\n    Args:\n        list_to_check: List to check.\n\n    Returns:\n        True if the list provided is empty or the variable is None\n    \"\"\"\n    return list_to_check is None or len(list_to_check) == 0\n</code></pre>"},{"location":"api/#midst_toolkit.data_processing.utils.infer_categorical_and_numerical_columns","title":"infer_categorical_and_numerical_columns","text":"<pre><code>infer_categorical_and_numerical_columns(\n    dataframe, categorical_threshold=10\n)\n</code></pre> <p>Helper function to take in a dataframe and extract the names of the categorical and numerical columns from the dataframe in separate lists. These are used to separately treat columns with these distinct types in downstream processing.</p> <p>NOTE: It is assumed that after identifying categorical columns, the remaining columns represent NUMERICAL values.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>Dataframe from which to extract the set of categorical and numerical columns.</p> required <code>categorical_threshold</code> <code>int</code> <p>Threshold below which a column with numerical values (integer or float for example) is deemed to represent a categorical encoding. The threshold is compared to the number of unique values present for the column in question. If set to 0 (or below), then no columns with numerical entries will be treated as categorical. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>tuple[list[str], list[str]]</code> <p>A tuple of column names that are deemed as holding categorical values and numerical values, respectively.</p> Source code in <code>src/midst_toolkit/data_processing/utils.py</code> <pre><code>def infer_categorical_and_numerical_columns(\n    dataframe: pd.DataFrame, categorical_threshold: int = 10\n) -&gt; tuple[list[str], list[str]]:\n    \"\"\"\n    Helper function to take in a dataframe and extract the names of the categorical and numerical columns from the\n    dataframe in separate lists. These are used to separately treat columns with these distinct types in downstream\n    processing.\n\n    NOTE: It is assumed that after identifying categorical columns, the remaining columns represent NUMERICAL values.\n\n    Args:\n        dataframe: Dataframe from which to extract the set of categorical and numerical columns.\n        categorical_threshold: Threshold below which a column with numerical values (integer or float for example) is\n            deemed to represent a categorical encoding. The threshold is compared to the number of unique values\n            present for the column in question. If set to 0 (or below), then no columns with numerical entries will be\n            treated as categorical. Defaults to 10.\n\n    Returns:\n        A tuple of column names that are deemed as holding categorical values and numerical values, respectively.\n    \"\"\"\n    categorical_columns = get_categorical_columns(dataframe, threshold=categorical_threshold)\n    log(INFO, f\"Automatically extracted categorical columns: {categorical_columns}\")\n\n    numerical_columns = [column for column in dataframe.columns if column not in categorical_columns]\n    log(INFO, f\"Numerical columns inferred as the complement of the categorical columns: {numerical_columns}\")\n\n    return categorical_columns, numerical_columns\n</code></pre>"},{"location":"api/#midst_toolkit.data_processing.utils.get_categorical_columns","title":"get_categorical_columns","text":"<pre><code>get_categorical_columns(dataframe, threshold)\n</code></pre> <p>This is a helper function to identify categorical columns in a dataframe. It is a bit brittle and certainly will not cover all possible setups. However, it can be helpful. The threshold variable controls how one deems a column with numerical values as constituting a categorical indicator. If a column has threshold unique values or less it is deemed a categorical column. For example, a hurricane might be rated from 1 to 5 in an integer based column. With a threshold of 10, this column would be added to the set of categorical columns.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>Dataframe from which to extract column names corresponding to categorical variables.</p> required <code>threshold</code> <code>int</code> <p>Threshold below which a column with numerical values (integer or float for example) is deemed to represent a categorical encoding. The threshold is compared to the number of unique values present for the column in question. If set to 0 (or below), then no columns with numerical entries will be treated as categorical.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of column names from the provided dataframe that correspond to categorical data.</p> Source code in <code>src/midst_toolkit/data_processing/utils.py</code> <pre><code>def get_categorical_columns(dataframe: pd.DataFrame, threshold: int) -&gt; list[str]:\n    \"\"\"\n    This is a helper function to identify categorical columns in a dataframe. It is a bit brittle and certainly will\n    not cover all possible setups. However, it can be helpful. The threshold variable controls how one deems a column\n    with numerical values as constituting a categorical indicator. If a column has threshold unique values or less\n    it is deemed a categorical column. For example, a hurricane might be rated from 1 to 5 in an integer based column.\n    With a threshold of 10, this column would be added to the set of categorical columns.\n\n    Args:\n        dataframe: Dataframe from which to extract column names corresponding to categorical variables.\n        threshold: Threshold below which a column with numerical values (integer or float for example) is deemed to\n            represent a categorical encoding. The threshold is compared to the number of unique values present for the\n            column in question. If set to 0 (or below), then no columns with numerical entries will be treated as\n            categorical.\n\n    Returns:\n        A list of column names from the provided dataframe that correspond to categorical data.\n    \"\"\"\n    categorical_variables: list[str] = []\n\n    for column_name in dataframe.columns:\n        # If dtype is an object (as str columns are), assume categorical\n        if (\n            dataframe[column_name].dtype == \"object\"\n            or is_column_type_numerical(dataframe, column_name)\n            and dataframe[column_name].nunique() &lt;= threshold\n        ):\n            categorical_variables.append(column_name)\n\n    return categorical_variables\n</code></pre>"},{"location":"api/#midst_toolkit.data_processing.utils.is_column_type_numerical","title":"is_column_type_numerical","text":"<pre><code>is_column_type_numerical(dataframe, column_name)\n</code></pre> <p>Determine with a column, as specified by <code>column_name</code> in the dataframe contains \"numerical\" values. This is a heuristic test based on the discussion in the link below.</p> <p>https://stackoverflow.com/questions/37726830/how-to-determine-if-a-number-is-any-type-of-int-core-or-numpy-signed-or-not</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>Dataframe whose column values are to be analyzed as being numerical or not.</p> required <code>column_name</code> <code>str</code> <p>Name of the column in the dataframe to be considered.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the column contains numerical values. False otherwise.</p> Source code in <code>src/midst_toolkit/data_processing/utils.py</code> <pre><code>def is_column_type_numerical(dataframe: pd.DataFrame, column_name: str) -&gt; bool:\n    \"\"\"\n    Determine with a column, as specified by ``column_name`` in the dataframe contains \"numerical\" values. This is\n    a heuristic test based on the discussion in the link below.\n\n    https://stackoverflow.com/questions/37726830/how-to-determine-if-a-number-is-any-type-of-int-core-or-numpy-signed-or-not\n\n    Args:\n        dataframe: Dataframe whose column values are to be analyzed as being numerical or not.\n        column_name: Name of the column in the dataframe to be considered.\n\n    Returns:\n        True if the column contains numerical values. False otherwise.\n    \"\"\"\n    column_dtype = dataframe[column_name].dtype\n\n    return np.issubdtype(column_dtype, np.integer) or np.issubdtype(column_dtype, np.floating)\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation","title":"evaluation","text":""},{"location":"api/#midst_toolkit.evaluation.metrics_base","title":"metrics_base","text":""},{"location":"api/#midst_toolkit.evaluation.metrics_base.MetricBase","title":"MetricBase","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>src/midst_toolkit/evaluation/metrics_base.py</code> <pre><code>class MetricBase(ABC):\n    @abstractmethod\n    def compute(self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame) -&gt; dict[str, float]:\n        \"\"\"\n        Abstract method for computing a synthetic data quality metric. Should be implemented by inheriting classes\n        and return a dictionary of values for the resulting metric computations.\n\n        Args:\n            real_data: Real data to which the synthetic data may be compared. In many cases this will be data used\n                to TRAIN the model that generated the synthetic data, but not always.\n            synthetic_data: Synthetically generated data whose quality is to be assessed.\n\n        Raises:\n            NotImplementedError: Must be implemented by inheriting metrics\n\n        Returns:\n            A dictionary with string keys containing float values computed by the metric. Some metrics return multiple\n            statistics. For example, in confidence interval estimation, one might have mean and standard deviation.\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must define compute\")\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.metrics_base.MetricBase.compute","title":"compute  <code>abstractmethod</code>","text":"<pre><code>compute(real_data, synthetic_data)\n</code></pre> <p>Abstract method for computing a synthetic data quality metric. Should be implemented by inheriting classes and return a dictionary of values for the resulting metric computations.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>DataFrame</code> <p>Real data to which the synthetic data may be compared. In many cases this will be data used to TRAIN the model that generated the synthetic data, but not always.</p> required <code>synthetic_data</code> <code>DataFrame</code> <p>Synthetically generated data whose quality is to be assessed.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented by inheriting metrics</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dictionary with string keys containing float values computed by the metric. Some metrics return multiple</p> <code>dict[str, float]</code> <p>statistics. For example, in confidence interval estimation, one might have mean and standard deviation.</p> Source code in <code>src/midst_toolkit/evaluation/metrics_base.py</code> <pre><code>@abstractmethod\ndef compute(self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame) -&gt; dict[str, float]:\n    \"\"\"\n    Abstract method for computing a synthetic data quality metric. Should be implemented by inheriting classes\n    and return a dictionary of values for the resulting metric computations.\n\n    Args:\n        real_data: Real data to which the synthetic data may be compared. In many cases this will be data used\n            to TRAIN the model that generated the synthetic data, but not always.\n        synthetic_data: Synthetically generated data whose quality is to be assessed.\n\n    Raises:\n        NotImplementedError: Must be implemented by inheriting metrics\n\n    Returns:\n        A dictionary with string keys containing float values computed by the metric. Some metrics return multiple\n        statistics. For example, in confidence interval estimation, one might have mean and standard deviation.\n    \"\"\"\n    raise NotImplementedError(\"Inheriting class must define compute\")\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.metrics_base.SynthEvalQualityMetric","title":"SynthEvalQualityMetric","text":"<p>               Bases: <code>MetricBase</code>, <code>ABC</code></p> Source code in <code>src/midst_toolkit/evaluation/metrics_base.py</code> <pre><code>class SynthEvalQualityMetric(MetricBase, ABC):\n    def __init__(\n        self,\n        categorical_columns: list[str],\n        numerical_columns: list[str],\n        do_preprocess: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Base class for SynthEval metrics. These metrics require designation of the column names that are associated\n        with categorical variables and those columns that are associated with numerical variables. These are used\n        to facilitate metric computation (for example, when only numerical columns are admissible) and for\n        preprocessing if desired. The default preprocessing pipeline for SynthEval is implemented by the\n        DataframeEncoding class. If desired this class can preprocess dataframes before performing computation.\n\n        Args:\n            categorical_columns: Column names corresponding to the categorical variables of any provided dataframe.\n            numerical_columns: Column names corresponding to the numerical variables of any provided dataframe.\n            do_preprocess: Whether or not to preprocess the dataframes with the default pipeline used by SynthEval.\n                Defaults to False.\n        \"\"\"\n        self.categorical_columns = categorical_columns\n        self.numerical_columns = numerical_columns\n        self.do_preprocess = do_preprocess\n\n        if do_preprocess:\n            log(INFO, \"Default preprocessing will be performed during computation.\")\n\n    @overload\n    def preprocess(\n        self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame\n    ) -&gt; tuple[pd.DataFrame, pd.DataFrame]: ...\n\n    @overload\n    def preprocess(\n        self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame, holdout_data: pd.DataFrame\n    ) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: ...\n\n    def preprocess(\n        self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame, holdout_data: pd.DataFrame | None = None\n    ) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame] | tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"\n        Applies the default dataframe preprocessing pipeline for SynthEval. This has been pulled into the library\n        to allow for this to be optional (it is not in the SynthEval library) and controllable for our purposes.\n        The preprocessing classes are fitted to the combination of all the provided dataframes and will be\n        refitted any time this is called.\n\n        Args:\n            real_data: Real data to which the synthetic data may be compared. In many cases this will be data used\n                to TRAIN the model that generated the synthetic data, but not always.\n            synthetic_data: Synthetically generated data whose quality is to be assessed.\n            holdout_data: An optional set of holdout data. Typically, this will be data drawn from the same\n                distribution as ``real_data`` but was explicitly NOT used to train the model that generated\n                ``synthetic_data``. Not all metrics will require a holdout set and it is, therefore optional.\n                Defaults to None.\n\n        Returns:\n            Transformed dataframes for the real, synthetic, and holdout dataframes (if provided)\n        \"\"\"\n        log(INFO, \"Performing default preprocessing using defined columns.\")\n        encoder = SynthEvalDataframeEncoding(\n            real_data, synthetic_data, self.categorical_columns, self.numerical_columns\n        )\n        real_data = encoder.encode(real_data)\n        synthetic_data = encoder.encode(synthetic_data)\n        holdout_data = encoder.encode(holdout_data) if holdout_data else None\n\n        if holdout_data is not None:\n            return real_data, synthetic_data, holdout_data\n        return real_data, synthetic_data\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.metrics_base.SynthEvalQualityMetric.__init__","title":"__init__","text":"<pre><code>__init__(\n    categorical_columns,\n    numerical_columns,\n    do_preprocess=False,\n)\n</code></pre> <p>Base class for SynthEval metrics. These metrics require designation of the column names that are associated with categorical variables and those columns that are associated with numerical variables. These are used to facilitate metric computation (for example, when only numerical columns are admissible) and for preprocessing if desired. The default preprocessing pipeline for SynthEval is implemented by the DataframeEncoding class. If desired this class can preprocess dataframes before performing computation.</p> <p>Parameters:</p> Name Type Description Default <code>categorical_columns</code> <code>list[str]</code> <p>Column names corresponding to the categorical variables of any provided dataframe.</p> required <code>numerical_columns</code> <code>list[str]</code> <p>Column names corresponding to the numerical variables of any provided dataframe.</p> required <code>do_preprocess</code> <code>bool</code> <p>Whether or not to preprocess the dataframes with the default pipeline used by SynthEval. Defaults to False.</p> <code>False</code> Source code in <code>src/midst_toolkit/evaluation/metrics_base.py</code> <pre><code>def __init__(\n    self,\n    categorical_columns: list[str],\n    numerical_columns: list[str],\n    do_preprocess: bool = False,\n) -&gt; None:\n    \"\"\"\n    Base class for SynthEval metrics. These metrics require designation of the column names that are associated\n    with categorical variables and those columns that are associated with numerical variables. These are used\n    to facilitate metric computation (for example, when only numerical columns are admissible) and for\n    preprocessing if desired. The default preprocessing pipeline for SynthEval is implemented by the\n    DataframeEncoding class. If desired this class can preprocess dataframes before performing computation.\n\n    Args:\n        categorical_columns: Column names corresponding to the categorical variables of any provided dataframe.\n        numerical_columns: Column names corresponding to the numerical variables of any provided dataframe.\n        do_preprocess: Whether or not to preprocess the dataframes with the default pipeline used by SynthEval.\n            Defaults to False.\n    \"\"\"\n    self.categorical_columns = categorical_columns\n    self.numerical_columns = numerical_columns\n    self.do_preprocess = do_preprocess\n\n    if do_preprocess:\n        log(INFO, \"Default preprocessing will be performed during computation.\")\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.metrics_base.SynthEvalQualityMetric.preprocess","title":"preprocess","text":"<pre><code>preprocess(\n    real_data: DataFrame, synthetic_data: DataFrame\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]\n</code></pre><pre><code>preprocess(\n    real_data: DataFrame,\n    synthetic_data: DataFrame,\n    holdout_data: DataFrame,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\n</code></pre> <pre><code>preprocess(real_data, synthetic_data, holdout_data=None)\n</code></pre> <p>Applies the default dataframe preprocessing pipeline for SynthEval. This has been pulled into the library to allow for this to be optional (it is not in the SynthEval library) and controllable for our purposes. The preprocessing classes are fitted to the combination of all the provided dataframes and will be refitted any time this is called.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>DataFrame</code> <p>Real data to which the synthetic data may be compared. In many cases this will be data used to TRAIN the model that generated the synthetic data, but not always.</p> required <code>synthetic_data</code> <code>DataFrame</code> <p>Synthetically generated data whose quality is to be assessed.</p> required <code>holdout_data</code> <code>DataFrame | None</code> <p>An optional set of holdout data. Typically, this will be data drawn from the same distribution as <code>real_data</code> but was explicitly NOT used to train the model that generated <code>synthetic_data</code>. Not all metrics will require a holdout set and it is, therefore optional. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame, DataFrame] | tuple[DataFrame, DataFrame]</code> <p>Transformed dataframes for the real, synthetic, and holdout dataframes (if provided)</p> Source code in <code>src/midst_toolkit/evaluation/metrics_base.py</code> <pre><code>def preprocess(\n    self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame, holdout_data: pd.DataFrame | None = None\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame] | tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Applies the default dataframe preprocessing pipeline for SynthEval. This has been pulled into the library\n    to allow for this to be optional (it is not in the SynthEval library) and controllable for our purposes.\n    The preprocessing classes are fitted to the combination of all the provided dataframes and will be\n    refitted any time this is called.\n\n    Args:\n        real_data: Real data to which the synthetic data may be compared. In many cases this will be data used\n            to TRAIN the model that generated the synthetic data, but not always.\n        synthetic_data: Synthetically generated data whose quality is to be assessed.\n        holdout_data: An optional set of holdout data. Typically, this will be data drawn from the same\n            distribution as ``real_data`` but was explicitly NOT used to train the model that generated\n            ``synthetic_data``. Not all metrics will require a holdout set and it is, therefore optional.\n            Defaults to None.\n\n    Returns:\n        Transformed dataframes for the real, synthetic, and holdout dataframes (if provided)\n    \"\"\"\n    log(INFO, \"Performing default preprocessing using defined columns.\")\n    encoder = SynthEvalDataframeEncoding(\n        real_data, synthetic_data, self.categorical_columns, self.numerical_columns\n    )\n    real_data = encoder.encode(real_data)\n    synthetic_data = encoder.encode(synthetic_data)\n    holdout_data = encoder.encode(holdout_data) if holdout_data else None\n\n    if holdout_data is not None:\n        return real_data, synthetic_data, holdout_data\n    return real_data, synthetic_data\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.privacy","title":"privacy","text":""},{"location":"api/#midst_toolkit.evaluation.privacy.distance_closest_record","title":"distance_closest_record","text":""},{"location":"api/#midst_toolkit.evaluation.privacy.distance_closest_record.DistanceToClosestRecordScore","title":"DistanceToClosestRecordScore","text":"<p>               Bases: <code>MetricBase</code></p> Source code in <code>src/midst_toolkit/evaluation/privacy/distance_closest_record.py</code> <pre><code>class DistanceToClosestRecordScore(MetricBase):\n    def __init__(\n        self,\n        norm: NormType = NormType.L1,\n        batch_size: int = 1000,\n        device: torch.device = DEVICE,\n        meta_info: dict[str, Any] | None = None,\n        do_preprocess: bool = False,\n    ):\n        \"\"\"\n        A class to compute the distance to closest record (DCR) score for the ``synthetic_data``. Here, DCR is\n        defined as the distance between a synthetic datapoint and its nearest real datapoint. DCR equal to zero means\n        that the synthetic data is at a higher risk of privacy leakage, while higher DCR values mean less risk of\n        privacy leakage.\n\n        This class computes the DCR of each synthetic datapoint to real data points in two different sets:\n\n        - Training (real) data used to train the model that generated the synthetic data.\n        - Holdout (real) data from the same distribution as the training data but that was NOT used to train the model.\n\n        It returns the proportion of synthetic data points that are closer to the training dataset than the\n        holdout dataset. If the size of the training and holdout datasets are equal, this score should ideally be\n        indicating that the model has not over fit to training data and the synthetic data points are not memorized\n        copies of training data. If the size of the training and holdout datasets are different, the ideal value for\n        this score is # ``real_data`` / (# ``real_data`` + # ``holdout_data``).\n\n        Args:\n            norm: Determines what norm the distances are computed in. Defaults to NormType.L1.\n            batch_size: Batch size used to compute the DCR iteratively. Just needed to manage memory. Defaults to 1000.\n            device: What device the tensors should be sent to in order to perform the calculations. Defaults to DEVICE.\n            meta_info: This is only required/used if ``do_preprocess`` is True. JSON with meta information about the\n                columns and their corresponding types that should be considered. At minimum, it should have the keys\n                'num_col_idx', 'cat_col_idx', 'target_col_idx', and 'task_type'. If None, then no preprocessing is\n                expected to be done. Defaults to None.\n            do_preprocess: Whether or not to preprocess the dataframes before performing the DCR computations.\n                Preprocessing is performed with the ``preprocess`` function Defaults to False.\n        \"\"\"\n        self.norm = norm\n        self.batch_size = batch_size\n        self.device = device\n        self.do_preprocess = do_preprocess\n        if self.do_preprocess and meta_info is None:\n            raise ValueError(\"Preprocessing requires meta_info to be defined, but it is None.\")\n        self.meta_info = meta_info if meta_info is not None else {}\n\n    def compute(\n        self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame, holdout_data: pd.DataFrame | None = None\n    ) -&gt; dict[str, float]:\n        \"\"\"\n        Computes the Distance to closest record (DCR) score between the synthetic data and two reference datasets. The\n        ``real_data`` dataframe should represent TRAINING data for the model that generated the synthetic data\n        while the ``holdout_data`` dataframe should represent heldout data, ideally from the same distribution as\n        ``real_data``, that was NOT used to train that model.\n\n        Here, the DCR score is the ratio of synthetic points that are closer to real_data to the combined size of\n        ``real_data`` and ``holdout_data``. Ideally, this would be proportionally to randomly selecting a point\n        from the combined datasets (# ``real_data`` / (# ``real_data`` + # ``holdout_data``)).\n\n        NOTE: The dataframes provided need to be pre-processed into numerical values for each column in some way. That\n        is, for example, the categorical variables should be one-hot encoded and the numerical values normalized in\n        some way. This can be done via the ``preprocess`` function beforehand or it can be done within compute if\n        ``do_preprocess`` is True and ``meta_info`` has been provided.\n\n        Args:\n            real_data: Real data that was used to train the model that generated the ``synthetic_data``.\n            synthetic_data: Synthetic data generated by a model that was trained on ``real_data``.\n            holdout_data: Real data that was NOT used to train the generating model. Defaults to None.\n\n        Returns:\n            A dictionary containing the Distance to Closest Record Score in the ``dcr_score`` key. Example:\n            { \"dcr_score\": 0.79 }\n        \"\"\"\n        assert holdout_data is not None, \"For DCR score calculations, a holdout dataset is required\"\n\n        if self.do_preprocess:\n            synthetic_data, real_data, holdout_data = preprocess(\n                self.meta_info, synthetic_data, real_data, holdout_data\n            )\n\n        real_data_train_tensor = torch.tensor(real_data.to_numpy()).to(self.device)\n        real_data_test_tensor = torch.tensor(holdout_data.to_numpy()).to(self.device)\n        synthetic_data_tensor = torch.tensor(synthetic_data.to_numpy()).to(self.device)\n\n        dcr_train = []\n        dcr_test = []\n\n        # Assumes that the tensors are 2D and arranged (n_samples, data dimension)\n        for start_index in tqdm(range(0, synthetic_data_tensor.size(0), self.batch_size)):\n            end_index = min(start_index + self.batch_size, synthetic_data_tensor.size(0))\n            synthetic_data_batch = synthetic_data_tensor[start_index:end_index]\n\n            # Calculate distances for real and test data in smaller batches\n            dcr_train_batch = minimum_distances(\n                synthetic_data_batch, real_data_train_tensor, self.batch_size, self.norm\n            )\n            dcr_test_batch = minimum_distances(synthetic_data_batch, real_data_test_tensor, self.batch_size, self.norm)\n\n            dcr_train.append(dcr_train_batch)\n            dcr_test.append(dcr_test_batch)\n\n        dcr_train_torch = torch.cat(dcr_train)\n        dcr_test_torch = torch.cat(dcr_test)\n\n        records_closer_to_train = (dcr_train_torch &lt; dcr_test_torch).long().sum()\n\n        score = records_closer_to_train / dcr_train_torch.shape[0]\n        log(INFO, f\"Distance to Closest Record Score = {score}\")\n        return {\"dcr_score\": score.item()}\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(\n    norm=NormType.L1,\n    batch_size=1000,\n    device=DEVICE,\n    meta_info=None,\n    do_preprocess=False,\n)\n</code></pre> <p>A class to compute the distance to closest record (DCR) score for the <code>synthetic_data</code>. Here, DCR is defined as the distance between a synthetic datapoint and its nearest real datapoint. DCR equal to zero means that the synthetic data is at a higher risk of privacy leakage, while higher DCR values mean less risk of privacy leakage.</p> <p>This class computes the DCR of each synthetic datapoint to real data points in two different sets:</p> <ul> <li>Training (real) data used to train the model that generated the synthetic data.</li> <li>Holdout (real) data from the same distribution as the training data but that was NOT used to train the model.</li> </ul> <p>It returns the proportion of synthetic data points that are closer to the training dataset than the holdout dataset. If the size of the training and holdout datasets are equal, this score should ideally be indicating that the model has not over fit to training data and the synthetic data points are not memorized copies of training data. If the size of the training and holdout datasets are different, the ideal value for this score is # <code>real_data</code> / (# <code>real_data</code> + # <code>holdout_data</code>).</p> <p>Parameters:</p> Name Type Description Default <code>norm</code> <code>NormType</code> <p>Determines what norm the distances are computed in. Defaults to NormType.L1.</p> <code>L1</code> <code>batch_size</code> <code>int</code> <p>Batch size used to compute the DCR iteratively. Just needed to manage memory. Defaults to 1000.</p> <code>1000</code> <code>device</code> <code>device</code> <p>What device the tensors should be sent to in order to perform the calculations. Defaults to DEVICE.</p> <code>DEVICE</code> <code>meta_info</code> <code>dict[str, Any] | None</code> <p>This is only required/used if <code>do_preprocess</code> is True. JSON with meta information about the columns and their corresponding types that should be considered. At minimum, it should have the keys 'num_col_idx', 'cat_col_idx', 'target_col_idx', and 'task_type'. If None, then no preprocessing is expected to be done. Defaults to None.</p> <code>None</code> <code>do_preprocess</code> <code>bool</code> <p>Whether or not to preprocess the dataframes before performing the DCR computations. Preprocessing is performed with the <code>preprocess</code> function Defaults to False.</p> <code>False</code> Source code in <code>src/midst_toolkit/evaluation/privacy/distance_closest_record.py</code> <pre><code>def __init__(\n    self,\n    norm: NormType = NormType.L1,\n    batch_size: int = 1000,\n    device: torch.device = DEVICE,\n    meta_info: dict[str, Any] | None = None,\n    do_preprocess: bool = False,\n):\n    \"\"\"\n    A class to compute the distance to closest record (DCR) score for the ``synthetic_data``. Here, DCR is\n    defined as the distance between a synthetic datapoint and its nearest real datapoint. DCR equal to zero means\n    that the synthetic data is at a higher risk of privacy leakage, while higher DCR values mean less risk of\n    privacy leakage.\n\n    This class computes the DCR of each synthetic datapoint to real data points in two different sets:\n\n    - Training (real) data used to train the model that generated the synthetic data.\n    - Holdout (real) data from the same distribution as the training data but that was NOT used to train the model.\n\n    It returns the proportion of synthetic data points that are closer to the training dataset than the\n    holdout dataset. If the size of the training and holdout datasets are equal, this score should ideally be\n    indicating that the model has not over fit to training data and the synthetic data points are not memorized\n    copies of training data. If the size of the training and holdout datasets are different, the ideal value for\n    this score is # ``real_data`` / (# ``real_data`` + # ``holdout_data``).\n\n    Args:\n        norm: Determines what norm the distances are computed in. Defaults to NormType.L1.\n        batch_size: Batch size used to compute the DCR iteratively. Just needed to manage memory. Defaults to 1000.\n        device: What device the tensors should be sent to in order to perform the calculations. Defaults to DEVICE.\n        meta_info: This is only required/used if ``do_preprocess`` is True. JSON with meta information about the\n            columns and their corresponding types that should be considered. At minimum, it should have the keys\n            'num_col_idx', 'cat_col_idx', 'target_col_idx', and 'task_type'. If None, then no preprocessing is\n            expected to be done. Defaults to None.\n        do_preprocess: Whether or not to preprocess the dataframes before performing the DCR computations.\n            Preprocessing is performed with the ``preprocess`` function Defaults to False.\n    \"\"\"\n    self.norm = norm\n    self.batch_size = batch_size\n    self.device = device\n    self.do_preprocess = do_preprocess\n    if self.do_preprocess and meta_info is None:\n        raise ValueError(\"Preprocessing requires meta_info to be defined, but it is None.\")\n    self.meta_info = meta_info if meta_info is not None else {}\n</code></pre> <code></code> compute \u00b6 <pre><code>compute(real_data, synthetic_data, holdout_data=None)\n</code></pre> <p>Computes the Distance to closest record (DCR) score between the synthetic data and two reference datasets. The <code>real_data</code> dataframe should represent TRAINING data for the model that generated the synthetic data while the <code>holdout_data</code> dataframe should represent heldout data, ideally from the same distribution as <code>real_data</code>, that was NOT used to train that model.</p> <p>Here, the DCR score is the ratio of synthetic points that are closer to real_data to the combined size of <code>real_data</code> and <code>holdout_data</code>. Ideally, this would be proportionally to randomly selecting a point from the combined datasets (# <code>real_data</code> / (# <code>real_data</code> + # <code>holdout_data</code>)).</p> <p>NOTE: The dataframes provided need to be pre-processed into numerical values for each column in some way. That is, for example, the categorical variables should be one-hot encoded and the numerical values normalized in some way. This can be done via the <code>preprocess</code> function beforehand or it can be done within compute if <code>do_preprocess</code> is True and <code>meta_info</code> has been provided.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>DataFrame</code> <p>Real data that was used to train the model that generated the <code>synthetic_data</code>.</p> required <code>synthetic_data</code> <code>DataFrame</code> <p>Synthetic data generated by a model that was trained on <code>real_data</code>.</p> required <code>holdout_data</code> <code>DataFrame | None</code> <p>Real data that was NOT used to train the generating model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dictionary containing the Distance to Closest Record Score in the <code>dcr_score</code> key. Example:</p> <code>dict[str, float]</code> <p>{ \"dcr_score\": 0.79 }</p> Source code in <code>src/midst_toolkit/evaluation/privacy/distance_closest_record.py</code> <pre><code>def compute(\n    self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame, holdout_data: pd.DataFrame | None = None\n) -&gt; dict[str, float]:\n    \"\"\"\n    Computes the Distance to closest record (DCR) score between the synthetic data and two reference datasets. The\n    ``real_data`` dataframe should represent TRAINING data for the model that generated the synthetic data\n    while the ``holdout_data`` dataframe should represent heldout data, ideally from the same distribution as\n    ``real_data``, that was NOT used to train that model.\n\n    Here, the DCR score is the ratio of synthetic points that are closer to real_data to the combined size of\n    ``real_data`` and ``holdout_data``. Ideally, this would be proportionally to randomly selecting a point\n    from the combined datasets (# ``real_data`` / (# ``real_data`` + # ``holdout_data``)).\n\n    NOTE: The dataframes provided need to be pre-processed into numerical values for each column in some way. That\n    is, for example, the categorical variables should be one-hot encoded and the numerical values normalized in\n    some way. This can be done via the ``preprocess`` function beforehand or it can be done within compute if\n    ``do_preprocess`` is True and ``meta_info`` has been provided.\n\n    Args:\n        real_data: Real data that was used to train the model that generated the ``synthetic_data``.\n        synthetic_data: Synthetic data generated by a model that was trained on ``real_data``.\n        holdout_data: Real data that was NOT used to train the generating model. Defaults to None.\n\n    Returns:\n        A dictionary containing the Distance to Closest Record Score in the ``dcr_score`` key. Example:\n        { \"dcr_score\": 0.79 }\n    \"\"\"\n    assert holdout_data is not None, \"For DCR score calculations, a holdout dataset is required\"\n\n    if self.do_preprocess:\n        synthetic_data, real_data, holdout_data = preprocess(\n            self.meta_info, synthetic_data, real_data, holdout_data\n        )\n\n    real_data_train_tensor = torch.tensor(real_data.to_numpy()).to(self.device)\n    real_data_test_tensor = torch.tensor(holdout_data.to_numpy()).to(self.device)\n    synthetic_data_tensor = torch.tensor(synthetic_data.to_numpy()).to(self.device)\n\n    dcr_train = []\n    dcr_test = []\n\n    # Assumes that the tensors are 2D and arranged (n_samples, data dimension)\n    for start_index in tqdm(range(0, synthetic_data_tensor.size(0), self.batch_size)):\n        end_index = min(start_index + self.batch_size, synthetic_data_tensor.size(0))\n        synthetic_data_batch = synthetic_data_tensor[start_index:end_index]\n\n        # Calculate distances for real and test data in smaller batches\n        dcr_train_batch = minimum_distances(\n            synthetic_data_batch, real_data_train_tensor, self.batch_size, self.norm\n        )\n        dcr_test_batch = minimum_distances(synthetic_data_batch, real_data_test_tensor, self.batch_size, self.norm)\n\n        dcr_train.append(dcr_train_batch)\n        dcr_test.append(dcr_test_batch)\n\n    dcr_train_torch = torch.cat(dcr_train)\n    dcr_test_torch = torch.cat(dcr_test)\n\n    records_closer_to_train = (dcr_train_torch &lt; dcr_test_torch).long().sum()\n\n    score = records_closer_to_train / dcr_train_torch.shape[0]\n    log(INFO, f\"Distance to Closest Record Score = {score}\")\n    return {\"dcr_score\": score.item()}\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.privacy.distance_closest_record.MedianDistanceToClosestRecordScore","title":"MedianDistanceToClosestRecordScore","text":"<p>               Bases: <code>MetricBase</code></p> Source code in <code>src/midst_toolkit/evaluation/privacy/distance_closest_record.py</code> <pre><code>class MedianDistanceToClosestRecordScore(MetricBase):\n    def __init__(\n        self,\n        norm: NormType = NormType.L1,\n        batch_size: int = 1000,\n        device: torch.device = DEVICE,\n        meta_info: dict[str, Any] | None = None,\n        do_preprocess: bool = False,\n    ):\n        \"\"\"\n        A metric to compute the Median Distance to Closest Record (Median DCR) metric as described in:\n        https://arxiv.org/pdf/2404.15821.\n\n        First, the minimum distance from points in the synthetic data to the real data are computed. Next the minimum\n        inter-record distances from the real training data with itself are calculated. The ratio of the median minimum\n        distance for synthetic to real data to the median minimum distance of real to real is returned.\n\n        Args:\n            norm: Determines what norm the distances are computed in. Defaults to NormType.L1.\n            batch_size: Batch size used to compute the DCR iteratively. Just needed to manage memory. Defaults to 1000.\n            device: What device the tensors should be sent to in order to perform the calculations. Defaults to DEVICE.\n            meta_info: This is only required/used if ``do_preprocess`` is True. JSON with meta information about the\n                columns and their corresponding types that should be considered. At minimum, it should have the keys\n                'num_col_idx', 'cat_col_idx', 'target_col_idx', and 'task_type'. If None, then no preprocessing is\n                expected to be done. Defaults to None.\n            do_preprocess: Whether or not to preprocess the dataframes before performing the DCR computations.\n                Preprocessing is performed with the ``preprocess`` function Defaults to False.\n        \"\"\"\n        self.norm = norm\n        self.batch_size = batch_size\n        self.device = device\n        self.do_preprocess = do_preprocess\n        if self.do_preprocess and meta_info is None:\n            raise ValueError(\"Preprocessing requires meta_info to be defined, but it is None.\")\n        self.meta_info = meta_info if meta_info is not None else {}\n\n    def compute(self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame) -&gt; dict[str, float]:\n        \"\"\"\n        Implementing the Median Distance to Closest Record (Median DCR) metric as described in:\n        https://arxiv.org/pdf/2404.15821.\n\n        First, the minimum distance from points in the synthetic data to the real data are computed. Next the minimum\n        inter-record distances from the real training data with itself are calculated. The ratio of the median minimum\n        distance for synthetic to real data to the median minimum distance of real to real is returned.\n\n        NOTE: The dataframes provided need to be pre-processed into numerical values for each column in some way. That\n        is, for example, the categorical variables should be one-hot encoded and the numerical values normalized in\n        some way. This can be done via the ``preprocess`` function beforehand or it can be done within compute if\n        ``do_preprocess`` is True and ``meta_info`` has been provided.\n\n        Args:\n            real_data: Dataframe containing real data that was used to train the model that generated the provided\n                synthetic data. This dataframe should already have been preprocessed as in the note above.\n            synthetic_data: Dataframe containing synthetically generated data for which we want to derive a DCR score.\n                This dataframe should already have been preprocessed as in the note above.\n\n        Returns:\n            A dictionary containing the Median Distance to Closest Record Score in the ``median_dcr_score`` key.\n            Example: { \"median_dcr_score\": 0.79 }\n        \"\"\"\n        if self.do_preprocess:\n            synthetic_data, real_data = preprocess(self.meta_info, synthetic_data, real_data)\n\n        real_data_tensor = torch.tensor(real_data.to_numpy()).to(self.device)\n        synthetic_data_tensor = torch.tensor(synthetic_data.to_numpy()).to(self.device)\n\n        dcr_synthetic_to_real = []\n        dcr_real_to_real = []\n\n        # Assumes that the tensors are 2D and arranged (n_samples, data dimension)\n        for start_index in tqdm(range(0, synthetic_data_tensor.size(0), self.batch_size)):\n            end_index = min(start_index + self.batch_size, synthetic_data_tensor.size(0))\n            synthetic_data_batch = synthetic_data_tensor[start_index:end_index]\n\n            # Calculate distances for synthetic data to real data in smaller batches\n            dcr_synthetic_to_real_batch = minimum_distances(\n                synthetic_data_batch, real_data_tensor, self.batch_size, self.norm, skip_diagonal=False\n            )\n            dcr_synthetic_to_real.append(dcr_synthetic_to_real_batch)\n\n        # Assumes that the tensors are 2D and arranged (n_samples, data dimension)\n        for start_index in tqdm(range(0, real_data_tensor.size(0), self.batch_size)):\n            end_index = min(start_index + self.batch_size, real_data_tensor.size(0))\n            real_data_batch = real_data_tensor[start_index:end_index]\n\n            # Calculate distances for synthetic data to real data in smaller batches\n            dcr_real_to_real_batch = minimum_distances(\n                real_data_batch, real_data_tensor, self.batch_size, self.norm, skip_diagonal=True\n            )\n            dcr_real_to_real.append(dcr_real_to_real_batch)\n\n        dcr_synthetic_to_real_torch = torch.cat(dcr_synthetic_to_real)\n        dcr_real_to_real_torch = torch.cat(dcr_real_to_real)\n\n        median_dcr_score = (\n            torch.median(dcr_synthetic_to_real_torch).item() / torch.median(dcr_real_to_real_torch).item()\n        )\n        return {\"median_dcr_score\": median_dcr_score}\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(\n    norm=NormType.L1,\n    batch_size=1000,\n    device=DEVICE,\n    meta_info=None,\n    do_preprocess=False,\n)\n</code></pre> <p>A metric to compute the Median Distance to Closest Record (Median DCR) metric as described in: https://arxiv.org/pdf/2404.15821.</p> <p>First, the minimum distance from points in the synthetic data to the real data are computed. Next the minimum inter-record distances from the real training data with itself are calculated. The ratio of the median minimum distance for synthetic to real data to the median minimum distance of real to real is returned.</p> <p>Parameters:</p> Name Type Description Default <code>norm</code> <code>NormType</code> <p>Determines what norm the distances are computed in. Defaults to NormType.L1.</p> <code>L1</code> <code>batch_size</code> <code>int</code> <p>Batch size used to compute the DCR iteratively. Just needed to manage memory. Defaults to 1000.</p> <code>1000</code> <code>device</code> <code>device</code> <p>What device the tensors should be sent to in order to perform the calculations. Defaults to DEVICE.</p> <code>DEVICE</code> <code>meta_info</code> <code>dict[str, Any] | None</code> <p>This is only required/used if <code>do_preprocess</code> is True. JSON with meta information about the columns and their corresponding types that should be considered. At minimum, it should have the keys 'num_col_idx', 'cat_col_idx', 'target_col_idx', and 'task_type'. If None, then no preprocessing is expected to be done. Defaults to None.</p> <code>None</code> <code>do_preprocess</code> <code>bool</code> <p>Whether or not to preprocess the dataframes before performing the DCR computations. Preprocessing is performed with the <code>preprocess</code> function Defaults to False.</p> <code>False</code> Source code in <code>src/midst_toolkit/evaluation/privacy/distance_closest_record.py</code> <pre><code>def __init__(\n    self,\n    norm: NormType = NormType.L1,\n    batch_size: int = 1000,\n    device: torch.device = DEVICE,\n    meta_info: dict[str, Any] | None = None,\n    do_preprocess: bool = False,\n):\n    \"\"\"\n    A metric to compute the Median Distance to Closest Record (Median DCR) metric as described in:\n    https://arxiv.org/pdf/2404.15821.\n\n    First, the minimum distance from points in the synthetic data to the real data are computed. Next the minimum\n    inter-record distances from the real training data with itself are calculated. The ratio of the median minimum\n    distance for synthetic to real data to the median minimum distance of real to real is returned.\n\n    Args:\n        norm: Determines what norm the distances are computed in. Defaults to NormType.L1.\n        batch_size: Batch size used to compute the DCR iteratively. Just needed to manage memory. Defaults to 1000.\n        device: What device the tensors should be sent to in order to perform the calculations. Defaults to DEVICE.\n        meta_info: This is only required/used if ``do_preprocess`` is True. JSON with meta information about the\n            columns and their corresponding types that should be considered. At minimum, it should have the keys\n            'num_col_idx', 'cat_col_idx', 'target_col_idx', and 'task_type'. If None, then no preprocessing is\n            expected to be done. Defaults to None.\n        do_preprocess: Whether or not to preprocess the dataframes before performing the DCR computations.\n            Preprocessing is performed with the ``preprocess`` function Defaults to False.\n    \"\"\"\n    self.norm = norm\n    self.batch_size = batch_size\n    self.device = device\n    self.do_preprocess = do_preprocess\n    if self.do_preprocess and meta_info is None:\n        raise ValueError(\"Preprocessing requires meta_info to be defined, but it is None.\")\n    self.meta_info = meta_info if meta_info is not None else {}\n</code></pre> <code></code> compute \u00b6 <pre><code>compute(real_data, synthetic_data)\n</code></pre> <p>Implementing the Median Distance to Closest Record (Median DCR) metric as described in: https://arxiv.org/pdf/2404.15821.</p> <p>First, the minimum distance from points in the synthetic data to the real data are computed. Next the minimum inter-record distances from the real training data with itself are calculated. The ratio of the median minimum distance for synthetic to real data to the median minimum distance of real to real is returned.</p> <p>NOTE: The dataframes provided need to be pre-processed into numerical values for each column in some way. That is, for example, the categorical variables should be one-hot encoded and the numerical values normalized in some way. This can be done via the <code>preprocess</code> function beforehand or it can be done within compute if <code>do_preprocess</code> is True and <code>meta_info</code> has been provided.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>DataFrame</code> <p>Dataframe containing real data that was used to train the model that generated the provided synthetic data. This dataframe should already have been preprocessed as in the note above.</p> required <code>synthetic_data</code> <code>DataFrame</code> <p>Dataframe containing synthetically generated data for which we want to derive a DCR score. This dataframe should already have been preprocessed as in the note above.</p> required <p>Returns:</p> Name Type Description <code>dict[str, float]</code> <p>A dictionary containing the Median Distance to Closest Record Score in the <code>median_dcr_score</code> key.</p> <code>Example</code> <code>dict[str, float]</code> <p>{ \"median_dcr_score\": 0.79 }</p> Source code in <code>src/midst_toolkit/evaluation/privacy/distance_closest_record.py</code> <pre><code>def compute(self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame) -&gt; dict[str, float]:\n    \"\"\"\n    Implementing the Median Distance to Closest Record (Median DCR) metric as described in:\n    https://arxiv.org/pdf/2404.15821.\n\n    First, the minimum distance from points in the synthetic data to the real data are computed. Next the minimum\n    inter-record distances from the real training data with itself are calculated. The ratio of the median minimum\n    distance for synthetic to real data to the median minimum distance of real to real is returned.\n\n    NOTE: The dataframes provided need to be pre-processed into numerical values for each column in some way. That\n    is, for example, the categorical variables should be one-hot encoded and the numerical values normalized in\n    some way. This can be done via the ``preprocess`` function beforehand or it can be done within compute if\n    ``do_preprocess`` is True and ``meta_info`` has been provided.\n\n    Args:\n        real_data: Dataframe containing real data that was used to train the model that generated the provided\n            synthetic data. This dataframe should already have been preprocessed as in the note above.\n        synthetic_data: Dataframe containing synthetically generated data for which we want to derive a DCR score.\n            This dataframe should already have been preprocessed as in the note above.\n\n    Returns:\n        A dictionary containing the Median Distance to Closest Record Score in the ``median_dcr_score`` key.\n        Example: { \"median_dcr_score\": 0.79 }\n    \"\"\"\n    if self.do_preprocess:\n        synthetic_data, real_data = preprocess(self.meta_info, synthetic_data, real_data)\n\n    real_data_tensor = torch.tensor(real_data.to_numpy()).to(self.device)\n    synthetic_data_tensor = torch.tensor(synthetic_data.to_numpy()).to(self.device)\n\n    dcr_synthetic_to_real = []\n    dcr_real_to_real = []\n\n    # Assumes that the tensors are 2D and arranged (n_samples, data dimension)\n    for start_index in tqdm(range(0, synthetic_data_tensor.size(0), self.batch_size)):\n        end_index = min(start_index + self.batch_size, synthetic_data_tensor.size(0))\n        synthetic_data_batch = synthetic_data_tensor[start_index:end_index]\n\n        # Calculate distances for synthetic data to real data in smaller batches\n        dcr_synthetic_to_real_batch = minimum_distances(\n            synthetic_data_batch, real_data_tensor, self.batch_size, self.norm, skip_diagonal=False\n        )\n        dcr_synthetic_to_real.append(dcr_synthetic_to_real_batch)\n\n    # Assumes that the tensors are 2D and arranged (n_samples, data dimension)\n    for start_index in tqdm(range(0, real_data_tensor.size(0), self.batch_size)):\n        end_index = min(start_index + self.batch_size, real_data_tensor.size(0))\n        real_data_batch = real_data_tensor[start_index:end_index]\n\n        # Calculate distances for synthetic data to real data in smaller batches\n        dcr_real_to_real_batch = minimum_distances(\n            real_data_batch, real_data_tensor, self.batch_size, self.norm, skip_diagonal=True\n        )\n        dcr_real_to_real.append(dcr_real_to_real_batch)\n\n    dcr_synthetic_to_real_torch = torch.cat(dcr_synthetic_to_real)\n    dcr_real_to_real_torch = torch.cat(dcr_real_to_real)\n\n    median_dcr_score = (\n        torch.median(dcr_synthetic_to_real_torch).item() / torch.median(dcr_real_to_real_torch).item()\n    )\n    return {\"median_dcr_score\": median_dcr_score}\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.privacy.distance_closest_record.preprocess","title":"preprocess","text":"<pre><code>preprocess(\n    meta_info: dict[str, Any],\n    synthetic_data: DataFrame,\n    real_data_train: DataFrame,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]\n</code></pre><pre><code>preprocess(\n    meta_info: dict[str, Any],\n    synthetic_data: DataFrame,\n    real_data_train: DataFrame,\n    real_data_test: DataFrame,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\n</code></pre> <pre><code>preprocess(\n    meta_info,\n    synthetic_data,\n    real_data_train,\n    real_data_test=None,\n)\n</code></pre> <p>This function performs preprocessing on Pandas dataframes to prepare for computation of the distance to closest record score. Specifically, this function filters the provided raw dataframes to the appropriate numerical and categorical columns based on the information of the <code>meta_info</code> JSON. For the numerical columns, it normalizes values by the distance between the largest and smallest value of each column of the <code>real_data_train</code> numerical values. The categorical columns are processed into one-hot encoding columns, where the transformation is fitted on the concatenation of columns from each dataset.</p> <p>Parameters:</p> Name Type Description Default <code>meta_info</code> <code>dict[str, Any]</code> <p>JSON with meta information about the columns and their corresponding types that should be considered.</p> required <code>synthetic_data</code> <code>DataFrame</code> <p>Dataframe containing all synthetically generated data.</p> required <code>real_data_train</code> <code>DataFrame</code> <p>Dataframe containing the real training data associated with the model that generated the <code>synthetic_data</code>.</p> required <code>real_data_test</code> <code>DataFrame | None</code> <p>Dataframe containing the real test data. It's important that this data was not seen by the model that generated <code>synthetic_data</code> during training. If None, then it will, of course, not be preprocessed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame, DataFrame] | tuple[DataFrame, DataFrame]</code> <p>Processed Pandas dataframes with the synthetic data, real data for training, real data for testing if it was</p> <code>tuple[DataFrame, DataFrame, DataFrame] | tuple[DataFrame, DataFrame]</code> <p>provided.</p> Source code in <code>src/midst_toolkit/evaluation/privacy/distance_closest_record.py</code> <pre><code>def preprocess(\n    meta_info: dict[str, Any],\n    synthetic_data: pd.DataFrame,\n    real_data_train: pd.DataFrame,\n    real_data_test: pd.DataFrame | None = None,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame] | tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    This function performs preprocessing on Pandas dataframes to prepare for computation of the distance to closest\n    record score. Specifically, this function filters the provided raw dataframes to the appropriate numerical and\n    categorical columns based on the information of the ``meta_info`` JSON. For the numerical columns, it normalizes\n    values by the distance between the largest and smallest value of each column of the ``real_data_train`` numerical\n    values. The categorical columns are processed into one-hot encoding columns, where the transformation is fitted\n    on the concatenation of columns from each dataset.\n\n    Args:\n        meta_info: JSON with meta information about the columns and their corresponding types that should be\n            considered.\n        synthetic_data: Dataframe containing all synthetically generated data.\n        real_data_train: Dataframe containing the real training data associated with the model that generated the\n            ``synthetic_data``.\n        real_data_test: Dataframe containing the real test data. It's important that this data was not seen by the\n            model that generated ``synthetic_data`` during training. If None, then it will, of course, not be\n            preprocessed. Defaults to None.\n\n    Returns:\n        Processed Pandas dataframes with the synthetic data, real data for training, real data for testing if it was\n        provided.\n    \"\"\"\n    numerical_synthetic_data, categorical_synthetic_data = extract_columns_based_on_meta_info(\n        synthetic_data, meta_info\n    )\n    numerical_real_data_train, categorical_real_data_train = extract_columns_based_on_meta_info(\n        real_data_train, meta_info\n    )\n\n    numerical_ranges = [\n        numerical_real_data_train[index].max() - numerical_real_data_train[index].min()\n        for index in numerical_real_data_train.columns\n    ]\n    numerical_ranges_np = np.array(numerical_ranges)\n\n    num_synthetic_data_np = numerical_synthetic_data.to_numpy()\n    num_real_data_train_np = numerical_real_data_train.to_numpy()\n\n    # Normalize the values of the numerical columns of the different datasets by the ranges of the train set.\n    num_synthetic_data_np = num_synthetic_data_np / numerical_ranges_np\n    num_real_data_train_np = num_real_data_train_np / numerical_ranges_np\n\n    cat_synthetic_data_np = categorical_synthetic_data.to_numpy().astype(\"str\")\n    cat_real_data_train_np = categorical_real_data_train.to_numpy().astype(\"str\")\n\n    if real_data_test is not None:\n        numerical_real_data_test, categorical_real_data_test = extract_columns_based_on_meta_info(\n            real_data_test, meta_info\n        )\n        num_real_data_test_np = numerical_real_data_test.to_numpy()\n        # Normalize the values of the numerical columns of the different datasets by the ranges of the train set.\n        num_real_data_test_np = num_real_data_test_np / numerical_ranges_np\n        cat_real_data_test_np = categorical_real_data_test.to_numpy().astype(\"str\")\n    else:\n        num_real_data_test_np, cat_real_data_test_np = None, None\n\n    if categorical_real_data_train.shape[1] &gt; 0:\n        encoder = OneHotEncoder()\n        if cat_real_data_test_np is not None:\n            encoder.fit(np.concatenate((cat_synthetic_data_np, cat_real_data_train_np, cat_real_data_test_np), axis=0))\n        else:\n            encoder.fit(np.concatenate((cat_synthetic_data_np, cat_real_data_train_np), axis=0))\n\n        cat_synthetic_data_oh = encoder.transform(cat_synthetic_data_np).toarray()\n        cat_real_data_train_oh = encoder.transform(cat_real_data_train_np).toarray()\n        if cat_real_data_test_np is not None:\n            cat_real_data_test_oh = encoder.transform(cat_real_data_test_np).toarray()\n\n    else:\n        cat_synthetic_data_oh = np.empty((categorical_synthetic_data.shape[0], 0))\n        cat_real_data_train_oh = np.empty((categorical_real_data_train.shape[0], 0))\n        if categorical_real_data_test is not None:\n            cat_real_data_test_oh = np.empty((categorical_real_data_test.shape[0], 0))\n\n    processed_real_data_train = pd.DataFrame(\n        np.concatenate((num_real_data_train_np, cat_real_data_train_oh), axis=1)\n    ).astype(float)\n    processed_synthetic_data = pd.DataFrame(\n        np.concatenate((num_synthetic_data_np, cat_synthetic_data_oh), axis=1)\n    ).astype(float)\n\n    if real_data_test is None:\n        return (processed_synthetic_data, processed_real_data_train)\n    return (\n        processed_synthetic_data,\n        processed_real_data_train,\n        pd.DataFrame(np.concatenate((num_real_data_test_np, cat_real_data_test_oh), axis=1)).astype(float),\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.privacy.distance_utils","title":"distance_utils","text":""},{"location":"api/#midst_toolkit.evaluation.privacy.distance_utils.compute_l1_distance","title":"compute_l1_distance","text":"<pre><code>compute_l1_distance(\n    target_data, reference_data, skip_diagonal=False\n)\n</code></pre> <p>Compute the smallest l1 distance between each point in the target data tensor compared to all points in the reference data tensor.</p> <p>Parameters:</p> Name Type Description Default <code>target_data</code> <code>Tensor</code> <p>Tensor of target data. Assumed to be a 2D tensor with batch size first, followed by data dimension.</p> required <code>reference_data</code> <code>Tensor</code> <p>Tensor of reference data. Assumed to be a 2D tensor with batch size first, followed by data dimension.</p> required <code>skip_diagonal</code> <code>bool</code> <p>Whether or not to skip computations on diagonal of distance matrix. This is generally only used when <code>target_data</code> and <code>reference_data</code> are the same set. In this case, the diagonal elements are the distance of the point from itself (which is 0). Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A 1D tensor containing the l1 minimum distances between each data point in the target data and all points in</p> <code>Tensor</code> <p>the reference data. Order will be the same as the target data.</p> Source code in <code>src/midst_toolkit/evaluation/privacy/distance_utils.py</code> <pre><code>def compute_l1_distance(\n    target_data: torch.Tensor, reference_data: torch.Tensor, skip_diagonal: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the smallest l1 distance between each point in the target data tensor compared to all points in the\n    reference data tensor.\n\n    Args:\n        target_data: Tensor of target data. Assumed to be a 2D tensor with batch size first, followed by\n            data dimension.\n        reference_data: Tensor of reference data. Assumed to be a 2D tensor with batch size first, followed by\n            data dimension.\n        skip_diagonal: Whether or not to skip computations on diagonal of distance matrix. This is generally only used\n            when ``target_data`` and ``reference_data`` are the same set. In this case, the diagonal elements are the\n            distance of the point from itself (which is 0). Defaults to False.\n\n    Returns:\n        A 1D tensor containing the l1 minimum distances between each data point in the target data and all points in\n        the reference data. Order will be the same as the target data.\n    \"\"\"\n    assert target_data.ndim == 2 and reference_data.ndim == 2, \"Target and Reference data tensors should be 2D\"\n    assert target_data.shape[1] == reference_data.shape[1], \"Data dimensions do not match for the provided tensors\"\n\n    # For target_data (n_target_points, data_dim), and reference_data (n_ref_points, data_dim), this subtracts\n    # every point in reference_data from every point in target_data to create a tensor of shape\n    # (n_target_points, n_ref_points, data_dim).\n    point_differences = target_data[:, None] - reference_data\n    distances = (point_differences).abs().sum(dim=2)\n\n    # Minimum distance of points in n_target_points compared to all other points in reference_data.\n    if not skip_diagonal:\n        min_batch_distances, _ = distances.min(dim=1)\n        return min_batch_distances\n\n    # Bottom two distances, because one of them might be the reference point to itself.\n    min_batch_distances, _ = torch.topk(distances, 2, dim=1, largest=False)\n    return min_batch_distances\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.privacy.distance_utils.compute_l2_distance","title":"compute_l2_distance","text":"<pre><code>compute_l2_distance(\n    target_data, reference_data, skip_diagonal=False\n)\n</code></pre> <p>Compute the smallest l2 distance between each point in the target data tensor compared to all points in the reference data tensor.</p> <p>Parameters:</p> Name Type Description Default <code>target_data</code> <code>Tensor</code> <p>Tensor of target data. Assumed to be a 2D tensor with batch size first, followed by data dimension.</p> required <code>reference_data</code> <code>Tensor</code> <p>Tensor of reference data. Assumed to be a 2D tensor with batch size first, followed by data dimension.</p> required <code>skip_diagonal</code> <code>bool</code> <p>Whether or not to skip computations on diagonal of distance matrix. This is generally only used when <code>target_data</code> and <code>reference_data</code> are the same set. In this case, the diagonal elements are the distance of the point from itself (which is 0). Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A 1D tensor containing the l2 minimum distances between each data point in the target data and all points in</p> <code>Tensor</code> <p>the reference data. Order will be the same as the target data.</p> Source code in <code>src/midst_toolkit/evaluation/privacy/distance_utils.py</code> <pre><code>def compute_l2_distance(\n    target_data: torch.Tensor, reference_data: torch.Tensor, skip_diagonal: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the smallest l2 distance between each point in the target data tensor compared to all points in the\n    reference data tensor.\n\n    Args:\n        target_data: Tensor of target data. Assumed to be a 2D tensor with batch size first, followed by\n            data dimension.\n        reference_data: Tensor of reference data. Assumed to be a 2D tensor with batch size first, followed by\n            data dimension.\n        skip_diagonal: Whether or not to skip computations on diagonal of distance matrix. This is generally only used\n            when ``target_data`` and ``reference_data`` are the same set. In this case, the diagonal elements are the\n            distance of the point from itself (which is 0). Defaults to False.\n\n    Returns:\n        A 1D tensor containing the l2 minimum distances between each data point in the target data and all points in\n        the reference data. Order will be the same as the target data.\n    \"\"\"\n    assert target_data.ndim == 2 and reference_data.ndim == 2, \"Target and Reference data tensors should be 2D\"\n    assert target_data.shape[1] == reference_data.shape[1], \"Data dimensions do not match for the provided tensors\"\n    # For target_data (n_target_points, data_dim), and reference_data (n_reference_points, data_dim), this subtracts\n    # every point in reference_data from every point in target_data to create a tensor of shape\n    # (n_target_points, n_reference_points, data_dim).\n    point_differences = target_data[:, None] - reference_data\n    distances = torch.sqrt(torch.pow(point_differences, 2.0).sum(dim=2))\n\n    # Minimum distance of points in n_target_points compared to all other points in reference_data.\n    if not skip_diagonal:\n        min_batch_distances, _ = distances.min(dim=1)\n        return min_batch_distances\n\n    # Bottom two distances, because one of them might be the reference point to itself.\n    min_batch_distances, _ = torch.topk(distances, 2, dim=1, largest=False)\n    return min_batch_distances\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.privacy.distance_utils.minimum_distances","title":"minimum_distances","text":"<pre><code>minimum_distances(\n    target_data,\n    reference_data,\n    batch_size=None,\n    norm=NormType.L1,\n    skip_diagonal=False,\n)\n</code></pre> <p>Function to calculate minimum distances between each point in the target data to those of the reference data provided. This can be done in batches if specified. Otherwise, the entire computation is done at once.</p> <p>Parameters:</p> Name Type Description Default <code>target_data</code> <code>Tensor</code> <p>The complete set of target data, stacked as a tensor with shape (n_samples, data dimension).</p> required <code>reference_data</code> <code>Tensor</code> <p>The complete set of reference data, stacked as a tensor with shape (n_samples, data dimension).</p> required <code>batch_size</code> <code>int | None</code> <p>Size of the batches to facilitate computing the minimum distances, if specified. Defaults to None.</p> <code>None</code> <code>norm</code> <code>NormType</code> <p>Which type of norm to use as the distance metric. Defaults to NormType.L1.</p> <code>L1</code> <code>skip_diagonal</code> <code>bool</code> <p>Whether or not to skip computations on diagonal of distance matrix. This is generally only used when <code>target_data</code> and <code>reference_data</code> are the same set. In this case, the diagonal elements are the distance of the point from itself (which is 0). Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A 1D tensor with the minimum distances. Should be of length n_samples. Order will be the same as</p> <code>Tensor</code> <p><code>target_data.</code></p> Source code in <code>src/midst_toolkit/evaluation/privacy/distance_utils.py</code> <pre><code>def minimum_distances(\n    target_data: torch.Tensor,\n    reference_data: torch.Tensor,\n    batch_size: int | None = None,\n    norm: NormType = NormType.L1,\n    skip_diagonal: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Function to calculate minimum distances between each point in the target data to those of the reference data\n    provided. This can be done in batches if specified. Otherwise, the entire computation is done at once.\n\n    Args:\n        target_data: The complete set of target data, stacked as a tensor with shape (n_samples, data dimension).\n        reference_data: The complete set of reference data, stacked as a tensor with shape (n_samples, data dimension).\n        batch_size: Size of the batches to facilitate computing the minimum distances, if specified. Defaults to None.\n        norm: Which type of norm to use as the distance metric. Defaults to NormType.L1.\n        skip_diagonal: Whether or not to skip computations on diagonal of distance matrix. This is generally only used\n            when ``target_data`` and ``reference_data`` are the same set. In this case, the diagonal elements are the\n            distance of the point from itself (which is 0). Defaults to False.\n\n    Returns:\n        A 1D tensor with the minimum distances. Should be of length n_samples. Order will be the same as\n        ``target_data.``\n    \"\"\"\n    if batch_size is None:\n        # If batch size isn't specified, do it all at once.\n        batch_size = target_data.size(0)\n\n    # Create a minimum distance for each target data sample\n    if skip_diagonal:\n        min_distances = torch.full((target_data.size(0), 2), float(\"inf\"), device=target_data.device)\n    else:\n        min_distances = torch.full((target_data.size(0),), float(\"inf\"), device=target_data.device)\n\n    # Iterate through the reference data in batches and compute distances\n    for start_index in range(0, reference_data.size(0), batch_size):\n        end_index = min(start_index + batch_size, reference_data.size(0))\n        reference_data_batch = reference_data[start_index:end_index]\n\n        if norm is NormType.L1:\n            min_batch_distances = compute_l1_distance(target_data, reference_data_batch, skip_diagonal)\n        elif norm is NormType.L2:\n            min_batch_distances = compute_l2_distance(target_data, reference_data_batch, skip_diagonal)\n        else:\n            raise ValueError(f\"Unrecognized norm type: {str(norm)}\")\n        if not skip_diagonal:\n            min_distances = torch.minimum(min_distances, min_batch_distances)\n        else:\n            combined_distances = torch.cat((min_distances, min_batch_distances), dim=1)\n            min_distances, _ = torch.topk(combined_distances, 2, dim=1, largest=False)\n    if skip_diagonal:\n        # Smallest distance should be point to itself. Second smallest is the rest.\n        return min_distances[:, 1]\n    return min_distances\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.privacy.mia_metrics","title":"mia_metrics","text":""},{"location":"api/#midst_toolkit.evaluation.privacy.mia_metrics.MembershipInferenceMetrics","title":"MembershipInferenceMetrics","text":"Source code in <code>src/midst_toolkit/evaluation/privacy/mia_metrics.py</code> <pre><code>class MembershipInferenceMetrics:\n    def __init__(self, metrics_to_compute: set[MiaMetrics] | None = None, fpr_thresholds: list[float] | None = None):\n        \"\"\"\n        Metrics class to compute all (or subset) of the metrics associated with membership inference attacks. This\n        class allows for specification of which metrics you want to compute and to provide a set of false positive\n        rate threshold, which are used in computing some of the metrics, such as TPR at FPR.\n\n        Args:\n            metrics_to_compute: An enum list that specifies which metrics to compute. If None, then all available\n                metrics are scheduled to be computed. Defaults to None.\n            fpr_thresholds: A set of thresholds at which to measure TPR. This only matters if computing TRP @ FPR\n                metrics. If not specified a default set of thresholds is used defined by DEFAULT_FPR_THRESHOLDS.\n                Defaults to None.\n        \"\"\"\n        self.fpr_thresholds = fpr_thresholds if fpr_thresholds else DEFAULT_FPR_THRESHOLDS\n        self.metrics_to_compute = metrics_to_compute if metrics_to_compute else DEFAULT_METRICS_TO_COMPUTE\n        self.metrics = self._map_enum_to_metrics()\n        self.computed = False\n\n    def _map_enum_to_metrics(self) -&gt; list[AttackScore]:\n        \"\"\"\n        Converts the list of enums specifying which metrics to use into objects that will compute the metrics.\n\n        Returns:\n            A list of metrics objects to be used for computing the desired metrics.\n        \"\"\"\n        metrics: list[AttackScore] = []\n        if MiaMetrics.BALANCED_ACCURACY in self.metrics_to_compute:\n            metrics.append(BalancedAccuracy())\n        if MiaMetrics.AUC in self.metrics_to_compute:\n            metrics.append(Auc())\n        if MiaMetrics.MIA_SCORE in self.metrics_to_compute:\n            metrics.append(MiaScore())\n        if MiaMetrics.TPR_AT_FPR in self.metrics_to_compute:\n            metrics.append(TprAtFpr(self.fpr_thresholds))\n\n        tpr_fpr_intersection = self.metrics_to_compute.intersection({MiaMetrics.TPR, MiaMetrics.FPR})\n        if len(tpr_fpr_intersection) &gt; 0:\n            metrics.append(TprFpr(tpr_fpr_intersection))\n        return metrics\n\n    def compute(self, true_membership: np.ndarray, predicted_membership: np.ndarray) -&gt; None:\n        \"\"\"\n        Calculates MIA scores based on true membership and predictions. The set of metrics computed depends on the\n        user specified values in metrics_to_compute. Options include true positive rate (TPR) at various fixed values\n        of false positive rate (FPR), ROC/AUC, the best accuracy, and an MIA score (max(TPR - FPR) across various\n        prediction thresholds).\n\n        Args:\n            true_membership: An array of values in {0,1} of shape (n_samples, ) indicating the membership of a\n                challenge point. 0: \"non-member\", 1: \"member\".\n            predicted_membership: An array of values in the range [0,1] of shape (n_samples, ) indicating the\n                confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the\n                predictor is about the hypothesis that the  challenge point is a member.\n        \"\"\"\n        self.computed = True\n        for metric in self.metrics:\n            metric.compute(true_membership, predicted_membership)\n\n    def to_dict(self) -&gt; dict[str, float | np.ndarray]:\n        \"\"\"\n        Converts to computed metrics into a dictionary for processing or reporting.\n\n        Returns:\n            A dictionary keyed by the name of the metric and either a metric score or set of scores in the form of a\n            numpy array.\n        \"\"\"\n        assert self.computed, \"Compute has not been run for this metrics class. This must be run first.\"\n        mia_metrics: dict[str, float | np.ndarray] = {}\n        for metric in self.metrics:\n            metric_dict = metric.to_dict()\n            mia_metrics.update(metric_dict)\n\n        return mia_metrics\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(metrics_to_compute=None, fpr_thresholds=None)\n</code></pre> <p>Metrics class to compute all (or subset) of the metrics associated with membership inference attacks. This class allows for specification of which metrics you want to compute and to provide a set of false positive rate threshold, which are used in computing some of the metrics, such as TPR at FPR.</p> <p>Parameters:</p> Name Type Description Default <code>metrics_to_compute</code> <code>set[MiaMetrics] | None</code> <p>An enum list that specifies which metrics to compute. If None, then all available metrics are scheduled to be computed. Defaults to None.</p> <code>None</code> <code>fpr_thresholds</code> <code>list[float] | None</code> <p>A set of thresholds at which to measure TPR. This only matters if computing TRP @ FPR metrics. If not specified a default set of thresholds is used defined by DEFAULT_FPR_THRESHOLDS. Defaults to None.</p> <code>None</code> Source code in <code>src/midst_toolkit/evaluation/privacy/mia_metrics.py</code> <pre><code>def __init__(self, metrics_to_compute: set[MiaMetrics] | None = None, fpr_thresholds: list[float] | None = None):\n    \"\"\"\n    Metrics class to compute all (or subset) of the metrics associated with membership inference attacks. This\n    class allows for specification of which metrics you want to compute and to provide a set of false positive\n    rate threshold, which are used in computing some of the metrics, such as TPR at FPR.\n\n    Args:\n        metrics_to_compute: An enum list that specifies which metrics to compute. If None, then all available\n            metrics are scheduled to be computed. Defaults to None.\n        fpr_thresholds: A set of thresholds at which to measure TPR. This only matters if computing TRP @ FPR\n            metrics. If not specified a default set of thresholds is used defined by DEFAULT_FPR_THRESHOLDS.\n            Defaults to None.\n    \"\"\"\n    self.fpr_thresholds = fpr_thresholds if fpr_thresholds else DEFAULT_FPR_THRESHOLDS\n    self.metrics_to_compute = metrics_to_compute if metrics_to_compute else DEFAULT_METRICS_TO_COMPUTE\n    self.metrics = self._map_enum_to_metrics()\n    self.computed = False\n</code></pre> <code></code> compute \u00b6 <pre><code>compute(true_membership, predicted_membership)\n</code></pre> <p>Calculates MIA scores based on true membership and predictions. The set of metrics computed depends on the user specified values in metrics_to_compute. Options include true positive rate (TPR) at various fixed values of false positive rate (FPR), ROC/AUC, the best accuracy, and an MIA score (max(TPR - FPR) across various prediction thresholds).</p> <p>Parameters:</p> Name Type Description Default <code>true_membership</code> <code>ndarray</code> <p>An array of values in {0,1} of shape (n_samples, ) indicating the membership of a challenge point. 0: \"non-member\", 1: \"member\".</p> required <code>predicted_membership</code> <code>ndarray</code> <p>An array of values in the range [0,1] of shape (n_samples, ) indicating the confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the predictor is about the hypothesis that the  challenge point is a member.</p> required Source code in <code>src/midst_toolkit/evaluation/privacy/mia_metrics.py</code> <pre><code>def compute(self, true_membership: np.ndarray, predicted_membership: np.ndarray) -&gt; None:\n    \"\"\"\n    Calculates MIA scores based on true membership and predictions. The set of metrics computed depends on the\n    user specified values in metrics_to_compute. Options include true positive rate (TPR) at various fixed values\n    of false positive rate (FPR), ROC/AUC, the best accuracy, and an MIA score (max(TPR - FPR) across various\n    prediction thresholds).\n\n    Args:\n        true_membership: An array of values in {0,1} of shape (n_samples, ) indicating the membership of a\n            challenge point. 0: \"non-member\", 1: \"member\".\n        predicted_membership: An array of values in the range [0,1] of shape (n_samples, ) indicating the\n            confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the\n            predictor is about the hypothesis that the  challenge point is a member.\n    \"\"\"\n    self.computed = True\n    for metric in self.metrics:\n        metric.compute(true_membership, predicted_membership)\n</code></pre> <code></code> to_dict \u00b6 <pre><code>to_dict()\n</code></pre> <p>Converts to computed metrics into a dictionary for processing or reporting.</p> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>A dictionary keyed by the name of the metric and either a metric score or set of scores in the form of a</p> <code>dict[str, float | ndarray]</code> <p>numpy array.</p> Source code in <code>src/midst_toolkit/evaluation/privacy/mia_metrics.py</code> <pre><code>def to_dict(self) -&gt; dict[str, float | np.ndarray]:\n    \"\"\"\n    Converts to computed metrics into a dictionary for processing or reporting.\n\n    Returns:\n        A dictionary keyed by the name of the metric and either a metric score or set of scores in the form of a\n        numpy array.\n    \"\"\"\n    assert self.computed, \"Compute has not been run for this metrics class. This must be run first.\"\n    mia_metrics: dict[str, float | np.ndarray] = {}\n    for metric in self.metrics:\n        metric_dict = metric.to_dict()\n        mia_metrics.update(metric_dict)\n\n    return mia_metrics\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.privacy.mia_scoring","title":"mia_scoring","text":""},{"location":"api/#midst_toolkit.evaluation.privacy.mia_scoring.AttackScore","title":"AttackScore","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>class AttackScore(ABC):\n    @abstractmethod\n    def compute(self, true_membership: np.ndarray, predicted_membership: np.ndarray) -&gt; None:\n        \"\"\"\n        Compute function converting the provided labels and prediction into a score or set of scores.\n\n        Args:\n            true_membership: An array of values in {0,1} of shape (n_samples, ) indicating the membership of a\n                challenge point. 0: \"non-member\", 1: \"member\".\n            predicted_membership: An array of values in the range [0,1] of shape (n_samples, ) indicating the\n                confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the\n                predictor is about the hypothesis that the  challenge point is a member.\n\n        Raises:\n            NotImplementedError: Must be implemented by inheriting classes.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def to_dict(self) -&gt; dict[str, float | np.ndarray]:\n        \"\"\"\n        Converts the computed metrics into a dictionary for processing or reporting.\n\n        Raises:\n            NotImplementedError: To be implemented by inheriting class\n\n        Returns:\n            A dictionary keyed by the name of the metric and either a metric score or set of scores in the form of a\n            numpy array.\n        \"\"\"\n        raise NotImplementedError\n</code></pre> <code></code> compute <code>abstractmethod</code> \u00b6 <pre><code>compute(true_membership, predicted_membership)\n</code></pre> <p>Compute function converting the provided labels and prediction into a score or set of scores.</p> <p>Parameters:</p> Name Type Description Default <code>true_membership</code> <code>ndarray</code> <p>An array of values in {0,1} of shape (n_samples, ) indicating the membership of a challenge point. 0: \"non-member\", 1: \"member\".</p> required <code>predicted_membership</code> <code>ndarray</code> <p>An array of values in the range [0,1] of shape (n_samples, ) indicating the confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the predictor is about the hypothesis that the  challenge point is a member.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented by inheriting classes.</p> Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>@abstractmethod\ndef compute(self, true_membership: np.ndarray, predicted_membership: np.ndarray) -&gt; None:\n    \"\"\"\n    Compute function converting the provided labels and prediction into a score or set of scores.\n\n    Args:\n        true_membership: An array of values in {0,1} of shape (n_samples, ) indicating the membership of a\n            challenge point. 0: \"non-member\", 1: \"member\".\n        predicted_membership: An array of values in the range [0,1] of shape (n_samples, ) indicating the\n            confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the\n            predictor is about the hypothesis that the  challenge point is a member.\n\n    Raises:\n        NotImplementedError: Must be implemented by inheriting classes.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> <code></code> to_dict <code>abstractmethod</code> \u00b6 <pre><code>to_dict()\n</code></pre> <p>Converts the computed metrics into a dictionary for processing or reporting.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>To be implemented by inheriting class</p> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>A dictionary keyed by the name of the metric and either a metric score or set of scores in the form of a</p> <code>dict[str, float | ndarray]</code> <p>numpy array.</p> Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>@abstractmethod\ndef to_dict(self) -&gt; dict[str, float | np.ndarray]:\n    \"\"\"\n    Converts the computed metrics into a dictionary for processing or reporting.\n\n    Raises:\n        NotImplementedError: To be implemented by inheriting class\n\n    Returns:\n        A dictionary keyed by the name of the metric and either a metric score or set of scores in the form of a\n        numpy array.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.privacy.mia_scoring.MiaScore","title":"MiaScore","text":"<p>               Bases: <code>AttackScore</code></p> Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>class MiaScore(AttackScore):\n    def __init__(self, score_key: str = MiaMetrics.MIA_SCORE.value):\n        \"\"\"\n        Class for computing an MIA score. This score is the maximum difference between TPR and FPR across the\n        TPR and FPR curves.\n\n        Args:\n            score_key: Metric name/key to use in the metric dictionary. Defaults to MiaMetrics.MIA_SCORE.value.\n        \"\"\"\n        self.score_key = score_key\n        self.score: float | None = None\n\n    def compute(self, true_membership: np.ndarray, predicted_membership: np.ndarray) -&gt; None:\n        \"\"\"\n        Compute the MIA score. This score is the maximum difference between TPR and FPR across the TPR and FPR curves.\n\n        Args:\n            true_membership: An array of values in {0,1} of shape (n_samples, ) indicating the membership of a\n                challenge point. 0: \"non-member\", 1: \"member\".\n            predicted_membership: An array of values in the range [0,1] of shape (n_samples, ) indicating the\n                confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the\n                predictor is about the hypothesis that the  challenge point is a member.\n        \"\"\"\n        fpr, tpr, _ = roc_curve(true_membership, predicted_membership)\n        self.score = np.max(tpr - fpr)\n\n    def to_dict(self) -&gt; dict[str, float | np.ndarray]:\n        \"\"\"\n        Converts the computed MIA score into a dictionary for processing or reporting.\n\n        Returns:\n            A dictionary keyed by the name of the metric and the computed MIA score\n        \"\"\"\n        assert self.score is not None, \"Score is None, compute may not have been called yet.\"\n        return {self.score_key: self.score}\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(score_key=MiaMetrics.MIA_SCORE.value)\n</code></pre> <p>Class for computing an MIA score. This score is the maximum difference between TPR and FPR across the TPR and FPR curves.</p> <p>Parameters:</p> Name Type Description Default <code>score_key</code> <code>str</code> <p>Metric name/key to use in the metric dictionary. Defaults to MiaMetrics.MIA_SCORE.value.</p> <code>MIA_SCORE.value</code> Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>def __init__(self, score_key: str = MiaMetrics.MIA_SCORE.value):\n    \"\"\"\n    Class for computing an MIA score. This score is the maximum difference between TPR and FPR across the\n    TPR and FPR curves.\n\n    Args:\n        score_key: Metric name/key to use in the metric dictionary. Defaults to MiaMetrics.MIA_SCORE.value.\n    \"\"\"\n    self.score_key = score_key\n    self.score: float | None = None\n</code></pre> <code></code> compute \u00b6 <pre><code>compute(true_membership, predicted_membership)\n</code></pre> <p>Compute the MIA score. This score is the maximum difference between TPR and FPR across the TPR and FPR curves.</p> <p>Parameters:</p> Name Type Description Default <code>true_membership</code> <code>ndarray</code> <p>An array of values in {0,1} of shape (n_samples, ) indicating the membership of a challenge point. 0: \"non-member\", 1: \"member\".</p> required <code>predicted_membership</code> <code>ndarray</code> <p>An array of values in the range [0,1] of shape (n_samples, ) indicating the confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the predictor is about the hypothesis that the  challenge point is a member.</p> required Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>def compute(self, true_membership: np.ndarray, predicted_membership: np.ndarray) -&gt; None:\n    \"\"\"\n    Compute the MIA score. This score is the maximum difference between TPR and FPR across the TPR and FPR curves.\n\n    Args:\n        true_membership: An array of values in {0,1} of shape (n_samples, ) indicating the membership of a\n            challenge point. 0: \"non-member\", 1: \"member\".\n        predicted_membership: An array of values in the range [0,1] of shape (n_samples, ) indicating the\n            confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the\n            predictor is about the hypothesis that the  challenge point is a member.\n    \"\"\"\n    fpr, tpr, _ = roc_curve(true_membership, predicted_membership)\n    self.score = np.max(tpr - fpr)\n</code></pre> <code></code> to_dict \u00b6 <pre><code>to_dict()\n</code></pre> <p>Converts the computed MIA score into a dictionary for processing or reporting.</p> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>A dictionary keyed by the name of the metric and the computed MIA score</p> Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>def to_dict(self) -&gt; dict[str, float | np.ndarray]:\n    \"\"\"\n    Converts the computed MIA score into a dictionary for processing or reporting.\n\n    Returns:\n        A dictionary keyed by the name of the metric and the computed MIA score\n    \"\"\"\n    assert self.score is not None, \"Score is None, compute may not have been called yet.\"\n    return {self.score_key: self.score}\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.privacy.mia_scoring.Auc","title":"Auc","text":"<p>               Bases: <code>AttackScore</code></p> Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>class Auc(AttackScore):\n    def __init__(self, score_key: str = MiaMetrics.AUC.value):\n        \"\"\"\n        Class for computing an AUC/ROC score.\n\n        Args:\n            score_key:  Metric name/key to use in the metric dictionary. Defaults to MiaMetrics.AUC.value.\n        \"\"\"\n        self.score_key = score_key\n        self.score: float | None = None\n\n    def compute(self, true_membership: np.ndarray, predicted_membership: np.ndarray) -&gt; None:\n        \"\"\"\n        Compute the AUC/ROC score for the provided labels and predictions.\n\n        Args:\n            true_membership: An array of values in {0,1} of shape (n_samples, ) indicating the membership of a\n                challenge point. 0: \"non-member\", 1: \"member\".\n            predicted_membership: An array of values in the range [0,1] of shape (n_samples, ) indicating the\n                confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the\n                predictor is about the hypothesis that the  challenge point is a member.\n        \"\"\"\n        self.score = roc_auc_score(true_membership, predicted_membership)\n\n    def to_dict(self) -&gt; dict[str, float | np.ndarray]:\n        \"\"\"\n        Converts the computed AUC/ROC into a dictionary for processing or reporting.\n\n        Returns:\n            A dictionary keyed by the name of the metric and the computed AUC/ROC score\n        \"\"\"\n        assert self.score is not None, \"Score is None, compute may not have been called yet.\"\n        return {self.score_key: self.score}\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(score_key=MiaMetrics.AUC.value)\n</code></pre> <p>Class for computing an AUC/ROC score.</p> <p>Parameters:</p> Name Type Description Default <code>score_key</code> <code>str</code> <p>Metric name/key to use in the metric dictionary. Defaults to MiaMetrics.AUC.value.</p> <code>AUC.value</code> Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>def __init__(self, score_key: str = MiaMetrics.AUC.value):\n    \"\"\"\n    Class for computing an AUC/ROC score.\n\n    Args:\n        score_key:  Metric name/key to use in the metric dictionary. Defaults to MiaMetrics.AUC.value.\n    \"\"\"\n    self.score_key = score_key\n    self.score: float | None = None\n</code></pre> <code></code> compute \u00b6 <pre><code>compute(true_membership, predicted_membership)\n</code></pre> <p>Compute the AUC/ROC score for the provided labels and predictions.</p> <p>Parameters:</p> Name Type Description Default <code>true_membership</code> <code>ndarray</code> <p>An array of values in {0,1} of shape (n_samples, ) indicating the membership of a challenge point. 0: \"non-member\", 1: \"member\".</p> required <code>predicted_membership</code> <code>ndarray</code> <p>An array of values in the range [0,1] of shape (n_samples, ) indicating the confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the predictor is about the hypothesis that the  challenge point is a member.</p> required Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>def compute(self, true_membership: np.ndarray, predicted_membership: np.ndarray) -&gt; None:\n    \"\"\"\n    Compute the AUC/ROC score for the provided labels and predictions.\n\n    Args:\n        true_membership: An array of values in {0,1} of shape (n_samples, ) indicating the membership of a\n            challenge point. 0: \"non-member\", 1: \"member\".\n        predicted_membership: An array of values in the range [0,1] of shape (n_samples, ) indicating the\n            confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the\n            predictor is about the hypothesis that the  challenge point is a member.\n    \"\"\"\n    self.score = roc_auc_score(true_membership, predicted_membership)\n</code></pre> <code></code> to_dict \u00b6 <pre><code>to_dict()\n</code></pre> <p>Converts the computed AUC/ROC into a dictionary for processing or reporting.</p> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>A dictionary keyed by the name of the metric and the computed AUC/ROC score</p> Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>def to_dict(self) -&gt; dict[str, float | np.ndarray]:\n    \"\"\"\n    Converts the computed AUC/ROC into a dictionary for processing or reporting.\n\n    Returns:\n        A dictionary keyed by the name of the metric and the computed AUC/ROC score\n    \"\"\"\n    assert self.score is not None, \"Score is None, compute may not have been called yet.\"\n    return {self.score_key: self.score}\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.privacy.mia_scoring.BalancedAccuracy","title":"BalancedAccuracy","text":"<p>               Bases: <code>AttackScore</code></p> Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>class BalancedAccuracy(AttackScore):\n    def __init__(self, score_key: str = MiaMetrics.BALANCED_ACCURACY.value):\n        \"\"\"\n        Class for computing an Balanced Accuracy score.\n\n        Args:\n            score_key:  Metric name/key to use in the metric dictionary. Defaults to\n                MiaMetrics.BALANCED_ACCURACY.value.\n        \"\"\"\n        self.score_key = score_key\n        self.score: float | None = None\n\n    def compute(self, true_membership: np.ndarray, predicted_membership: np.ndarray) -&gt; None:\n        \"\"\"\n        Compute the Balanced Accuracy score for the provided labels and predictions.\n\n        Args:\n            true_membership: An array of values in {0,1} of shape (n_samples, ) indicating the membership of a\n                challenge point. 0: \"non-member\", 1: \"member\".\n            predicted_membership: An array of values in the range [0,1] of shape (n_samples, ) indicating the\n                confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the\n                predictor is about the hypothesis that the  challenge point is a member.\n        \"\"\"\n        fpr, tpr, _ = roc_curve(true_membership, predicted_membership)\n        self.score = np.max(((1 - fpr) + tpr) / 2)\n\n    def to_dict(self) -&gt; dict[str, float | np.ndarray]:\n        \"\"\"\n        Converts the computed balanced accuracy score into a dictionary for processing or reporting.\n\n        Returns:\n            A dictionary keyed by the name of the metric and the computed balanced accuracy score\n        \"\"\"\n        assert self.score is not None, \"Score is None, compute may not have been called yet.\"\n        return {self.score_key: self.score}\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(score_key=MiaMetrics.BALANCED_ACCURACY.value)\n</code></pre> <p>Class for computing an Balanced Accuracy score.</p> <p>Parameters:</p> Name Type Description Default <code>score_key</code> <code>str</code> <p>Metric name/key to use in the metric dictionary. Defaults to MiaMetrics.BALANCED_ACCURACY.value.</p> <code>BALANCED_ACCURACY.value</code> Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>def __init__(self, score_key: str = MiaMetrics.BALANCED_ACCURACY.value):\n    \"\"\"\n    Class for computing an Balanced Accuracy score.\n\n    Args:\n        score_key:  Metric name/key to use in the metric dictionary. Defaults to\n            MiaMetrics.BALANCED_ACCURACY.value.\n    \"\"\"\n    self.score_key = score_key\n    self.score: float | None = None\n</code></pre> <code></code> compute \u00b6 <pre><code>compute(true_membership, predicted_membership)\n</code></pre> <p>Compute the Balanced Accuracy score for the provided labels and predictions.</p> <p>Parameters:</p> Name Type Description Default <code>true_membership</code> <code>ndarray</code> <p>An array of values in {0,1} of shape (n_samples, ) indicating the membership of a challenge point. 0: \"non-member\", 1: \"member\".</p> required <code>predicted_membership</code> <code>ndarray</code> <p>An array of values in the range [0,1] of shape (n_samples, ) indicating the confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the predictor is about the hypothesis that the  challenge point is a member.</p> required Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>def compute(self, true_membership: np.ndarray, predicted_membership: np.ndarray) -&gt; None:\n    \"\"\"\n    Compute the Balanced Accuracy score for the provided labels and predictions.\n\n    Args:\n        true_membership: An array of values in {0,1} of shape (n_samples, ) indicating the membership of a\n            challenge point. 0: \"non-member\", 1: \"member\".\n        predicted_membership: An array of values in the range [0,1] of shape (n_samples, ) indicating the\n            confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the\n            predictor is about the hypothesis that the  challenge point is a member.\n    \"\"\"\n    fpr, tpr, _ = roc_curve(true_membership, predicted_membership)\n    self.score = np.max(((1 - fpr) + tpr) / 2)\n</code></pre> <code></code> to_dict \u00b6 <pre><code>to_dict()\n</code></pre> <p>Converts the computed balanced accuracy score into a dictionary for processing or reporting.</p> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>A dictionary keyed by the name of the metric and the computed balanced accuracy score</p> Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>def to_dict(self) -&gt; dict[str, float | np.ndarray]:\n    \"\"\"\n    Converts the computed balanced accuracy score into a dictionary for processing or reporting.\n\n    Returns:\n        A dictionary keyed by the name of the metric and the computed balanced accuracy score\n    \"\"\"\n    assert self.score is not None, \"Score is None, compute may not have been called yet.\"\n    return {self.score_key: self.score}\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.privacy.mia_scoring.TprFpr","title":"TprFpr","text":"<p>               Bases: <code>AttackScore</code></p> Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>class TprFpr(AttackScore):\n    def __init__(\n        self,\n        config: set[MiaMetrics] | None = None,\n        fpr_score_key: str = MiaMetrics.FPR.value,\n        tpr_score_key: str = MiaMetrics.TPR.value,\n    ):\n        \"\"\"\n        Class for computing an the TPR and FPR curves score.\n\n        Args:\n            config: Whether to include one or both of FPR and TPR curves. If none, then both curves are computed and\n                returned. Defaults to None.\n            fpr_score_key:  Metric name/key to use in the metric dictionary for the FPR curves. Defaults to\n                MiaMetrics.FPR.value.\n            tpr_score_key:  Metric name/key to use in the metric dictionary for the TPR curves. Defaults to\n                MiaMetrics.TPR.value.\n        \"\"\"\n        self.config = config if config else {MiaMetrics.FPR, MiaMetrics.TPR}\n        assert len(self.config) &gt; 0, \"Configuration is empty.\"\n\n        self.fpr_score_key = fpr_score_key\n        self.tpr_score_key = tpr_score_key\n        self.fpr: np.ndarray | None = None\n        self.tpr: np.ndarray | None = None\n\n    def compute(self, true_membership: np.ndarray, predicted_membership: np.ndarray) -&gt; None:\n        \"\"\"\n        Compute the TPR and FPR curves for the provided labels and predictions.\n\n        Args:\n            true_membership: An array of values in {0,1} of shape (n_samples, ) indicating the membership of a\n                challenge point. 0: \"non-member\", 1: \"member\".\n            predicted_membership: An array of values in the range [0,1] of shape (n_samples, ) indicating the\n                confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the\n                predictor is about the hypothesis that the  challenge point is a member.\n        \"\"\"\n        self.fpr, self.tpr, _ = roc_curve(true_membership, predicted_membership)\n\n    def to_dict(self) -&gt; dict[str, float | np.ndarray]:\n        \"\"\"\n        Converts the computed TPR and FPR curves into a dictionary for processing or reporting.\n\n        Returns:\n            A dictionary keyed by the name of the TPR and FPR metrics and the numpy arrays describing the TPR and FPR\n            curves.\n        \"\"\"\n        assert self.fpr is not None, \"FPR is None, compute may not have been called yet.\"\n        assert self.tpr is not None, \"TPR is None, compute may not have been called yet.\"\n        results_dict: dict[str, float | np.ndarray] = {}\n        if MiaMetrics.FPR in self.config:\n            results_dict[self.fpr_score_key] = self.fpr\n        if MiaMetrics.TPR in self.config:\n            results_dict[self.tpr_score_key] = self.tpr\n        return results_dict\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(\n    config=None,\n    fpr_score_key=MiaMetrics.FPR.value,\n    tpr_score_key=MiaMetrics.TPR.value,\n)\n</code></pre> <p>Class for computing an the TPR and FPR curves score.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>set[MiaMetrics] | None</code> <p>Whether to include one or both of FPR and TPR curves. If none, then both curves are computed and returned. Defaults to None.</p> <code>None</code> <code>fpr_score_key</code> <code>str</code> <p>Metric name/key to use in the metric dictionary for the FPR curves. Defaults to MiaMetrics.FPR.value.</p> <code>FPR.value</code> <code>tpr_score_key</code> <code>str</code> <p>Metric name/key to use in the metric dictionary for the TPR curves. Defaults to MiaMetrics.TPR.value.</p> <code>TPR.value</code> Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>def __init__(\n    self,\n    config: set[MiaMetrics] | None = None,\n    fpr_score_key: str = MiaMetrics.FPR.value,\n    tpr_score_key: str = MiaMetrics.TPR.value,\n):\n    \"\"\"\n    Class for computing an the TPR and FPR curves score.\n\n    Args:\n        config: Whether to include one or both of FPR and TPR curves. If none, then both curves are computed and\n            returned. Defaults to None.\n        fpr_score_key:  Metric name/key to use in the metric dictionary for the FPR curves. Defaults to\n            MiaMetrics.FPR.value.\n        tpr_score_key:  Metric name/key to use in the metric dictionary for the TPR curves. Defaults to\n            MiaMetrics.TPR.value.\n    \"\"\"\n    self.config = config if config else {MiaMetrics.FPR, MiaMetrics.TPR}\n    assert len(self.config) &gt; 0, \"Configuration is empty.\"\n\n    self.fpr_score_key = fpr_score_key\n    self.tpr_score_key = tpr_score_key\n    self.fpr: np.ndarray | None = None\n    self.tpr: np.ndarray | None = None\n</code></pre> <code></code> compute \u00b6 <pre><code>compute(true_membership, predicted_membership)\n</code></pre> <p>Compute the TPR and FPR curves for the provided labels and predictions.</p> <p>Parameters:</p> Name Type Description Default <code>true_membership</code> <code>ndarray</code> <p>An array of values in {0,1} of shape (n_samples, ) indicating the membership of a challenge point. 0: \"non-member\", 1: \"member\".</p> required <code>predicted_membership</code> <code>ndarray</code> <p>An array of values in the range [0,1] of shape (n_samples, ) indicating the confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the predictor is about the hypothesis that the  challenge point is a member.</p> required Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>def compute(self, true_membership: np.ndarray, predicted_membership: np.ndarray) -&gt; None:\n    \"\"\"\n    Compute the TPR and FPR curves for the provided labels and predictions.\n\n    Args:\n        true_membership: An array of values in {0,1} of shape (n_samples, ) indicating the membership of a\n            challenge point. 0: \"non-member\", 1: \"member\".\n        predicted_membership: An array of values in the range [0,1] of shape (n_samples, ) indicating the\n            confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the\n            predictor is about the hypothesis that the  challenge point is a member.\n    \"\"\"\n    self.fpr, self.tpr, _ = roc_curve(true_membership, predicted_membership)\n</code></pre> <code></code> to_dict \u00b6 <pre><code>to_dict()\n</code></pre> <p>Converts the computed TPR and FPR curves into a dictionary for processing or reporting.</p> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>A dictionary keyed by the name of the TPR and FPR metrics and the numpy arrays describing the TPR and FPR</p> <code>dict[str, float | ndarray]</code> <p>curves.</p> Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>def to_dict(self) -&gt; dict[str, float | np.ndarray]:\n    \"\"\"\n    Converts the computed TPR and FPR curves into a dictionary for processing or reporting.\n\n    Returns:\n        A dictionary keyed by the name of the TPR and FPR metrics and the numpy arrays describing the TPR and FPR\n        curves.\n    \"\"\"\n    assert self.fpr is not None, \"FPR is None, compute may not have been called yet.\"\n    assert self.tpr is not None, \"TPR is None, compute may not have been called yet.\"\n    results_dict: dict[str, float | np.ndarray] = {}\n    if MiaMetrics.FPR in self.config:\n        results_dict[self.fpr_score_key] = self.fpr\n    if MiaMetrics.TPR in self.config:\n        results_dict[self.tpr_score_key] = self.tpr\n    return results_dict\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.privacy.mia_scoring.TprAtFpr","title":"TprAtFpr","text":"<p>               Bases: <code>AttackScore</code></p> Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>class TprAtFpr(AttackScore):\n    def __init__(self, fpr_thresholds: list[float]):\n        \"\"\"\n        Class for computing an the TPR at FPR thresholds for the provided set of thresholds.\n\n        Args:\n            fpr_thresholds: These are the various thresholds for FPR at which the TPR values will be measured.\n        \"\"\"\n        assert len(fpr_thresholds) &gt; 0, \"No FPR Thresholds specified, fpr_threshold list is empty\"\n        self.fpr_thresholds = fpr_thresholds\n        self.tpr_at_fprs: dict[str, float | np.ndarray] = {}\n\n    def compute(self, true_membership: np.ndarray, predicted_membership: np.ndarray) -&gt; None:\n        \"\"\"\n        Provided a set of challenge points, the goal of membership inference is to classify whether a given point was\n        part of the training set for the target model or not. This function calculates true positive rate (TPR) at a\n        given set of false positive rate (FPR) thresholds for membership inference.\n\n        Args:\n            true_membership: An array of values in {0,1} of shape (n_samples, ) indicating the membership of a\n                challenge point. 0: \"non-member\", 1: \"member\".\n            predicted_membership: An array of values in the range [0,1] of shape (n_samples, ) indicating the\n                confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the\n                predictor is about the hypothesis that the  challenge point is a member.\n        \"\"\"\n        assert len(true_membership) == len(predicted_membership), (\n            \"Membership predictions must be the same length as labels\"\n        )\n        assert np.all(predicted_membership &gt;= 0), \"Some predictions are &lt; 0\"\n        assert np.all(predicted_membership &lt;= 1), \"Some predictions are &gt; 1\"\n\n        fpr, tpr, _ = roc_curve(true_membership, predicted_membership)\n\n        for fpr_threshold in self.fpr_thresholds:\n            # Just shifting 4 significant digits to create a string (0.0025 -&gt; \"25\")\n            formatted_score_key = f\"TPR_FPR_{int(1e4 * fpr_threshold)}\"\n            fpr_measurements_below_threshold = fpr &lt; fpr_threshold\n            if np.any(fpr_measurements_below_threshold):\n                # If any FPRs are below threshold, return best TPR seen.\n                self.tpr_at_fprs[formatted_score_key] = max(tpr[fpr_measurements_below_threshold])\n            else:\n                self.tpr_at_fprs[formatted_score_key] = 0.0\n\n    def to_dict(self) -&gt; dict[str, float | np.ndarray]:\n        \"\"\"\n        Converts the computed TPR at FPR metrics at various thresholds into a dictionary for processing or reporting.\n\n        Returns:\n            A dictionary keyed by the name of the TPR at FPR for various threshold and the computed TPR values for\n            that threshold\n        \"\"\"\n        return self.tpr_at_fprs\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(fpr_thresholds)\n</code></pre> <p>Class for computing an the TPR at FPR thresholds for the provided set of thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>fpr_thresholds</code> <code>list[float]</code> <p>These are the various thresholds for FPR at which the TPR values will be measured.</p> required Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>def __init__(self, fpr_thresholds: list[float]):\n    \"\"\"\n    Class for computing an the TPR at FPR thresholds for the provided set of thresholds.\n\n    Args:\n        fpr_thresholds: These are the various thresholds for FPR at which the TPR values will be measured.\n    \"\"\"\n    assert len(fpr_thresholds) &gt; 0, \"No FPR Thresholds specified, fpr_threshold list is empty\"\n    self.fpr_thresholds = fpr_thresholds\n    self.tpr_at_fprs: dict[str, float | np.ndarray] = {}\n</code></pre> <code></code> compute \u00b6 <pre><code>compute(true_membership, predicted_membership)\n</code></pre> <p>Provided a set of challenge points, the goal of membership inference is to classify whether a given point was part of the training set for the target model or not. This function calculates true positive rate (TPR) at a given set of false positive rate (FPR) thresholds for membership inference.</p> <p>Parameters:</p> Name Type Description Default <code>true_membership</code> <code>ndarray</code> <p>An array of values in {0,1} of shape (n_samples, ) indicating the membership of a challenge point. 0: \"non-member\", 1: \"member\".</p> required <code>predicted_membership</code> <code>ndarray</code> <p>An array of values in the range [0,1] of shape (n_samples, ) indicating the confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the predictor is about the hypothesis that the  challenge point is a member.</p> required Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>def compute(self, true_membership: np.ndarray, predicted_membership: np.ndarray) -&gt; None:\n    \"\"\"\n    Provided a set of challenge points, the goal of membership inference is to classify whether a given point was\n    part of the training set for the target model or not. This function calculates true positive rate (TPR) at a\n    given set of false positive rate (FPR) thresholds for membership inference.\n\n    Args:\n        true_membership: An array of values in {0,1} of shape (n_samples, ) indicating the membership of a\n            challenge point. 0: \"non-member\", 1: \"member\".\n        predicted_membership: An array of values in the range [0,1] of shape (n_samples, ) indicating the\n            confidence that a challenge point is a member. The closer the value to 1, the more \"confident\" the\n            predictor is about the hypothesis that the  challenge point is a member.\n    \"\"\"\n    assert len(true_membership) == len(predicted_membership), (\n        \"Membership predictions must be the same length as labels\"\n    )\n    assert np.all(predicted_membership &gt;= 0), \"Some predictions are &lt; 0\"\n    assert np.all(predicted_membership &lt;= 1), \"Some predictions are &gt; 1\"\n\n    fpr, tpr, _ = roc_curve(true_membership, predicted_membership)\n\n    for fpr_threshold in self.fpr_thresholds:\n        # Just shifting 4 significant digits to create a string (0.0025 -&gt; \"25\")\n        formatted_score_key = f\"TPR_FPR_{int(1e4 * fpr_threshold)}\"\n        fpr_measurements_below_threshold = fpr &lt; fpr_threshold\n        if np.any(fpr_measurements_below_threshold):\n            # If any FPRs are below threshold, return best TPR seen.\n            self.tpr_at_fprs[formatted_score_key] = max(tpr[fpr_measurements_below_threshold])\n        else:\n            self.tpr_at_fprs[formatted_score_key] = 0.0\n</code></pre> <code></code> to_dict \u00b6 <pre><code>to_dict()\n</code></pre> <p>Converts the computed TPR at FPR metrics at various thresholds into a dictionary for processing or reporting.</p> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>A dictionary keyed by the name of the TPR at FPR for various threshold and the computed TPR values for</p> <code>dict[str, float | ndarray]</code> <p>that threshold</p> Source code in <code>src/midst_toolkit/evaluation/privacy/mia_scoring.py</code> <pre><code>def to_dict(self) -&gt; dict[str, float | np.ndarray]:\n    \"\"\"\n    Converts the computed TPR at FPR metrics at various thresholds into a dictionary for processing or reporting.\n\n    Returns:\n        A dictionary keyed by the name of the TPR at FPR for various threshold and the computed TPR values for\n        that threshold\n    \"\"\"\n    return self.tpr_at_fprs\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.privacy.mia_utils","title":"mia_utils","text":""},{"location":"api/#midst_toolkit.evaluation.privacy.mia_utils.generate_html","title":"generate_html","text":"<pre><code>generate_html(\n    scenario_scores, column_name_replacement=None\n)\n</code></pre> <p>Generates the HTML document as a string, containing the various detailed scores.</p> <p>Parameters:</p> Name Type Description Default <code>scenario_scores</code> <code>dict[str, dict[str, float | ndarray]]</code> <p>Scores for a set of scenarios. The outer dictionary is keyed by a scenario name and each inner  dictionary contains metrics for membership inference from the specific scenario. The keys of the inner dictionaries are the names of the metrics and values are the metric scores.</p> required <code>column_name_replacement</code> <code>dict[str, str] | None</code> <p>How the column names should be renamed in the dataframe. If None, it defaults to the renaming convention based on the default metrics in the scoring function. The keys of this dictionary need to match those of scores. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>An HTML document as a string which renders the scores in a more visual way.</p> Source code in <code>src/midst_toolkit/evaluation/privacy/mia_utils.py</code> <pre><code>def generate_html(\n    scenario_scores: dict[str, dict[str, float | np.ndarray]], column_name_replacement: dict[str, str] | None = None\n) -&gt; str:\n    \"\"\"\n    Generates the HTML document as a string, containing the various detailed scores.\n\n    Args:\n        scenario_scores: Scores for a set of scenarios. The outer dictionary is keyed by a scenario name and each\n            inner  dictionary contains metrics for membership inference from the specific scenario. The keys of the\n            inner dictionaries are the names of the metrics and values are the metric scores.\n        column_name_replacement: How the column names should be renamed in the dataframe. If None, it defaults to the\n            renaming convention based on the default metrics in the scoring function. The keys of this dictionary need\n            to match those of scores. Defaults to None.\n\n    Returns:\n        An HTML document as a string which renders the scores in a more visual way.\n    \"\"\"\n    column_name_replacement = column_name_replacement if column_name_replacement else DEFAULT_COLUMN_RENAME\n\n    imgs_html = \"\"\n    for scenario in scenario_scores:\n        fpr = scenario_scores[scenario][\"fpr\"]\n        tpr = scenario_scores[scenario][\"tpr\"]\n        assert isinstance(fpr, np.ndarray)\n        assert isinstance(tpr, np.ndarray)\n        fig = _generate_roc_figure(fpr, tpr)\n        fig.tight_layout(pad=1.0)\n\n        imgs_html = f\"{imgs_html}&lt;h2&gt;{scenario}&lt;/h2&gt;&lt;div&gt;{_image_to_html(fig)}&lt;/div&gt;\"\n\n    table = _generate_table(scenario_scores, column_name_replacement)\n    table_html = table.to_html(border=0, float_format=\"{:0.4f}\".format, escape=False)\n\n    with open(\"src/midst_toolkit/evaluation/privacy/index.html\", \"r\") as html_file:\n        html_string = html_file.read()\n        html_string = html_string.replace(\"{MIA_TABLES}\", table_html)\n        return html_string.replace(\"{MIA_SCENARIOS}\", imgs_html)\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.privacy.scripts","title":"scripts","text":""},{"location":"api/#midst_toolkit.evaluation.privacy.scripts.midst_mia_scoring","title":"midst_mia_scoring","text":"compute_mia_scores_across_scenarios \u00b6 <pre><code>compute_mia_scores_across_scenarios(\n    labels_scenarios_dir,\n    preds_scenarios_dir,\n    fpr_thresholds,\n)\n</code></pre> <p>Give a path to a set of scenario folders holding membership predictions and labels, this function runs scoring for all of the scenarios and stores the results in a dictionary keyed by the scenario folder name. The values of the dictionary are the sets of scores for the membership predictions keyed by the name of the metric. The folder structure at each of the paths is assumed to be //solutions.csv or predictions.csv. <p>Parameters:</p> Name Type Description Default <code>labels_scenarios_dir</code> <code>Path</code> <p>Directory containing scenario membership labels.</p> required <code>preds_scenarios_dir</code> <code>Path</code> <p>Directory containing scenario membership predictions.</p> required <code>fpr_thresholds</code> <code>list[float]</code> <p>false positive rate thresholds to measure TRP @ FPR metrics for the membership inference. For more details, see documentation in <code>compute_mia_metrics</code> and the sub-methods therein.</p> required <p>Returns:</p> Type Description <code>dict[str, dict[str, float | ndarray]]</code> <p>Scoring for the various scenarios folders in the provided directories. The dictionary is keyed by the</p> <code>dict[str, dict[str, float | ndarray]]</code> <p>scenario folder name. The values of the dictionary are the sets of scores for the membership predictions</p> <code>dict[str, dict[str, float | ndarray]]</code> <p>keyed by the name of the metric.</p> Source code in <code>src/midst_toolkit/evaluation/privacy/scripts/midst_mia_scoring.py</code> <pre><code>def compute_mia_scores_across_scenarios(\n    labels_scenarios_dir: Path, preds_scenarios_dir: Path, fpr_thresholds: list[float]\n) -&gt; dict[str, dict[str, float | np.ndarray]]:\n    \"\"\"\n    Give a path to a set of scenario folders holding membership predictions and labels, this function runs scoring\n    for all of the scenarios and stores the results in a dictionary keyed by the scenario folder name. The values\n    of the dictionary are the sets of scores for the membership predictions keyed by the name of the metric. The\n    folder structure at each of the paths is assumed to be &lt;scenario&gt;/&lt;model_id&gt;/solutions.csv or predictions.csv.\n\n    Args:\n        labels_scenarios_dir: Directory containing scenario membership labels.\n        preds_scenarios_dir: Directory containing scenario membership predictions.\n        fpr_thresholds: false positive rate thresholds to measure TRP @ FPR metrics for the membership inference. For\n            more details, see documentation in ``compute_mia_metrics`` and the sub-methods therein.\n\n    Returns:\n        Scoring for the various scenarios folders in the provided directories. The dictionary is keyed by the\n        scenario folder name. The values of the dictionary are the sets of scores for the membership predictions\n        keyed by the name of the metric.\n    \"\"\"\n    scenarios = set(os.listdir(labels_scenarios_dir))\n    assert scenarios == set(os.listdir(preds_scenarios_dir)), \"Label and Predictions scenario folders do not match.\"\n\n    log(INFO, f\"Processing Scenarios: {scenarios}\")\n\n    scores_by_scenario: dict[str, dict[str, float | np.ndarray]] = {}\n    for scenario in scenarios:\n        log(INFO, f\"Processing scenario: {scenario}\")\n\n        # By default, computing all metrics\n        metrics = MembershipInferenceMetrics(fpr_thresholds=fpr_thresholds)\n\n        scenario_labels = []\n        scenario_predictions = []\n\n        label_scenario_path = labels_scenarios_dir / scenario\n        prediction_scenario_path = preds_scenarios_dir / scenario\n\n        # NOTE: Performance is scored ACROSS model IDs within a scenario. That is, for each scenario, if you have\n        # multiple model ID folders, the predictions and labels are agglomerated and treated as ONE set of membership\n        # predictions for metrics computation\n        for model_id in os.listdir(label_scenario_path):\n            log(INFO, f\"Processing Model ID: {model_id}\")\n\n            label_file = label_scenario_path / model_id / \"solution.csv\"\n            prediction_file = prediction_scenario_path / model_id / \"prediction.csv\"\n\n            scenario_labels.append(np.loadtxt(label_file, delimiter=\",\"))\n            scenario_predictions.append(np.loadtxt(prediction_file, delimiter=\",\"))\n\n        # Grouping all predictions and labels ACROSS the model IDs for metric computation.\n        all_labels = np.concatenate(scenario_labels)\n        all_predictions = np.concatenate(scenario_predictions)\n\n        metrics.compute(all_labels, all_predictions)\n\n        scores = metrics.to_dict()\n        scores_by_scenario[scenario] = scores\n        log(INFO, f\"Scores for {scenario}: {scores}\")\n\n    return scores_by_scenario\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.quality","title":"quality","text":""},{"location":"api/#midst_toolkit.evaluation.quality.alpha_precision","title":"alpha_precision","text":""},{"location":"api/#midst_toolkit.evaluation.quality.alpha_precision.AlphaPrecision","title":"AlphaPrecision","text":"<p>               Bases: <code>MetricBase</code></p> Source code in <code>src/midst_toolkit/evaluation/quality/alpha_precision.py</code> <pre><code>class AlphaPrecision(MetricBase):\n    def __init__(self, naive_only: bool = True):\n        \"\"\"\n        Compute several quality metrics based on the Alpha Precision measure originally proposed in\n        https://arxiv.org/abs/2301.07573 comparing the quality of synthetically generated data to real data. The\n        implementation is based heavily on the Synthcity library (https://github.com/vanderschaarlab/synthcity).\n        Specifically, this class computes the alpha-precision, beta-recall, and authenticity scores between the two\n        datasets. If the ``naive_only`` boolean is True, then only the \"naive\" metrics are reported, i.e. metrics\n        with \"naive\" in their name. Naive scores are based on a set \"by-hand\" transformations rather than a classifier\n        embedding.\n\n        Args:\n            naive_only: Determines whether to report only the \"naive\" metrics for each of alpha-precision,\n            beta-recall, and authenticity scores. Defaults to True.\n        \"\"\"\n        self.naive_only = naive_only\n\n    def compute(self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame) -&gt; dict[str, float]:\n        \"\"\"\n        Computes the alpha-precision, beta-recall, and authenticity scores between the real and synthetic datasets.\n        If the ``naive_only`` boolean is True, then only the \"naive\" metrics are reported, i.e. metrics\n        with \"naive\" in their name. Naive scores are based on a set \"by-hand\" transformations rather than a classifier\n        embedding.\n\n        Args:\n            real_data: Real data that the synthetic data is meant to mimic/replace.\n            synthetic_data: Synthetic data to be compared against the provided real data.\n\n        Returns:\n            A dictionary containing the computed scores using the AlphaPrecision class in the Synthcity library.\n        \"\"\"\n        # Wrap the dataframes in a Synthcity compatible dataloader\n        real_data_loader = GenericDataLoader(real_data)\n        synthetic_data_loader = GenericDataLoader(synthetic_data)\n\n        quality_evaluator = SynthcityAlphaPrecision()\n        quality_results = quality_evaluator.evaluate(real_data_loader, synthetic_data_loader)\n\n        # Log results and filter to naive keys if requested\n        for metric_key, metric_value in quality_results.items():\n            log(INFO, f\"{metric_key}: {metric_value}\")\n            if self.naive_only and (NAIVE_METRIC_SUFFIX not in metric_key):\n                del quality_results[metric_key]\n\n        return quality_results\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(naive_only=True)\n</code></pre> <p>Compute several quality metrics based on the Alpha Precision measure originally proposed in https://arxiv.org/abs/2301.07573 comparing the quality of synthetically generated data to real data. The implementation is based heavily on the Synthcity library (https://github.com/vanderschaarlab/synthcity). Specifically, this class computes the alpha-precision, beta-recall, and authenticity scores between the two datasets. If the <code>naive_only</code> boolean is True, then only the \"naive\" metrics are reported, i.e. metrics with \"naive\" in their name. Naive scores are based on a set \"by-hand\" transformations rather than a classifier embedding.</p> <p>Parameters:</p> Name Type Description Default <code>naive_only</code> <code>bool</code> <p>Determines whether to report only the \"naive\" metrics for each of alpha-precision,</p> <code>True</code> Source code in <code>src/midst_toolkit/evaluation/quality/alpha_precision.py</code> <pre><code>def __init__(self, naive_only: bool = True):\n    \"\"\"\n    Compute several quality metrics based on the Alpha Precision measure originally proposed in\n    https://arxiv.org/abs/2301.07573 comparing the quality of synthetically generated data to real data. The\n    implementation is based heavily on the Synthcity library (https://github.com/vanderschaarlab/synthcity).\n    Specifically, this class computes the alpha-precision, beta-recall, and authenticity scores between the two\n    datasets. If the ``naive_only`` boolean is True, then only the \"naive\" metrics are reported, i.e. metrics\n    with \"naive\" in their name. Naive scores are based on a set \"by-hand\" transformations rather than a classifier\n    embedding.\n\n    Args:\n        naive_only: Determines whether to report only the \"naive\" metrics for each of alpha-precision,\n        beta-recall, and authenticity scores. Defaults to True.\n    \"\"\"\n    self.naive_only = naive_only\n</code></pre> <code></code> compute \u00b6 <pre><code>compute(real_data, synthetic_data)\n</code></pre> <p>Computes the alpha-precision, beta-recall, and authenticity scores between the real and synthetic datasets. If the <code>naive_only</code> boolean is True, then only the \"naive\" metrics are reported, i.e. metrics with \"naive\" in their name. Naive scores are based on a set \"by-hand\" transformations rather than a classifier embedding.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>DataFrame</code> <p>Real data that the synthetic data is meant to mimic/replace.</p> required <code>synthetic_data</code> <code>DataFrame</code> <p>Synthetic data to be compared against the provided real data.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dictionary containing the computed scores using the AlphaPrecision class in the Synthcity library.</p> Source code in <code>src/midst_toolkit/evaluation/quality/alpha_precision.py</code> <pre><code>def compute(self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame) -&gt; dict[str, float]:\n    \"\"\"\n    Computes the alpha-precision, beta-recall, and authenticity scores between the real and synthetic datasets.\n    If the ``naive_only`` boolean is True, then only the \"naive\" metrics are reported, i.e. metrics\n    with \"naive\" in their name. Naive scores are based on a set \"by-hand\" transformations rather than a classifier\n    embedding.\n\n    Args:\n        real_data: Real data that the synthetic data is meant to mimic/replace.\n        synthetic_data: Synthetic data to be compared against the provided real data.\n\n    Returns:\n        A dictionary containing the computed scores using the AlphaPrecision class in the Synthcity library.\n    \"\"\"\n    # Wrap the dataframes in a Synthcity compatible dataloader\n    real_data_loader = GenericDataLoader(real_data)\n    synthetic_data_loader = GenericDataLoader(synthetic_data)\n\n    quality_evaluator = SynthcityAlphaPrecision()\n    quality_results = quality_evaluator.evaluate(real_data_loader, synthetic_data_loader)\n\n    # Log results and filter to naive keys if requested\n    for metric_key, metric_value in quality_results.items():\n        log(INFO, f\"{metric_key}: {metric_value}\")\n        if self.naive_only and (NAIVE_METRIC_SUFFIX not in metric_key):\n            del quality_results[metric_key]\n\n    return quality_results\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.quality.confidence_interval_overlap","title":"confidence_interval_overlap","text":""},{"location":"api/#midst_toolkit.evaluation.quality.confidence_interval_overlap.MeanConfidenceInternalOverlap","title":"MeanConfidenceInternalOverlap","text":"<p>               Bases: <code>SynthEvalQualityMetric</code></p> Source code in <code>src/midst_toolkit/evaluation/quality/confidence_interval_overlap.py</code> <pre><code>class MeanConfidenceInternalOverlap(SynthEvalQualityMetric):\n    def __init__(\n        self,\n        categorical_columns: list[str],\n        numerical_columns: list[str],\n        do_preprocess: bool = False,\n        confidence_level: ConfidenceLevel = ConfidenceLevel.NinetyFive,\n    ) -&gt; None:\n        \"\"\"\n        This class computes the mean of the interval overlap percentages for the confidence intervals (CIs) of each\n        NUMERICAL column. The confidence intervals are interval estimates for the mean value of a particular column\n        Within each column the value is the average percentage of overlap for the real and synthetic column CIs.\n\n        More overlap is better.\n\n        For example:\n\n        If the real column has CI [1.0, 3.0] and the synthetic column has CI [2.0, 5.0], then the overlap width is 1.0\n        and the overlap value for the column is 0.5*(1.0/2.0 + 1.0/3.0), where 2.0 is the width of the first interval\n        and 3.0 is the width of the second. The final score is the average of these computations across all columns.\n\n        This metric also computes the number of intervals that DO NOT overlap, the percentage thereof, and an estimate\n        on the uncertainty of the mean number of overlaps themselves.\n\n        NOTE: This uses z-scores for confidence interval construction NOT t-scores\n\n        Args:\n            categorical_columns: Column names corresponding to the categorical variables of any provided dataframe.\n            numerical_columns: Column names corresponding to the numerical variables of any provided dataframe.\n            do_preprocess: Whether or not to preprocess the dataframes with the default pipeline used by SynthEval.\n                Defaults to False.\n            confidence_level: The confidence level for confidence interval calculations for each of the numerical\n                columns. These are the confidence intervals constructed around the mean of the numerical column\n                values. Defaults to ConfidenceLevel.NinetyFive.\n        \"\"\"\n        super().__init__(categorical_columns, numerical_columns, do_preprocess)\n        self.confidence_level = confidence_level\n\n    def compute(self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame) -&gt; dict[str, float]:\n        \"\"\"\n        Computes the mean of the interval overlap percentages for the confidence intervals (CIs) of each\n        NUMERICAL column. The confidence intervals are interval estimates for the mean value of a particular column\n        Within each column the value is the average percentage of overlap for the real and synthetic column CIs.\n\n        For example:\n\n        If the real column has CI [1.0, 3.0] and the synthetic column has CI [2.0, 5.0], then the overlap width is 1.0\n        and the overlap value for the column is 0.5*(1.0/2.0 + 1.0/3.0), where 2.0 is the width of the first interval\n        and 3.0 is the width of the second. The final score is the average of these computations across all columns.\n\n        This metric also computes the number of intervals that DO NOT overlap, the percentage thereof, and an estimate\n        on the uncertainty of the mean number of overlaps themselves.\n\n        Args:\n            real_data: Real data to which the synthetic data may be compared. In many cases this will be data used\n                to TRAIN the model that generated the synthetic data, but not always.\n            synthetic_data: Synthetically generated data whose quality is to be assessed.\n\n        Returns:\n            A dictionary with the various metric values for confidence interval overlap.\n\n            - \"avg overlap\": The mean of the overlap value for all numerical columns.\n            - \"overlap err\": An estimate of the uncertainty associated with the overlap percentage.\n            - \"num non-overlaps\": This is the number of columns whose confidence interval DO NOT overlap.\n            - \"frac non-overlaps\": It's the percentage of columns without a CI overlap.\n        \"\"\"\n        if self.do_preprocess:\n            real_data, synthetic_data = self.preprocess(real_data, synthetic_data)\n\n        self.syntheval_metric = ConfidenceIntervalOverlap(\n            real_data=real_data,\n            synt_data=synthetic_data,\n            hout_data=None,\n            cat_cols=self.categorical_columns,\n            num_cols=self.numerical_columns,\n            do_preprocessing=False,\n            verbose=False,\n        )\n\n        return self.syntheval_metric.evaluate(self.confidence_level.value)\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(\n    categorical_columns,\n    numerical_columns,\n    do_preprocess=False,\n    confidence_level=ConfidenceLevel.NinetyFive,\n)\n</code></pre> <p>This class computes the mean of the interval overlap percentages for the confidence intervals (CIs) of each NUMERICAL column. The confidence intervals are interval estimates for the mean value of a particular column Within each column the value is the average percentage of overlap for the real and synthetic column CIs.</p> <p>More overlap is better.</p> <p>For example:</p> <p>If the real column has CI [1.0, 3.0] and the synthetic column has CI [2.0, 5.0], then the overlap width is 1.0 and the overlap value for the column is 0.5*(1.0/2.0 + 1.0/3.0), where 2.0 is the width of the first interval and 3.0 is the width of the second. The final score is the average of these computations across all columns.</p> <p>This metric also computes the number of intervals that DO NOT overlap, the percentage thereof, and an estimate on the uncertainty of the mean number of overlaps themselves.</p> <p>NOTE: This uses z-scores for confidence interval construction NOT t-scores</p> <p>Parameters:</p> Name Type Description Default <code>categorical_columns</code> <code>list[str]</code> <p>Column names corresponding to the categorical variables of any provided dataframe.</p> required <code>numerical_columns</code> <code>list[str]</code> <p>Column names corresponding to the numerical variables of any provided dataframe.</p> required <code>do_preprocess</code> <code>bool</code> <p>Whether or not to preprocess the dataframes with the default pipeline used by SynthEval. Defaults to False.</p> <code>False</code> <code>confidence_level</code> <code>ConfidenceLevel</code> <p>The confidence level for confidence interval calculations for each of the numerical columns. These are the confidence intervals constructed around the mean of the numerical column values. Defaults to ConfidenceLevel.NinetyFive.</p> <code>NinetyFive</code> Source code in <code>src/midst_toolkit/evaluation/quality/confidence_interval_overlap.py</code> <pre><code>def __init__(\n    self,\n    categorical_columns: list[str],\n    numerical_columns: list[str],\n    do_preprocess: bool = False,\n    confidence_level: ConfidenceLevel = ConfidenceLevel.NinetyFive,\n) -&gt; None:\n    \"\"\"\n    This class computes the mean of the interval overlap percentages for the confidence intervals (CIs) of each\n    NUMERICAL column. The confidence intervals are interval estimates for the mean value of a particular column\n    Within each column the value is the average percentage of overlap for the real and synthetic column CIs.\n\n    More overlap is better.\n\n    For example:\n\n    If the real column has CI [1.0, 3.0] and the synthetic column has CI [2.0, 5.0], then the overlap width is 1.0\n    and the overlap value for the column is 0.5*(1.0/2.0 + 1.0/3.0), where 2.0 is the width of the first interval\n    and 3.0 is the width of the second. The final score is the average of these computations across all columns.\n\n    This metric also computes the number of intervals that DO NOT overlap, the percentage thereof, and an estimate\n    on the uncertainty of the mean number of overlaps themselves.\n\n    NOTE: This uses z-scores for confidence interval construction NOT t-scores\n\n    Args:\n        categorical_columns: Column names corresponding to the categorical variables of any provided dataframe.\n        numerical_columns: Column names corresponding to the numerical variables of any provided dataframe.\n        do_preprocess: Whether or not to preprocess the dataframes with the default pipeline used by SynthEval.\n            Defaults to False.\n        confidence_level: The confidence level for confidence interval calculations for each of the numerical\n            columns. These are the confidence intervals constructed around the mean of the numerical column\n            values. Defaults to ConfidenceLevel.NinetyFive.\n    \"\"\"\n    super().__init__(categorical_columns, numerical_columns, do_preprocess)\n    self.confidence_level = confidence_level\n</code></pre> <code></code> compute \u00b6 <pre><code>compute(real_data, synthetic_data)\n</code></pre> <p>Computes the mean of the interval overlap percentages for the confidence intervals (CIs) of each NUMERICAL column. The confidence intervals are interval estimates for the mean value of a particular column Within each column the value is the average percentage of overlap for the real and synthetic column CIs.</p> <p>For example:</p> <p>If the real column has CI [1.0, 3.0] and the synthetic column has CI [2.0, 5.0], then the overlap width is 1.0 and the overlap value for the column is 0.5*(1.0/2.0 + 1.0/3.0), where 2.0 is the width of the first interval and 3.0 is the width of the second. The final score is the average of these computations across all columns.</p> <p>This metric also computes the number of intervals that DO NOT overlap, the percentage thereof, and an estimate on the uncertainty of the mean number of overlaps themselves.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>DataFrame</code> <p>Real data to which the synthetic data may be compared. In many cases this will be data used to TRAIN the model that generated the synthetic data, but not always.</p> required <code>synthetic_data</code> <code>DataFrame</code> <p>Synthetically generated data whose quality is to be assessed.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dictionary with the various metric values for confidence interval overlap.</p> <code>dict[str, float]</code> <ul> <li>\"avg overlap\": The mean of the overlap value for all numerical columns.</li> </ul> <code>dict[str, float]</code> <ul> <li>\"overlap err\": An estimate of the uncertainty associated with the overlap percentage.</li> </ul> <code>dict[str, float]</code> <ul> <li>\"num non-overlaps\": This is the number of columns whose confidence interval DO NOT overlap.</li> </ul> <code>dict[str, float]</code> <ul> <li>\"frac non-overlaps\": It's the percentage of columns without a CI overlap.</li> </ul> Source code in <code>src/midst_toolkit/evaluation/quality/confidence_interval_overlap.py</code> <pre><code>def compute(self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame) -&gt; dict[str, float]:\n    \"\"\"\n    Computes the mean of the interval overlap percentages for the confidence intervals (CIs) of each\n    NUMERICAL column. The confidence intervals are interval estimates for the mean value of a particular column\n    Within each column the value is the average percentage of overlap for the real and synthetic column CIs.\n\n    For example:\n\n    If the real column has CI [1.0, 3.0] and the synthetic column has CI [2.0, 5.0], then the overlap width is 1.0\n    and the overlap value for the column is 0.5*(1.0/2.0 + 1.0/3.0), where 2.0 is the width of the first interval\n    and 3.0 is the width of the second. The final score is the average of these computations across all columns.\n\n    This metric also computes the number of intervals that DO NOT overlap, the percentage thereof, and an estimate\n    on the uncertainty of the mean number of overlaps themselves.\n\n    Args:\n        real_data: Real data to which the synthetic data may be compared. In many cases this will be data used\n            to TRAIN the model that generated the synthetic data, but not always.\n        synthetic_data: Synthetically generated data whose quality is to be assessed.\n\n    Returns:\n        A dictionary with the various metric values for confidence interval overlap.\n\n        - \"avg overlap\": The mean of the overlap value for all numerical columns.\n        - \"overlap err\": An estimate of the uncertainty associated with the overlap percentage.\n        - \"num non-overlaps\": This is the number of columns whose confidence interval DO NOT overlap.\n        - \"frac non-overlaps\": It's the percentage of columns without a CI overlap.\n    \"\"\"\n    if self.do_preprocess:\n        real_data, synthetic_data = self.preprocess(real_data, synthetic_data)\n\n    self.syntheval_metric = ConfidenceIntervalOverlap(\n        real_data=real_data,\n        synt_data=synthetic_data,\n        hout_data=None,\n        cat_cols=self.categorical_columns,\n        num_cols=self.numerical_columns,\n        do_preprocessing=False,\n        verbose=False,\n    )\n\n    return self.syntheval_metric.evaluate(self.confidence_level.value)\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.quality.correlation_matrix_difference","title":"correlation_matrix_difference","text":""},{"location":"api/#midst_toolkit.evaluation.quality.correlation_matrix_difference.CorrelationMatrixDifference","title":"CorrelationMatrixDifference","text":"<p>               Bases: <code>SynthEvalQualityMetric</code></p> Source code in <code>src/midst_toolkit/evaluation/quality/correlation_matrix_difference.py</code> <pre><code>class CorrelationMatrixDifference(SynthEvalQualityMetric):\n    def __init__(\n        self,\n        categorical_columns: list[str],\n        numerical_columns: list[str],\n        do_preprocess: bool = False,\n        compute_mixed_correlations: bool = False,\n    ):\n        \"\"\"\n        This class computes the correlation matrices between each of the columns of both real and synthetic dataframes\n        Then the difference between the correlation matrices is computed and the Froebenius norm of that difference\n        is returned. A smaller norm is better.\n\n        - Regardless of settings, correlations between the numerical columns are computed with Pearson correlation\n          coefficients.\n        - If ``compute_mixed_correlations`` is True, then correlations between the categorical variables is computed\n          using Cramer's V, and correlations between the numerical and categorical variable is done with a correlation\n          ratio, eta, as suggested in https://ieeexplore.ieee.org/document/10020639/.\n        - If ``compute_mixed_correlations`` is False, ONLY numerical correlations are computed.\n\n        NOTE: Categorical variables need not be one-hot encoded for correlations to work.\n\n        Args:\n            categorical_columns: Column names corresponding to the categorical variables of any provided dataframe.\n            numerical_columns: Column names corresponding to the numerical variables of any provided dataframe.\n            do_preprocess: Whether or not to preprocess the dataframes with the default pipeline used by SynthEval.\n                Defaults to False.\n            compute_mixed_correlations: Whether or not to compute correlations between the categorical variables and\n                the categorical and numerical variables. See documentation above for a longer description. Defaults to\n                False.\n        \"\"\"\n        super().__init__(categorical_columns, numerical_columns, do_preprocess)\n        self.compute_mixed_correlations = compute_mixed_correlations\n\n    def compute(self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame) -&gt; dict[str, float]:\n        \"\"\"\n        Computes the Froebenius norm of the difference between the correlation matrices associated with the\n        ``real_data`` and ``synthetic_data`` dataframes. The correlations computed depends on the value of\n        ``compute_mixed_correlations`` for the class.\n\n        - Regardless of settings, correlations between the numerical columns are computed with Pearson correlation\n          coefficients.\n        - If ``compute_mixed_correlations`` is True, then correlations between the categorical variables is computed\n          using Cramer's V, and correlations between the numerical and categorical variable is done with a correlation\n          ration, eta, as suggested in https://ieeexplore.ieee.org/document/10020639/.\n        - If ``compute_mixed_correlations`` is False, ONLY numerical correlations are computed.\n\n        Args:\n            real_data: Real data to which the synthetic data may be compared. In many cases this will be data used\n                to TRAIN the model that generated the synthetic data, but not always.\n            synthetic_data: Synthetically generated data whose quality is to be assessed.\n\n        Returns:\n            The Froebenius norm of the difference between the two real and synthetic data correlation matrices and the\n            number of columns in the computed correlations (rows/columns count of the correlation matrices). These\n            are keyed under 'corr_mat_diff' and 'corr_mat_dims' respectively.\n        \"\"\"\n        if self.do_preprocess:\n            real_data, synthetic_data = self.preprocess(real_data, synthetic_data)\n\n        self.syntheval_metric = MixedCorrelation(\n            real_data=real_data,\n            synt_data=synthetic_data,\n            hout_data=None,\n            cat_cols=self.categorical_columns,\n            num_cols=self.numerical_columns,\n            do_preprocessing=False,\n            verbose=False,\n        )\n\n        return self.syntheval_metric.evaluate(mixed_corr=self.compute_mixed_correlations, return_mats=False)\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(\n    categorical_columns,\n    numerical_columns,\n    do_preprocess=False,\n    compute_mixed_correlations=False,\n)\n</code></pre> <p>This class computes the correlation matrices between each of the columns of both real and synthetic dataframes Then the difference between the correlation matrices is computed and the Froebenius norm of that difference is returned. A smaller norm is better.</p> <ul> <li>Regardless of settings, correlations between the numerical columns are computed with Pearson correlation   coefficients.</li> <li>If <code>compute_mixed_correlations</code> is True, then correlations between the categorical variables is computed   using Cramer's V, and correlations between the numerical and categorical variable is done with a correlation   ratio, eta, as suggested in https://ieeexplore.ieee.org/document/10020639/.</li> <li>If <code>compute_mixed_correlations</code> is False, ONLY numerical correlations are computed.</li> </ul> <p>NOTE: Categorical variables need not be one-hot encoded for correlations to work.</p> <p>Parameters:</p> Name Type Description Default <code>categorical_columns</code> <code>list[str]</code> <p>Column names corresponding to the categorical variables of any provided dataframe.</p> required <code>numerical_columns</code> <code>list[str]</code> <p>Column names corresponding to the numerical variables of any provided dataframe.</p> required <code>do_preprocess</code> <code>bool</code> <p>Whether or not to preprocess the dataframes with the default pipeline used by SynthEval. Defaults to False.</p> <code>False</code> <code>compute_mixed_correlations</code> <code>bool</code> <p>Whether or not to compute correlations between the categorical variables and the categorical and numerical variables. See documentation above for a longer description. Defaults to False.</p> <code>False</code> Source code in <code>src/midst_toolkit/evaluation/quality/correlation_matrix_difference.py</code> <pre><code>def __init__(\n    self,\n    categorical_columns: list[str],\n    numerical_columns: list[str],\n    do_preprocess: bool = False,\n    compute_mixed_correlations: bool = False,\n):\n    \"\"\"\n    This class computes the correlation matrices between each of the columns of both real and synthetic dataframes\n    Then the difference between the correlation matrices is computed and the Froebenius norm of that difference\n    is returned. A smaller norm is better.\n\n    - Regardless of settings, correlations between the numerical columns are computed with Pearson correlation\n      coefficients.\n    - If ``compute_mixed_correlations`` is True, then correlations between the categorical variables is computed\n      using Cramer's V, and correlations between the numerical and categorical variable is done with a correlation\n      ratio, eta, as suggested in https://ieeexplore.ieee.org/document/10020639/.\n    - If ``compute_mixed_correlations`` is False, ONLY numerical correlations are computed.\n\n    NOTE: Categorical variables need not be one-hot encoded for correlations to work.\n\n    Args:\n        categorical_columns: Column names corresponding to the categorical variables of any provided dataframe.\n        numerical_columns: Column names corresponding to the numerical variables of any provided dataframe.\n        do_preprocess: Whether or not to preprocess the dataframes with the default pipeline used by SynthEval.\n            Defaults to False.\n        compute_mixed_correlations: Whether or not to compute correlations between the categorical variables and\n            the categorical and numerical variables. See documentation above for a longer description. Defaults to\n            False.\n    \"\"\"\n    super().__init__(categorical_columns, numerical_columns, do_preprocess)\n    self.compute_mixed_correlations = compute_mixed_correlations\n</code></pre> <code></code> compute \u00b6 <pre><code>compute(real_data, synthetic_data)\n</code></pre> <p>Computes the Froebenius norm of the difference between the correlation matrices associated with the <code>real_data</code> and <code>synthetic_data</code> dataframes. The correlations computed depends on the value of <code>compute_mixed_correlations</code> for the class.</p> <ul> <li>Regardless of settings, correlations between the numerical columns are computed with Pearson correlation   coefficients.</li> <li>If <code>compute_mixed_correlations</code> is True, then correlations between the categorical variables is computed   using Cramer's V, and correlations between the numerical and categorical variable is done with a correlation   ration, eta, as suggested in https://ieeexplore.ieee.org/document/10020639/.</li> <li>If <code>compute_mixed_correlations</code> is False, ONLY numerical correlations are computed.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>DataFrame</code> <p>Real data to which the synthetic data may be compared. In many cases this will be data used to TRAIN the model that generated the synthetic data, but not always.</p> required <code>synthetic_data</code> <code>DataFrame</code> <p>Synthetically generated data whose quality is to be assessed.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The Froebenius norm of the difference between the two real and synthetic data correlation matrices and the</p> <code>dict[str, float]</code> <p>number of columns in the computed correlations (rows/columns count of the correlation matrices). These</p> <code>dict[str, float]</code> <p>are keyed under 'corr_mat_diff' and 'corr_mat_dims' respectively.</p> Source code in <code>src/midst_toolkit/evaluation/quality/correlation_matrix_difference.py</code> <pre><code>def compute(self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame) -&gt; dict[str, float]:\n    \"\"\"\n    Computes the Froebenius norm of the difference between the correlation matrices associated with the\n    ``real_data`` and ``synthetic_data`` dataframes. The correlations computed depends on the value of\n    ``compute_mixed_correlations`` for the class.\n\n    - Regardless of settings, correlations between the numerical columns are computed with Pearson correlation\n      coefficients.\n    - If ``compute_mixed_correlations`` is True, then correlations between the categorical variables is computed\n      using Cramer's V, and correlations between the numerical and categorical variable is done with a correlation\n      ration, eta, as suggested in https://ieeexplore.ieee.org/document/10020639/.\n    - If ``compute_mixed_correlations`` is False, ONLY numerical correlations are computed.\n\n    Args:\n        real_data: Real data to which the synthetic data may be compared. In many cases this will be data used\n            to TRAIN the model that generated the synthetic data, but not always.\n        synthetic_data: Synthetically generated data whose quality is to be assessed.\n\n    Returns:\n        The Froebenius norm of the difference between the two real and synthetic data correlation matrices and the\n        number of columns in the computed correlations (rows/columns count of the correlation matrices). These\n        are keyed under 'corr_mat_diff' and 'corr_mat_dims' respectively.\n    \"\"\"\n    if self.do_preprocess:\n        real_data, synthetic_data = self.preprocess(real_data, synthetic_data)\n\n    self.syntheval_metric = MixedCorrelation(\n        real_data=real_data,\n        synt_data=synthetic_data,\n        hout_data=None,\n        cat_cols=self.categorical_columns,\n        num_cols=self.numerical_columns,\n        do_preprocessing=False,\n        verbose=False,\n    )\n\n    return self.syntheval_metric.evaluate(mixed_corr=self.compute_mixed_correlations, return_mats=False)\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.quality.dimensionwise_mean_difference","title":"dimensionwise_mean_difference","text":""},{"location":"api/#midst_toolkit.evaluation.quality.dimensionwise_mean_difference.DimensionwiseMeanDifference","title":"DimensionwiseMeanDifference","text":"<p>               Bases: <code>SynthEvalQualityMetric</code></p> Source code in <code>src/midst_toolkit/evaluation/quality/dimensionwise_mean_difference.py</code> <pre><code>class DimensionwiseMeanDifference(SynthEvalQualityMetric):\n    def compute(self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame) -&gt; dict[str, float]:\n        \"\"\"\n        Function to compute the dimensionwise mean difference between the dataframe of real data and the provided\n        synthetic data. The metric computes the mean value for each of the NUMERICAL columns in the two dataframes\n        then computes the differences of these means per columns and then computes the mean of the absolute value\n        of each of these differences. Ideally, this should be close to zero. An estimate of the cumulative standard\n        error across all dimensions is also computed to approximate the variance associated with the cumulative\n        difference of means across the numerical columns.\n\n        Args:\n            real_data: Real data to which the synthetic data may be compared. In many cases this will be data used\n                to TRAIN the model that generated the synthetic data, but not always.\n            synthetic_data: Synthetically generated data whose quality is to be assessed.\n\n        Returns:\n            A dictionary with the cumulative difference of means between the real and synthetic data columns, keyed\n            under \"dimensionwise_mean_difference\" and an estimate of the cumulative standard error across all\n            dimensions. This is keyed under \"standard_error_difference\"\n        \"\"\"\n        if self.do_preprocess:\n            real_data, synthetic_data = self.preprocess(real_data, synthetic_data)\n\n        self.syntheval_metric = SynthEvalDwm(\n            real_data=real_data,\n            synt_data=synthetic_data,\n            cat_cols=self.categorical_columns,\n            num_cols=self.numerical_columns,\n            do_preprocessing=False,\n            verbose=False,\n        )\n\n        result = self.syntheval_metric.evaluate()\n        result[\"dimensionwise_mean_difference\"] = result.pop(\"avg\")\n        result[\"standard_error_difference\"] = result.pop(\"err\")\n        return result\n</code></pre> <code></code> compute \u00b6 <pre><code>compute(real_data, synthetic_data)\n</code></pre> <p>Function to compute the dimensionwise mean difference between the dataframe of real data and the provided synthetic data. The metric computes the mean value for each of the NUMERICAL columns in the two dataframes then computes the differences of these means per columns and then computes the mean of the absolute value of each of these differences. Ideally, this should be close to zero. An estimate of the cumulative standard error across all dimensions is also computed to approximate the variance associated with the cumulative difference of means across the numerical columns.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>DataFrame</code> <p>Real data to which the synthetic data may be compared. In many cases this will be data used to TRAIN the model that generated the synthetic data, but not always.</p> required <code>synthetic_data</code> <code>DataFrame</code> <p>Synthetically generated data whose quality is to be assessed.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dictionary with the cumulative difference of means between the real and synthetic data columns, keyed</p> <code>dict[str, float]</code> <p>under \"dimensionwise_mean_difference\" and an estimate of the cumulative standard error across all</p> <code>dict[str, float]</code> <p>dimensions. This is keyed under \"standard_error_difference\"</p> Source code in <code>src/midst_toolkit/evaluation/quality/dimensionwise_mean_difference.py</code> <pre><code>def compute(self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame) -&gt; dict[str, float]:\n    \"\"\"\n    Function to compute the dimensionwise mean difference between the dataframe of real data and the provided\n    synthetic data. The metric computes the mean value for each of the NUMERICAL columns in the two dataframes\n    then computes the differences of these means per columns and then computes the mean of the absolute value\n    of each of these differences. Ideally, this should be close to zero. An estimate of the cumulative standard\n    error across all dimensions is also computed to approximate the variance associated with the cumulative\n    difference of means across the numerical columns.\n\n    Args:\n        real_data: Real data to which the synthetic data may be compared. In many cases this will be data used\n            to TRAIN the model that generated the synthetic data, but not always.\n        synthetic_data: Synthetically generated data whose quality is to be assessed.\n\n    Returns:\n        A dictionary with the cumulative difference of means between the real and synthetic data columns, keyed\n        under \"dimensionwise_mean_difference\" and an estimate of the cumulative standard error across all\n        dimensions. This is keyed under \"standard_error_difference\"\n    \"\"\"\n    if self.do_preprocess:\n        real_data, synthetic_data = self.preprocess(real_data, synthetic_data)\n\n    self.syntheval_metric = SynthEvalDwm(\n        real_data=real_data,\n        synt_data=synthetic_data,\n        cat_cols=self.categorical_columns,\n        num_cols=self.numerical_columns,\n        do_preprocessing=False,\n        verbose=False,\n    )\n\n    result = self.syntheval_metric.evaluate()\n    result[\"dimensionwise_mean_difference\"] = result.pop(\"avg\")\n    result[\"standard_error_difference\"] = result.pop(\"err\")\n    return result\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.quality.kolmogorov_smirnov_total_variation","title":"kolmogorov_smirnov_total_variation","text":""},{"location":"api/#midst_toolkit.evaluation.quality.kolmogorov_smirnov_total_variation.KolmogorovSmirnovAndTotalVariation","title":"KolmogorovSmirnovAndTotalVariation","text":"<p>               Bases: <code>SynthEvalQualityMetric</code></p> Source code in <code>src/midst_toolkit/evaluation/quality/kolmogorov_smirnov_total_variation.py</code> <pre><code>class KolmogorovSmirnovAndTotalVariation(SynthEvalQualityMetric):\n    def __init__(\n        self,\n        categorical_columns: list[str],\n        numerical_columns: list[str],\n        do_preprocess: bool = False,\n        significance_level: float = 0.05,\n        permutations: int = 1000,\n    ):\n        \"\"\"\n        This class performs a univariate comparison of corresponding columns in provided ``real_data`` and\n        ``synthetic_data`` dataframes. The distribution of numerical columns is compared using a Kolmogorov-Smirnov\n        (KS) test and categorical columns are compared with a Total Variation Distance (TVD) with significance\n        established using a permutation test. Both a performed as two-sided hypothesis tests to determine whether it\n        is likely that the distribution of a given column is the same between the two dataframes (null).\n\n        The main score is the average test statistic across all evaluated columns. Smaller is better. Other scores\n        returned include:\n\n        - Average statistic and standard error thereof for numerical columns.\n        - Average statistic and standard error thereof for categorical columns.\n        - Average p-values for the statistics of all columns.\n        - The number and percentage of columns that have statistically significant differences.\n\n        Args:\n            categorical_columns: Column names corresponding to the categorical variables of any provided dataframe. If\n                no columns are provided, the associated stat values will be NaN\n            numerical_columns: Column names corresponding to the numerical variables of any provided dataframe. If no\n                columns are provided, the associated stat values will be NaN\n            do_preprocess: Whether or not to preprocess the dataframes with the default pipeline used by SynthEval.\n                Defaults to False.\n            significance_level: Level of significance for the KS/TVD test statistics for a column of real vs. synthetic\n                data to be considered significantly different. Lower implies a higher significance requirement.\n            permutations: The number of permutations to run through to establish the TVD test statistic.\n        \"\"\"\n        super().__init__(categorical_columns, numerical_columns, do_preprocess)\n        self.significance_level = significance_level\n        self.permutations = permutations\n        self.all_columns = categorical_columns + numerical_columns\n\n    def compute(self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame) -&gt; dict[str, float]:\n        \"\"\"\n        Compares the columns of ``real_data`` with those of ``synthetic_data`` pairwise with statistical tests. For\n        numerical columns, this uses the Kolmogorov-Smirnov (KS) test and categorical columns are compared with a\n        Total Variation Distance (TVD) with significance established using a permutation test.\n\n        Args:\n            real_data: Real data to which the synthetic data may be compared. In many cases this will be data used\n                to TRAIN the model that generated the synthetic data, but not always.\n            synthetic_data: Synthetically generated data whose quality is to be assessed.\n\n        Returns:\n            The results of both tests are combined into a single score and reported separately. These are keyed as\n            follows:\n\n            - 'avg stat', 'stat err': Average of all statistics (KS and TVD) and the standard error of the stats.\n            - 'avg ks', 'ks err' : Average statistic and standard error thereof for numerical columns.\n            - 'avg tvd', 'tvd err': Average statistic and standard error thereof for categorical columns.\n            - 'avg pval', 'pval err': Average p-values for the statistics of all columns.\n            - 'num sigs', 'frac sigs': The number and percentage of columns that have significance differences.\n        \"\"\"\n        if self.do_preprocess:\n            real_data, synthetic_data = self.preprocess(real_data, synthetic_data)\n\n        # NOTE: The SynthEval KolmogorovSmirnovTest class ignores column specifications by default. However, for\n        # other classes (correlation_matrix_difference for example), specifying less than all of the columns restricts\n        # the score computation to just those columns. To make this consistent we do that here, before passing to the\n        # SynthEval class.\n        filtered_real_data = real_data[self.all_columns]\n        filtered_synthetic_data = synthetic_data[self.all_columns]\n\n        self.syntheval_metric = KolmogorovSmirnovTest(\n            real_data=filtered_real_data,\n            synt_data=filtered_synthetic_data,\n            hout_data=None,\n            cat_cols=self.categorical_columns,\n            num_cols=self.numerical_columns,\n            do_preprocessing=False,\n            verbose=False,\n        )\n\n        return self.syntheval_metric.evaluate(sig_lvl=self.significance_level, n_perms=self.permutations)\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(\n    categorical_columns,\n    numerical_columns,\n    do_preprocess=False,\n    significance_level=0.05,\n    permutations=1000,\n)\n</code></pre> <p>This class performs a univariate comparison of corresponding columns in provided <code>real_data</code> and <code>synthetic_data</code> dataframes. The distribution of numerical columns is compared using a Kolmogorov-Smirnov (KS) test and categorical columns are compared with a Total Variation Distance (TVD) with significance established using a permutation test. Both a performed as two-sided hypothesis tests to determine whether it is likely that the distribution of a given column is the same between the two dataframes (null).</p> <p>The main score is the average test statistic across all evaluated columns. Smaller is better. Other scores returned include:</p> <ul> <li>Average statistic and standard error thereof for numerical columns.</li> <li>Average statistic and standard error thereof for categorical columns.</li> <li>Average p-values for the statistics of all columns.</li> <li>The number and percentage of columns that have statistically significant differences.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>categorical_columns</code> <code>list[str]</code> <p>Column names corresponding to the categorical variables of any provided dataframe. If no columns are provided, the associated stat values will be NaN</p> required <code>numerical_columns</code> <code>list[str]</code> <p>Column names corresponding to the numerical variables of any provided dataframe. If no columns are provided, the associated stat values will be NaN</p> required <code>do_preprocess</code> <code>bool</code> <p>Whether or not to preprocess the dataframes with the default pipeline used by SynthEval. Defaults to False.</p> <code>False</code> <code>significance_level</code> <code>float</code> <p>Level of significance for the KS/TVD test statistics for a column of real vs. synthetic data to be considered significantly different. Lower implies a higher significance requirement.</p> <code>0.05</code> <code>permutations</code> <code>int</code> <p>The number of permutations to run through to establish the TVD test statistic.</p> <code>1000</code> Source code in <code>src/midst_toolkit/evaluation/quality/kolmogorov_smirnov_total_variation.py</code> <pre><code>def __init__(\n    self,\n    categorical_columns: list[str],\n    numerical_columns: list[str],\n    do_preprocess: bool = False,\n    significance_level: float = 0.05,\n    permutations: int = 1000,\n):\n    \"\"\"\n    This class performs a univariate comparison of corresponding columns in provided ``real_data`` and\n    ``synthetic_data`` dataframes. The distribution of numerical columns is compared using a Kolmogorov-Smirnov\n    (KS) test and categorical columns are compared with a Total Variation Distance (TVD) with significance\n    established using a permutation test. Both a performed as two-sided hypothesis tests to determine whether it\n    is likely that the distribution of a given column is the same between the two dataframes (null).\n\n    The main score is the average test statistic across all evaluated columns. Smaller is better. Other scores\n    returned include:\n\n    - Average statistic and standard error thereof for numerical columns.\n    - Average statistic and standard error thereof for categorical columns.\n    - Average p-values for the statistics of all columns.\n    - The number and percentage of columns that have statistically significant differences.\n\n    Args:\n        categorical_columns: Column names corresponding to the categorical variables of any provided dataframe. If\n            no columns are provided, the associated stat values will be NaN\n        numerical_columns: Column names corresponding to the numerical variables of any provided dataframe. If no\n            columns are provided, the associated stat values will be NaN\n        do_preprocess: Whether or not to preprocess the dataframes with the default pipeline used by SynthEval.\n            Defaults to False.\n        significance_level: Level of significance for the KS/TVD test statistics for a column of real vs. synthetic\n            data to be considered significantly different. Lower implies a higher significance requirement.\n        permutations: The number of permutations to run through to establish the TVD test statistic.\n    \"\"\"\n    super().__init__(categorical_columns, numerical_columns, do_preprocess)\n    self.significance_level = significance_level\n    self.permutations = permutations\n    self.all_columns = categorical_columns + numerical_columns\n</code></pre> <code></code> compute \u00b6 <pre><code>compute(real_data, synthetic_data)\n</code></pre> <p>Compares the columns of <code>real_data</code> with those of <code>synthetic_data</code> pairwise with statistical tests. For numerical columns, this uses the Kolmogorov-Smirnov (KS) test and categorical columns are compared with a Total Variation Distance (TVD) with significance established using a permutation test.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>DataFrame</code> <p>Real data to which the synthetic data may be compared. In many cases this will be data used to TRAIN the model that generated the synthetic data, but not always.</p> required <code>synthetic_data</code> <code>DataFrame</code> <p>Synthetically generated data whose quality is to be assessed.</p> required <p>Returns:</p> Name Type Description <code>dict[str, float]</code> <p>The results of both tests are combined into a single score and reported separately. These are keyed as</p> <code>follows</code> <code>dict[str, float]</code> <code>dict[str, float]</code> <ul> <li>'avg stat', 'stat err': Average of all statistics (KS and TVD) and the standard error of the stats.</li> </ul> <code>dict[str, float]</code> <ul> <li>'avg ks', 'ks err' : Average statistic and standard error thereof for numerical columns.</li> </ul> <code>dict[str, float]</code> <ul> <li>'avg tvd', 'tvd err': Average statistic and standard error thereof for categorical columns.</li> </ul> <code>dict[str, float]</code> <ul> <li>'avg pval', 'pval err': Average p-values for the statistics of all columns.</li> </ul> <code>dict[str, float]</code> <ul> <li>'num sigs', 'frac sigs': The number and percentage of columns that have significance differences.</li> </ul> Source code in <code>src/midst_toolkit/evaluation/quality/kolmogorov_smirnov_total_variation.py</code> <pre><code>def compute(self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame) -&gt; dict[str, float]:\n    \"\"\"\n    Compares the columns of ``real_data`` with those of ``synthetic_data`` pairwise with statistical tests. For\n    numerical columns, this uses the Kolmogorov-Smirnov (KS) test and categorical columns are compared with a\n    Total Variation Distance (TVD) with significance established using a permutation test.\n\n    Args:\n        real_data: Real data to which the synthetic data may be compared. In many cases this will be data used\n            to TRAIN the model that generated the synthetic data, but not always.\n        synthetic_data: Synthetically generated data whose quality is to be assessed.\n\n    Returns:\n        The results of both tests are combined into a single score and reported separately. These are keyed as\n        follows:\n\n        - 'avg stat', 'stat err': Average of all statistics (KS and TVD) and the standard error of the stats.\n        - 'avg ks', 'ks err' : Average statistic and standard error thereof for numerical columns.\n        - 'avg tvd', 'tvd err': Average statistic and standard error thereof for categorical columns.\n        - 'avg pval', 'pval err': Average p-values for the statistics of all columns.\n        - 'num sigs', 'frac sigs': The number and percentage of columns that have significance differences.\n    \"\"\"\n    if self.do_preprocess:\n        real_data, synthetic_data = self.preprocess(real_data, synthetic_data)\n\n    # NOTE: The SynthEval KolmogorovSmirnovTest class ignores column specifications by default. However, for\n    # other classes (correlation_matrix_difference for example), specifying less than all of the columns restricts\n    # the score computation to just those columns. To make this consistent we do that here, before passing to the\n    # SynthEval class.\n    filtered_real_data = real_data[self.all_columns]\n    filtered_synthetic_data = synthetic_data[self.all_columns]\n\n    self.syntheval_metric = KolmogorovSmirnovTest(\n        real_data=filtered_real_data,\n        synt_data=filtered_synthetic_data,\n        hout_data=None,\n        cat_cols=self.categorical_columns,\n        num_cols=self.numerical_columns,\n        do_preprocessing=False,\n        verbose=False,\n    )\n\n    return self.syntheval_metric.evaluate(sig_lvl=self.significance_level, n_perms=self.permutations)\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.quality.mutual_information_difference","title":"mutual_information_difference","text":""},{"location":"api/#midst_toolkit.evaluation.quality.mutual_information_difference.MutualInformationDifference","title":"MutualInformationDifference","text":"<p>               Bases: <code>SynthEvalQualityMetric</code></p> Source code in <code>src/midst_toolkit/evaluation/quality/mutual_information_difference.py</code> <pre><code>class MutualInformationDifference(SynthEvalQualityMetric):\n    def __init__(\n        self,\n        categorical_columns: list[str],\n        numerical_columns: list[str],\n        do_preprocess: bool = False,\n        include_numerical_columns: bool = True,\n    ):\n        \"\"\"\n        This class computes the Froebenius norm of the difference between the Mutual Information (MI) score matrices\n        associated with two dataframes being compared. A smaller norm is better.\n\n        The computation is based on:\n\n        Ping H, Stoyanovich J, Howe B. DataSynthesizer: privacy-preserving synthetic datasets. 2017\n        Presented at: Proceedings of the 29th International Conference on Scientific and Statistical Database\n        Management; 2017; Chicago. [doi:10.1145/3085504.3091117]\n\n        It leverages ``normalized_mutual_info_score`` from sklearn under the hood. The function computes the MI\n        matrices, comparing the individual columns of the dataframes to each other. Then the difference of the\n        two matrices is taken and the Froebenius norm computed for the final score.\n\n        NOTE: Mutual Information works well for categorical variables. However, by default, SynthEval essentially\n        just converts numerical columns to string representations for the computation. This isn't a great idea for\n        things like floats. By default, this class respects SynthEval's choice, but you can override it and compute\n        MI difference score for categorical columns only by setting ``include_numerical_columns`` to False, or\n        providing an empty list for ``numerical_columns``.\n\n        Args:\n            categorical_columns: Column names corresponding to the categorical variables of any provided dataframe.\n            numerical_columns: Column names corresponding to the numerical variables of any provided dataframe.\n            do_preprocess: Whether or not to preprocess the dataframes with the default pipeline used by SynthEval.\n                Defaults to False.\n            include_numerical_columns: Whether to include any provided numerical columns in the MI difference score\n                computation. See the note above for why you might not want to include them.\n        \"\"\"\n        super().__init__(categorical_columns, numerical_columns, do_preprocess)\n        self.include_numerical_columns = include_numerical_columns\n        self.all_columns = categorical_columns + numerical_columns\n\n    def compute(self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame) -&gt; dict[str, float]:\n        \"\"\"\n        Computes the Froebenius norm of the difference between the Mutual Information (MI) score matrices associated\n        with  the ``real_data`` and ``synthetic_data`` dataframes. The computation is based on the work below.\n\n        Ping H, Stoyanovich J, Howe B. DataSynthesizer: privacy-preserving synthetic datasets. 2017\n        Presented at: Proceedings of the 29th International Conference on Scientific and Statistical Database\n        Management; 2017; Chicago. [doi:10.1145/3085504.3091117]\n\n        It leverages ``normalized_mutual_info_score`` from sklearn under the hood. The function computes the MI\n        matrices, comparing the individual columns of the dataframes to each other. Then the difference of the\n        two matrices is taken and the Froebenius norm computed for the final score.\n\n        NOTE: Mutual Information works well for categorical variables. However, by default, SynthEval essentially\n        just converts numerical columns to string representations for the computation. This isn't a great idea for\n        things like floats. By default, this class respects SynthEval's choice, but you can override it and compute\n        MI difference score for categorical columns only by setting ``self.include_numerical_columns`` to False, or\n        ``self.numerical_columns`` to an empty list.\n\n        Args:\n            real_data: Real data to which the synthetic data may be compared. In many cases this will be data used\n                to TRAIN the model that generated the synthetic data, but not always.\n            synthetic_data: Synthetically generated data whose quality is to be assessed.\n\n        Returns:\n            The Froebenius norm of the difference between the two real and synthetic data MI matrices and the\n            number of columns in the computed mutual information (rows/columns count of the correlation matrices).\n            These are keyed under 'mutual_inf_diff' and 'mi_mat_dims' respectively.\n        \"\"\"\n        if self.do_preprocess:\n            real_data, synthetic_data = self.preprocess(real_data, synthetic_data)\n\n        # NOTE: The SynthEval MutualInformation class ignores column specifications by default. However, for\n        # other classes (correlation_matrix_difference for example), specifying less than all of the columns restricts\n        # the score computation to just those columns. To make this consistent we do that here, before passing to the\n        # SynthEval class.\n        filtered_real_data = (\n            real_data[self.all_columns] if self.include_numerical_columns else real_data[self.categorical_columns]\n        )\n        filtered_synthetic_data = (\n            synthetic_data[self.all_columns]\n            if self.include_numerical_columns\n            else synthetic_data[self.categorical_columns]\n        )\n\n        self.syntheval_metric = MutualInformation(\n            real_data=filtered_real_data,\n            synt_data=filtered_synthetic_data,\n            hout_data=None,\n            cat_cols=self.categorical_columns,\n            num_cols=self.numerical_columns,\n            do_preprocessing=False,\n            verbose=False,\n        )\n\n        return self.syntheval_metric.evaluate()\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(\n    categorical_columns,\n    numerical_columns,\n    do_preprocess=False,\n    include_numerical_columns=True,\n)\n</code></pre> <p>This class computes the Froebenius norm of the difference between the Mutual Information (MI) score matrices associated with two dataframes being compared. A smaller norm is better.</p> <p>The computation is based on:</p> <p>Ping H, Stoyanovich J, Howe B. DataSynthesizer: privacy-preserving synthetic datasets. 2017 Presented at: Proceedings of the 29th International Conference on Scientific and Statistical Database Management; 2017; Chicago. [doi:10.1145/3085504.3091117]</p> <p>It leverages <code>normalized_mutual_info_score</code> from sklearn under the hood. The function computes the MI matrices, comparing the individual columns of the dataframes to each other. Then the difference of the two matrices is taken and the Froebenius norm computed for the final score.</p> <p>NOTE: Mutual Information works well for categorical variables. However, by default, SynthEval essentially just converts numerical columns to string representations for the computation. This isn't a great idea for things like floats. By default, this class respects SynthEval's choice, but you can override it and compute MI difference score for categorical columns only by setting <code>include_numerical_columns</code> to False, or providing an empty list for <code>numerical_columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>categorical_columns</code> <code>list[str]</code> <p>Column names corresponding to the categorical variables of any provided dataframe.</p> required <code>numerical_columns</code> <code>list[str]</code> <p>Column names corresponding to the numerical variables of any provided dataframe.</p> required <code>do_preprocess</code> <code>bool</code> <p>Whether or not to preprocess the dataframes with the default pipeline used by SynthEval. Defaults to False.</p> <code>False</code> <code>include_numerical_columns</code> <code>bool</code> <p>Whether to include any provided numerical columns in the MI difference score computation. See the note above for why you might not want to include them.</p> <code>True</code> Source code in <code>src/midst_toolkit/evaluation/quality/mutual_information_difference.py</code> <pre><code>def __init__(\n    self,\n    categorical_columns: list[str],\n    numerical_columns: list[str],\n    do_preprocess: bool = False,\n    include_numerical_columns: bool = True,\n):\n    \"\"\"\n    This class computes the Froebenius norm of the difference between the Mutual Information (MI) score matrices\n    associated with two dataframes being compared. A smaller norm is better.\n\n    The computation is based on:\n\n    Ping H, Stoyanovich J, Howe B. DataSynthesizer: privacy-preserving synthetic datasets. 2017\n    Presented at: Proceedings of the 29th International Conference on Scientific and Statistical Database\n    Management; 2017; Chicago. [doi:10.1145/3085504.3091117]\n\n    It leverages ``normalized_mutual_info_score`` from sklearn under the hood. The function computes the MI\n    matrices, comparing the individual columns of the dataframes to each other. Then the difference of the\n    two matrices is taken and the Froebenius norm computed for the final score.\n\n    NOTE: Mutual Information works well for categorical variables. However, by default, SynthEval essentially\n    just converts numerical columns to string representations for the computation. This isn't a great idea for\n    things like floats. By default, this class respects SynthEval's choice, but you can override it and compute\n    MI difference score for categorical columns only by setting ``include_numerical_columns`` to False, or\n    providing an empty list for ``numerical_columns``.\n\n    Args:\n        categorical_columns: Column names corresponding to the categorical variables of any provided dataframe.\n        numerical_columns: Column names corresponding to the numerical variables of any provided dataframe.\n        do_preprocess: Whether or not to preprocess the dataframes with the default pipeline used by SynthEval.\n            Defaults to False.\n        include_numerical_columns: Whether to include any provided numerical columns in the MI difference score\n            computation. See the note above for why you might not want to include them.\n    \"\"\"\n    super().__init__(categorical_columns, numerical_columns, do_preprocess)\n    self.include_numerical_columns = include_numerical_columns\n    self.all_columns = categorical_columns + numerical_columns\n</code></pre> <code></code> compute \u00b6 <pre><code>compute(real_data, synthetic_data)\n</code></pre> <p>Computes the Froebenius norm of the difference between the Mutual Information (MI) score matrices associated with  the <code>real_data</code> and <code>synthetic_data</code> dataframes. The computation is based on the work below.</p> <p>Ping H, Stoyanovich J, Howe B. DataSynthesizer: privacy-preserving synthetic datasets. 2017 Presented at: Proceedings of the 29th International Conference on Scientific and Statistical Database Management; 2017; Chicago. [doi:10.1145/3085504.3091117]</p> <p>It leverages <code>normalized_mutual_info_score</code> from sklearn under the hood. The function computes the MI matrices, comparing the individual columns of the dataframes to each other. Then the difference of the two matrices is taken and the Froebenius norm computed for the final score.</p> <p>NOTE: Mutual Information works well for categorical variables. However, by default, SynthEval essentially just converts numerical columns to string representations for the computation. This isn't a great idea for things like floats. By default, this class respects SynthEval's choice, but you can override it and compute MI difference score for categorical columns only by setting <code>self.include_numerical_columns</code> to False, or <code>self.numerical_columns</code> to an empty list.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>DataFrame</code> <p>Real data to which the synthetic data may be compared. In many cases this will be data used to TRAIN the model that generated the synthetic data, but not always.</p> required <code>synthetic_data</code> <code>DataFrame</code> <p>Synthetically generated data whose quality is to be assessed.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The Froebenius norm of the difference between the two real and synthetic data MI matrices and the</p> <code>dict[str, float]</code> <p>number of columns in the computed mutual information (rows/columns count of the correlation matrices).</p> <code>dict[str, float]</code> <p>These are keyed under 'mutual_inf_diff' and 'mi_mat_dims' respectively.</p> Source code in <code>src/midst_toolkit/evaluation/quality/mutual_information_difference.py</code> <pre><code>def compute(self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame) -&gt; dict[str, float]:\n    \"\"\"\n    Computes the Froebenius norm of the difference between the Mutual Information (MI) score matrices associated\n    with  the ``real_data`` and ``synthetic_data`` dataframes. The computation is based on the work below.\n\n    Ping H, Stoyanovich J, Howe B. DataSynthesizer: privacy-preserving synthetic datasets. 2017\n    Presented at: Proceedings of the 29th International Conference on Scientific and Statistical Database\n    Management; 2017; Chicago. [doi:10.1145/3085504.3091117]\n\n    It leverages ``normalized_mutual_info_score`` from sklearn under the hood. The function computes the MI\n    matrices, comparing the individual columns of the dataframes to each other. Then the difference of the\n    two matrices is taken and the Froebenius norm computed for the final score.\n\n    NOTE: Mutual Information works well for categorical variables. However, by default, SynthEval essentially\n    just converts numerical columns to string representations for the computation. This isn't a great idea for\n    things like floats. By default, this class respects SynthEval's choice, but you can override it and compute\n    MI difference score for categorical columns only by setting ``self.include_numerical_columns`` to False, or\n    ``self.numerical_columns`` to an empty list.\n\n    Args:\n        real_data: Real data to which the synthetic data may be compared. In many cases this will be data used\n            to TRAIN the model that generated the synthetic data, but not always.\n        synthetic_data: Synthetically generated data whose quality is to be assessed.\n\n    Returns:\n        The Froebenius norm of the difference between the two real and synthetic data MI matrices and the\n        number of columns in the computed mutual information (rows/columns count of the correlation matrices).\n        These are keyed under 'mutual_inf_diff' and 'mi_mat_dims' respectively.\n    \"\"\"\n    if self.do_preprocess:\n        real_data, synthetic_data = self.preprocess(real_data, synthetic_data)\n\n    # NOTE: The SynthEval MutualInformation class ignores column specifications by default. However, for\n    # other classes (correlation_matrix_difference for example), specifying less than all of the columns restricts\n    # the score computation to just those columns. To make this consistent we do that here, before passing to the\n    # SynthEval class.\n    filtered_real_data = (\n        real_data[self.all_columns] if self.include_numerical_columns else real_data[self.categorical_columns]\n    )\n    filtered_synthetic_data = (\n        synthetic_data[self.all_columns]\n        if self.include_numerical_columns\n        else synthetic_data[self.categorical_columns]\n    )\n\n    self.syntheval_metric = MutualInformation(\n        real_data=filtered_real_data,\n        synt_data=filtered_synthetic_data,\n        hout_data=None,\n        cat_cols=self.categorical_columns,\n        num_cols=self.numerical_columns,\n        do_preprocessing=False,\n        verbose=False,\n    )\n\n    return self.syntheval_metric.evaluate()\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.quality.synthcity","title":"synthcity","text":"<p>This Module reproduces the code necessary to run the Synthcity alpha precision evaluation computations. The files are pulled and modified from this repository https://github.com/vanderschaarlab/synthcity.</p> <p>There are two reasons for this port.</p> <p>1: Synthcity, in its larger form, is incompatible with newer versions of PyTorch, beyond 2.4, which is limiting.</p> <p>2: On Mac OS, every time its imported, some external python process is kicked off. This manifests as a launcher icon appearing temporarily in the dock and then disappearing. If you're using VS code, it periodically re-imports the library in the background (for whatever reason), resulting in the launcher icon appearing repeatedly and endlessly.</p> <p>We don't need much of the Synthcity library for our evals. So having the computations reproduced here locally alleviates both issues. We can re-evaluate this port with newer versions of Synthcity, if they come.</p>"},{"location":"api/#midst_toolkit.evaluation.quality.synthcity.dataloader","title":"dataloader","text":"DataLoader \u00b6 Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>class DataLoader(metaclass=ABCMeta):\n    def __init__(\n        self,\n        data_type: str,\n        data: Any,\n        static_features: list[str] | None = None,\n        temporal_features: list[str] | None = None,\n        sensitive_features: list[str] | None = None,\n        important_features: list[str] | None = None,\n        outcome_features: list[str] | None = None,\n        train_size: float = 0.8,\n        random_state: int = 0,\n    ) -&gt; None:\n        \"\"\"\n        Base class for all data loaders.\n\n        Args:\n            data_type: The type of DataLoader, currently supports \"generic\"\n            data: The object that contains the data\n            static_features: List of feature names that are static features (as opposed to temporal features).\n                Defaults to None.\n            temporal_features: List of feature names that are temporal features, i.e. observed over time.\n                Defaults to None.\n            sensitive_features: Name of sensitive features. Defaults to None.\n            important_features: Only relevant for SurvivalGAN method. Defaults to None.\n            outcome_features: The feature name that provides labels for downstream tasks. Defaults to None.\n            train_size: Proportion of data to be used for training, versus evaluation. Defaults to 0.8.\n            random_state: Random state for sampling from the dataloaders. Defaults to 0.\n        \"\"\"\n        self.static_features = static_features if static_features else []\n        self.temporal_features = temporal_features if temporal_features else []\n        self.sensitive_features = sensitive_features if sensitive_features else []\n        self.important_features = important_features if important_features else []\n        self.outcome_features = outcome_features if outcome_features else []\n        self.random_state = random_state\n\n        self.data = data\n        self.data_type = data_type\n        self.train_size = train_size\n\n    def raw(self) -&gt; Any:\n        \"\"\"Just return the data in the dataloader.\"\"\"\n        return self.data\n\n    @abstractmethod\n    def unpack(self, as_numpy: bool = False, pad: bool = False) -&gt; Any:\n        \"\"\"A method that unpacks the columns and returns features and labels (X, y).\"\"\"\n        ...\n\n    @abstractmethod\n    def decorate(self, data: Any) -&gt; DataLoader:\n        \"\"\"\n        A method that creates a new instance of DataLoader by decorating the input data with the same DataLoader\n        properties (e.g. sensitive features, target column, etc.).\n        \"\"\"\n        ...\n\n    def type(self) -&gt; str:\n        \"\"\"Return data type.\"\"\"\n        return self.data_type\n\n    @property\n    @abstractmethod\n    def shape(self) -&gt; tuple:\n        \"\"\"Return shape of the data.\"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def columns(self) -&gt; list:\n        \"\"\"Return list of data columns.\"\"\"\n        ...\n\n    @abstractmethod\n    def dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"A method that returns the pandas dataframe that contains all features and samples.\"\"\"\n        ...\n\n    @abstractmethod\n    def numpy(self) -&gt; np.ndarray:\n        \"\"\"A method that returns the numpy array that contains all features and samples.\"\"\"\n        ...\n\n    @property\n    def values(self) -&gt; np.ndarray:\n        \"\"\"Pass through to the numpy method.\"\"\"\n        return self.numpy()\n\n    @abstractmethod\n    def info(self) -&gt; dict:\n        \"\"\"A method that returns a dictionary of DataLoader information.\"\"\"\n        ...\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"A method that returns the number of samples in the DataLoader.\"\"\"\n        ...\n\n    @staticmethod\n    @abstractmethod\n    def from_info(data: pd.DataFrame, info: dict) -&gt; DataLoader:\n        \"\"\"A static method that creates a DataLoader from the data and the information dictionary.\"\"\"\n        ...\n\n    @abstractmethod\n    def sample(self, count: int, random_state: int = 0) -&gt; DataLoader:\n        \"\"\"Returns a new DataLoader that contains a random subset of N samples.\"\"\"\n        ...\n\n    @abstractmethod\n    def drop(self, columns: list | None) -&gt; DataLoader:\n        \"\"\"Returns a new DataLoader with a list of columns dropped.\"\"\"\n        ...\n\n    @abstractmethod\n    def __getitem__(self, feature: str | list) -&gt; Any:\n        \"\"\"Get an item as specified in the feature argument.\"\"\"\n        ...\n\n    @abstractmethod\n    def __setitem__(self, feature: str, val: Any) -&gt; None:\n        \"\"\"Set an item in the dataloader to the provided value.\"\"\"\n        ...\n\n    @abstractmethod\n    def train(self) -&gt; DataLoader:\n        \"\"\"Returns a DataLoader containing the training set.\"\"\"\n        ...\n\n    @abstractmethod\n    def test(self) -&gt; DataLoader:\n        \"\"\"Returns a DataLoader containing the test set.\"\"\"\n        ...\n\n    def __repr__(self, *args: Any, **kwargs: Any) -&gt; str:\n        \"\"\"Return a string representation.\"\"\"\n        return self.dataframe().__repr__(*args, **kwargs)\n\n    def _repr_html_(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Return a string representation in html format.\"\"\"\n        return self.dataframe()._repr_html_(*args, **kwargs)\n\n    @abstractmethod\n    def fillna(self, value: Any) -&gt; DataLoader:\n        \"\"\"Returns a DataLoader with NaN filled by the provided number(s).\"\"\"\n        ...\n\n    @abstractmethod\n    def compression_protected_features(self) -&gt; list:\n        \"\"\"No idea.\"\"\"\n        ...\n\n    def domain(self) -&gt; str | None:\n        \"\"\"Domain of the data.\"\"\"\n        return None\n\n    @abstractmethod\n    def is_tabular(self) -&gt; bool:\n        \"\"\"Specifies whether the dataloader represents tabular data.\"\"\"\n        ...\n\n    @abstractmethod\n    def get_fairness_column(self) -&gt; str | Any:\n        \"\"\"Get the name of the column associated with Fairness.\"\"\"\n        ...\n</code></pre> <code></code> shape <code>abstractmethod</code> <code>property</code> \u00b6 <pre><code>shape\n</code></pre> <p>Return shape of the data.</p> <code></code> columns <code>abstractmethod</code> <code>property</code> \u00b6 <pre><code>columns\n</code></pre> <p>Return list of data columns.</p> <code></code> values <code>property</code> \u00b6 <pre><code>values\n</code></pre> <p>Pass through to the numpy method.</p> <code></code> __init__ \u00b6 <pre><code>__init__(\n    data_type,\n    data,\n    static_features=None,\n    temporal_features=None,\n    sensitive_features=None,\n    important_features=None,\n    outcome_features=None,\n    train_size=0.8,\n    random_state=0,\n)\n</code></pre> <p>Base class for all data loaders.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>str</code> <p>The type of DataLoader, currently supports \"generic\"</p> required <code>data</code> <code>Any</code> <p>The object that contains the data</p> required <code>static_features</code> <code>list[str] | None</code> <p>List of feature names that are static features (as opposed to temporal features). Defaults to None.</p> <code>None</code> <code>temporal_features</code> <code>list[str] | None</code> <p>List of feature names that are temporal features, i.e. observed over time. Defaults to None.</p> <code>None</code> <code>sensitive_features</code> <code>list[str] | None</code> <p>Name of sensitive features. Defaults to None.</p> <code>None</code> <code>important_features</code> <code>list[str] | None</code> <p>Only relevant for SurvivalGAN method. Defaults to None.</p> <code>None</code> <code>outcome_features</code> <code>list[str] | None</code> <p>The feature name that provides labels for downstream tasks. Defaults to None.</p> <code>None</code> <code>train_size</code> <code>float</code> <p>Proportion of data to be used for training, versus evaluation. Defaults to 0.8.</p> <code>0.8</code> <code>random_state</code> <code>int</code> <p>Random state for sampling from the dataloaders. Defaults to 0.</p> <code>0</code> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def __init__(\n    self,\n    data_type: str,\n    data: Any,\n    static_features: list[str] | None = None,\n    temporal_features: list[str] | None = None,\n    sensitive_features: list[str] | None = None,\n    important_features: list[str] | None = None,\n    outcome_features: list[str] | None = None,\n    train_size: float = 0.8,\n    random_state: int = 0,\n) -&gt; None:\n    \"\"\"\n    Base class for all data loaders.\n\n    Args:\n        data_type: The type of DataLoader, currently supports \"generic\"\n        data: The object that contains the data\n        static_features: List of feature names that are static features (as opposed to temporal features).\n            Defaults to None.\n        temporal_features: List of feature names that are temporal features, i.e. observed over time.\n            Defaults to None.\n        sensitive_features: Name of sensitive features. Defaults to None.\n        important_features: Only relevant for SurvivalGAN method. Defaults to None.\n        outcome_features: The feature name that provides labels for downstream tasks. Defaults to None.\n        train_size: Proportion of data to be used for training, versus evaluation. Defaults to 0.8.\n        random_state: Random state for sampling from the dataloaders. Defaults to 0.\n    \"\"\"\n    self.static_features = static_features if static_features else []\n    self.temporal_features = temporal_features if temporal_features else []\n    self.sensitive_features = sensitive_features if sensitive_features else []\n    self.important_features = important_features if important_features else []\n    self.outcome_features = outcome_features if outcome_features else []\n    self.random_state = random_state\n\n    self.data = data\n    self.data_type = data_type\n    self.train_size = train_size\n</code></pre> <code></code> raw \u00b6 <pre><code>raw()\n</code></pre> <p>Just return the data in the dataloader.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def raw(self) -&gt; Any:\n    \"\"\"Just return the data in the dataloader.\"\"\"\n    return self.data\n</code></pre> <code></code> unpack <code>abstractmethod</code> \u00b6 <pre><code>unpack(as_numpy=False, pad=False)\n</code></pre> <p>A method that unpacks the columns and returns features and labels (X, y).</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef unpack(self, as_numpy: bool = False, pad: bool = False) -&gt; Any:\n    \"\"\"A method that unpacks the columns and returns features and labels (X, y).\"\"\"\n    ...\n</code></pre> <code></code> decorate <code>abstractmethod</code> \u00b6 <pre><code>decorate(data)\n</code></pre> <p>A method that creates a new instance of DataLoader by decorating the input data with the same DataLoader properties (e.g. sensitive features, target column, etc.).</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef decorate(self, data: Any) -&gt; DataLoader:\n    \"\"\"\n    A method that creates a new instance of DataLoader by decorating the input data with the same DataLoader\n    properties (e.g. sensitive features, target column, etc.).\n    \"\"\"\n    ...\n</code></pre> <code></code> type \u00b6 <pre><code>type()\n</code></pre> <p>Return data type.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def type(self) -&gt; str:\n    \"\"\"Return data type.\"\"\"\n    return self.data_type\n</code></pre> <code></code> dataframe <code>abstractmethod</code> \u00b6 <pre><code>dataframe()\n</code></pre> <p>A method that returns the pandas dataframe that contains all features and samples.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"A method that returns the pandas dataframe that contains all features and samples.\"\"\"\n    ...\n</code></pre> <code></code> numpy <code>abstractmethod</code> \u00b6 <pre><code>numpy()\n</code></pre> <p>A method that returns the numpy array that contains all features and samples.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef numpy(self) -&gt; np.ndarray:\n    \"\"\"A method that returns the numpy array that contains all features and samples.\"\"\"\n    ...\n</code></pre> <code></code> info <code>abstractmethod</code> \u00b6 <pre><code>info()\n</code></pre> <p>A method that returns a dictionary of DataLoader information.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef info(self) -&gt; dict:\n    \"\"\"A method that returns a dictionary of DataLoader information.\"\"\"\n    ...\n</code></pre> <code></code> __len__ <code>abstractmethod</code> \u00b6 <pre><code>__len__()\n</code></pre> <p>A method that returns the number of samples in the DataLoader.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"A method that returns the number of samples in the DataLoader.\"\"\"\n    ...\n</code></pre> <code></code> from_info <code>abstractmethod</code> <code>staticmethod</code> \u00b6 <pre><code>from_info(data, info)\n</code></pre> <p>A static method that creates a DataLoader from the data and the information dictionary.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef from_info(data: pd.DataFrame, info: dict) -&gt; DataLoader:\n    \"\"\"A static method that creates a DataLoader from the data and the information dictionary.\"\"\"\n    ...\n</code></pre> <code></code> sample <code>abstractmethod</code> \u00b6 <pre><code>sample(count, random_state=0)\n</code></pre> <p>Returns a new DataLoader that contains a random subset of N samples.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef sample(self, count: int, random_state: int = 0) -&gt; DataLoader:\n    \"\"\"Returns a new DataLoader that contains a random subset of N samples.\"\"\"\n    ...\n</code></pre> <code></code> drop <code>abstractmethod</code> \u00b6 <pre><code>drop(columns)\n</code></pre> <p>Returns a new DataLoader with a list of columns dropped.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef drop(self, columns: list | None) -&gt; DataLoader:\n    \"\"\"Returns a new DataLoader with a list of columns dropped.\"\"\"\n    ...\n</code></pre> <code></code> __getitem__ <code>abstractmethod</code> \u00b6 <pre><code>__getitem__(feature)\n</code></pre> <p>Get an item as specified in the feature argument.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, feature: str | list) -&gt; Any:\n    \"\"\"Get an item as specified in the feature argument.\"\"\"\n    ...\n</code></pre> <code></code> __setitem__ <code>abstractmethod</code> \u00b6 <pre><code>__setitem__(feature, val)\n</code></pre> <p>Set an item in the dataloader to the provided value.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef __setitem__(self, feature: str, val: Any) -&gt; None:\n    \"\"\"Set an item in the dataloader to the provided value.\"\"\"\n    ...\n</code></pre> <code></code> train <code>abstractmethod</code> \u00b6 <pre><code>train()\n</code></pre> <p>Returns a DataLoader containing the training set.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef train(self) -&gt; DataLoader:\n    \"\"\"Returns a DataLoader containing the training set.\"\"\"\n    ...\n</code></pre> <code></code> test <code>abstractmethod</code> \u00b6 <pre><code>test()\n</code></pre> <p>Returns a DataLoader containing the test set.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef test(self) -&gt; DataLoader:\n    \"\"\"Returns a DataLoader containing the test set.\"\"\"\n    ...\n</code></pre> <code></code> __repr__ \u00b6 <pre><code>__repr__(*args, **kwargs)\n</code></pre> <p>Return a string representation.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def __repr__(self, *args: Any, **kwargs: Any) -&gt; str:\n    \"\"\"Return a string representation.\"\"\"\n    return self.dataframe().__repr__(*args, **kwargs)\n</code></pre> <code></code> fillna <code>abstractmethod</code> \u00b6 <pre><code>fillna(value)\n</code></pre> <p>Returns a DataLoader with NaN filled by the provided number(s).</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef fillna(self, value: Any) -&gt; DataLoader:\n    \"\"\"Returns a DataLoader with NaN filled by the provided number(s).\"\"\"\n    ...\n</code></pre> <code></code> compression_protected_features <code>abstractmethod</code> \u00b6 <pre><code>compression_protected_features()\n</code></pre> <p>No idea.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef compression_protected_features(self) -&gt; list:\n    \"\"\"No idea.\"\"\"\n    ...\n</code></pre> <code></code> domain \u00b6 <pre><code>domain()\n</code></pre> <p>Domain of the data.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def domain(self) -&gt; str | None:\n    \"\"\"Domain of the data.\"\"\"\n    return None\n</code></pre> <code></code> is_tabular <code>abstractmethod</code> \u00b6 <pre><code>is_tabular()\n</code></pre> <p>Specifies whether the dataloader represents tabular data.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef is_tabular(self) -&gt; bool:\n    \"\"\"Specifies whether the dataloader represents tabular data.\"\"\"\n    ...\n</code></pre> <code></code> get_fairness_column <code>abstractmethod</code> \u00b6 <pre><code>get_fairness_column()\n</code></pre> <p>Get the name of the column associated with Fairness.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef get_fairness_column(self) -&gt; str | Any:\n    \"\"\"Get the name of the column associated with Fairness.\"\"\"\n    ...\n</code></pre> <code></code> GenericDataLoader \u00b6 <p>               Bases: <code>DataLoader</code></p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>class GenericDataLoader(DataLoader):\n    def __init__(\n        self,\n        data: pd.DataFrame | list | np.ndarray,\n        sensitive_features: list[str] | None = None,\n        important_features: list[str] | None = None,\n        target_column: str | None = None,\n        fairness_column: str | None = None,\n        domain_column: str | None = None,\n        random_state: int = 0,\n        train_size: float = 0.8,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Data loader for generic tabular data.\n\n        Args:\n            data: The dataset. Either a Pandas DataFrame, list, or a Numpy Array.\n            sensitive_features:  Name of sensitive features.. Defaults to None.\n            important_features: Only relevant for SurvivalGAN method. Defaults to None.\n            target_column: The feature name that provides labels for downstream tasks. Defaults to None.\n            fairness_column:  Optional fairness column label, used for fairness benchmarking. Defaults to None.\n            domain_column: Optional domain label, used for domain adaptation algorithms. Defaults to None.\n            random_state: Random state for sampling from the dataloaders. Defaults to 0.\n            train_size: Proportion of data to be used for training, versus evaluation. Defaults to 0.8.\n            kwargs: Other settings to be processed.\n        \"\"\"\n        if not isinstance(data, pd.DataFrame):\n            data = pd.DataFrame(data)\n\n        data.columns = data.columns.astype(str)\n        if target_column is not None:\n            self.target_column = target_column\n        elif len(data.columns) &gt; 0:\n            self.target_column = data.columns[-1]\n        else:\n            self.target_column = \"---\"\n\n        self.fairness_column = fairness_column\n        self.domain_column = domain_column\n\n        super().__init__(\n            data_type=\"generic\",\n            data=data,\n            static_features=list(data.columns),\n            sensitive_features=sensitive_features,\n            important_features=important_features,\n            outcome_features=[self.target_column],\n            random_state=random_state,\n            train_size=train_size,\n            **kwargs,\n        )\n\n    @property\n    def shape(self) -&gt; tuple:\n        \"\"\"Return the shape of the data.\"\"\"\n        return self.data.shape\n\n    def domain(self) -&gt; str | None:\n        \"\"\"Return the domain column if it exists.\"\"\"\n        return self.domain_column\n\n    def get_fairness_column(self) -&gt; str | Any:\n        \"\"\"Return the fairness column if it exists.\"\"\"\n        return self.fairness_column\n\n    @property\n    def columns(self) -&gt; list:\n        \"\"\"Return a list of the data columns.\"\"\"\n        return list(self.data.columns)\n\n    def compression_protected_features(self) -&gt; list:\n        \"\"\"No idea.\"\"\"\n        out = [self.target_column]\n        domain = self.domain()\n\n        if domain is not None:\n            out.append(domain)\n\n        return out\n\n    def unpack(self, as_numpy: bool = False, pad: bool = False) -&gt; Any:\n        \"\"\"A method that unpacks the columns and returns features and labels (X, y).\"\"\"\n        x = self.data.drop(columns=[self.target_column])\n        y = self.data[self.target_column]\n\n        if as_numpy:\n            return np.asarray(x), np.asarray(y)\n        return x, y\n\n    def dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"A method that returns the pandas dataframe that contains all features and samples.\"\"\"\n        return self.data\n\n    def numpy(self) -&gt; np.ndarray:\n        \"\"\"A method that returns the numpy array that contains all features and samples.\"\"\"\n        return self.dataframe().values\n\n    def info(self) -&gt; dict:\n        \"\"\"A method that returns a dictionary of DataLoader information.\"\"\"\n        return {\n            \"data_type\": self.data_type,\n            \"len\": len(self),\n            \"static_features\": self.static_features,\n            \"sensitive_features\": self.sensitive_features,\n            \"important_features\": self.important_features,\n            \"outcome_features\": self.outcome_features,\n            \"target_column\": self.target_column,\n            \"fairness_column\": self.fairness_column,\n            \"domain_column\": self.domain_column,\n            \"train_size\": self.train_size,\n        }\n\n    def __len__(self) -&gt; int:\n        \"\"\"A method that returns the number of samples in the DataLoader.\"\"\"\n        return len(self.data)\n\n    def decorate(self, data: Any) -&gt; DataLoader:\n        \"\"\"\n        A method that creates a new instance of DataLoader by decorating the input data with the same DataLoader\n        properties (e.g. sensitive features, target column, etc.).\n        \"\"\"\n        return GenericDataLoader(\n            data,\n            sensitive_features=self.sensitive_features,\n            important_features=self.important_features,\n            target_column=self.target_column,\n            random_state=self.random_state,\n            train_size=self.train_size,\n            fairness_column=self.fairness_column,\n            domain_column=self.domain_column,\n        )\n\n    def sample(self, count: int, random_state: int = 0) -&gt; DataLoader:\n        \"\"\"Returns a new DataLoader that contains a random subset of N samples.\"\"\"\n        return self.decorate(self.data.sample(count, random_state=random_state))\n\n    def drop(self, columns: list | None = None) -&gt; DataLoader:\n        \"\"\"Returns a new DataLoader with a list of columns dropped.\"\"\"\n        return self.decorate(self.data.drop(columns=(columns if columns else [])))\n\n    @staticmethod\n    def from_info(data: pd.DataFrame, info: dict) -&gt; GenericDataLoader:\n        \"\"\"A static method that creates a DataLoader from the data and the information dictionary.\"\"\"\n        if not isinstance(data, pd.DataFrame):\n            raise ValueError(f\"Invalid data type {type(data)}\")\n\n        return GenericDataLoader(\n            data,\n            sensitive_features=info[\"sensitive_features\"],\n            important_features=info[\"important_features\"],\n            target_column=info[\"target_column\"],\n            fairness_column=info[\"fairness_column\"],\n            domain_column=info[\"domain_column\"],\n            train_size=info[\"train_size\"],\n        )\n\n    def __getitem__(self, feature: str | list | int) -&gt; Any:\n        \"\"\"Get an item from the dataloader.\"\"\"\n        return self.data[feature]\n\n    def __setitem__(self, feature: str, val: Any) -&gt; None:\n        \"\"\"Replace an item in the dataloader with the specified value.\"\"\"\n        self.data[feature] = val\n\n    def _train_test_split(self) -&gt; tuple:\n        \"\"\"Split the dataset into train and test sets according to a specified ratio.\"\"\"\n        stratify = None\n        if self.target_column in self.data:\n            target = self.data[self.target_column]\n            if target.value_counts().min() &gt; 1:\n                stratify = target\n\n        return train_test_split(\n            self.data,\n            train_size=self.train_size,\n            random_state=self.random_state,\n            stratify=stratify,\n        )\n\n    def train(self) -&gt; DataLoader:\n        \"\"\"Returns a DataLoader containing the training set.\"\"\"\n        train_data, _ = self._train_test_split()\n        return self.decorate(train_data.reset_index(drop=True))\n\n    def test(self) -&gt; DataLoader:\n        \"\"\"Returns a DataLoader containing the training set.\"\"\"\n        _, test_data = self._train_test_split()\n        return self.decorate(test_data.reset_index(drop=True))\n\n    def fillna(self, value: Any) -&gt; DataLoader:\n        \"\"\"Returns a DataLoader with NaN filled by the provided number(s).\"\"\"\n        self.data = self.data.fillna(value)\n        return self\n\n    def is_tabular(self) -&gt; bool:\n        \"\"\"Always represents a tabular dataset.\"\"\"\n        return True\n</code></pre> <code></code> shape <code>property</code> \u00b6 <pre><code>shape\n</code></pre> <p>Return the shape of the data.</p> <code></code> columns <code>property</code> \u00b6 <pre><code>columns\n</code></pre> <p>Return a list of the data columns.</p> <code></code> __init__ \u00b6 <pre><code>__init__(\n    data,\n    sensitive_features=None,\n    important_features=None,\n    target_column=None,\n    fairness_column=None,\n    domain_column=None,\n    random_state=0,\n    train_size=0.8,\n    **kwargs,\n)\n</code></pre> <p>Data loader for generic tabular data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame | list | ndarray</code> <p>The dataset. Either a Pandas DataFrame, list, or a Numpy Array.</p> required <code>sensitive_features</code> <code>list[str] | None</code> <p>Name of sensitive features.. Defaults to None.</p> <code>None</code> <code>important_features</code> <code>list[str] | None</code> <p>Only relevant for SurvivalGAN method. Defaults to None.</p> <code>None</code> <code>target_column</code> <code>str | None</code> <p>The feature name that provides labels for downstream tasks. Defaults to None.</p> <code>None</code> <code>fairness_column</code> <code>str | None</code> <p>Optional fairness column label, used for fairness benchmarking. Defaults to None.</p> <code>None</code> <code>domain_column</code> <code>str | None</code> <p>Optional domain label, used for domain adaptation algorithms. Defaults to None.</p> <code>None</code> <code>random_state</code> <code>int</code> <p>Random state for sampling from the dataloaders. Defaults to 0.</p> <code>0</code> <code>train_size</code> <code>float</code> <p>Proportion of data to be used for training, versus evaluation. Defaults to 0.8.</p> <code>0.8</code> <code>kwargs</code> <code>Any</code> <p>Other settings to be processed.</p> <code>{}</code> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame | list | np.ndarray,\n    sensitive_features: list[str] | None = None,\n    important_features: list[str] | None = None,\n    target_column: str | None = None,\n    fairness_column: str | None = None,\n    domain_column: str | None = None,\n    random_state: int = 0,\n    train_size: float = 0.8,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Data loader for generic tabular data.\n\n    Args:\n        data: The dataset. Either a Pandas DataFrame, list, or a Numpy Array.\n        sensitive_features:  Name of sensitive features.. Defaults to None.\n        important_features: Only relevant for SurvivalGAN method. Defaults to None.\n        target_column: The feature name that provides labels for downstream tasks. Defaults to None.\n        fairness_column:  Optional fairness column label, used for fairness benchmarking. Defaults to None.\n        domain_column: Optional domain label, used for domain adaptation algorithms. Defaults to None.\n        random_state: Random state for sampling from the dataloaders. Defaults to 0.\n        train_size: Proportion of data to be used for training, versus evaluation. Defaults to 0.8.\n        kwargs: Other settings to be processed.\n    \"\"\"\n    if not isinstance(data, pd.DataFrame):\n        data = pd.DataFrame(data)\n\n    data.columns = data.columns.astype(str)\n    if target_column is not None:\n        self.target_column = target_column\n    elif len(data.columns) &gt; 0:\n        self.target_column = data.columns[-1]\n    else:\n        self.target_column = \"---\"\n\n    self.fairness_column = fairness_column\n    self.domain_column = domain_column\n\n    super().__init__(\n        data_type=\"generic\",\n        data=data,\n        static_features=list(data.columns),\n        sensitive_features=sensitive_features,\n        important_features=important_features,\n        outcome_features=[self.target_column],\n        random_state=random_state,\n        train_size=train_size,\n        **kwargs,\n    )\n</code></pre> <code></code> domain \u00b6 <pre><code>domain()\n</code></pre> <p>Return the domain column if it exists.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def domain(self) -&gt; str | None:\n    \"\"\"Return the domain column if it exists.\"\"\"\n    return self.domain_column\n</code></pre> <code></code> get_fairness_column \u00b6 <pre><code>get_fairness_column()\n</code></pre> <p>Return the fairness column if it exists.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def get_fairness_column(self) -&gt; str | Any:\n    \"\"\"Return the fairness column if it exists.\"\"\"\n    return self.fairness_column\n</code></pre> <code></code> compression_protected_features \u00b6 <pre><code>compression_protected_features()\n</code></pre> <p>No idea.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def compression_protected_features(self) -&gt; list:\n    \"\"\"No idea.\"\"\"\n    out = [self.target_column]\n    domain = self.domain()\n\n    if domain is not None:\n        out.append(domain)\n\n    return out\n</code></pre> <code></code> unpack \u00b6 <pre><code>unpack(as_numpy=False, pad=False)\n</code></pre> <p>A method that unpacks the columns and returns features and labels (X, y).</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def unpack(self, as_numpy: bool = False, pad: bool = False) -&gt; Any:\n    \"\"\"A method that unpacks the columns and returns features and labels (X, y).\"\"\"\n    x = self.data.drop(columns=[self.target_column])\n    y = self.data[self.target_column]\n\n    if as_numpy:\n        return np.asarray(x), np.asarray(y)\n    return x, y\n</code></pre> <code></code> dataframe \u00b6 <pre><code>dataframe()\n</code></pre> <p>A method that returns the pandas dataframe that contains all features and samples.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"A method that returns the pandas dataframe that contains all features and samples.\"\"\"\n    return self.data\n</code></pre> <code></code> numpy \u00b6 <pre><code>numpy()\n</code></pre> <p>A method that returns the numpy array that contains all features and samples.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def numpy(self) -&gt; np.ndarray:\n    \"\"\"A method that returns the numpy array that contains all features and samples.\"\"\"\n    return self.dataframe().values\n</code></pre> <code></code> info \u00b6 <pre><code>info()\n</code></pre> <p>A method that returns a dictionary of DataLoader information.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def info(self) -&gt; dict:\n    \"\"\"A method that returns a dictionary of DataLoader information.\"\"\"\n    return {\n        \"data_type\": self.data_type,\n        \"len\": len(self),\n        \"static_features\": self.static_features,\n        \"sensitive_features\": self.sensitive_features,\n        \"important_features\": self.important_features,\n        \"outcome_features\": self.outcome_features,\n        \"target_column\": self.target_column,\n        \"fairness_column\": self.fairness_column,\n        \"domain_column\": self.domain_column,\n        \"train_size\": self.train_size,\n    }\n</code></pre> <code></code> __len__ \u00b6 <pre><code>__len__()\n</code></pre> <p>A method that returns the number of samples in the DataLoader.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"A method that returns the number of samples in the DataLoader.\"\"\"\n    return len(self.data)\n</code></pre> <code></code> decorate \u00b6 <pre><code>decorate(data)\n</code></pre> <p>A method that creates a new instance of DataLoader by decorating the input data with the same DataLoader properties (e.g. sensitive features, target column, etc.).</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def decorate(self, data: Any) -&gt; DataLoader:\n    \"\"\"\n    A method that creates a new instance of DataLoader by decorating the input data with the same DataLoader\n    properties (e.g. sensitive features, target column, etc.).\n    \"\"\"\n    return GenericDataLoader(\n        data,\n        sensitive_features=self.sensitive_features,\n        important_features=self.important_features,\n        target_column=self.target_column,\n        random_state=self.random_state,\n        train_size=self.train_size,\n        fairness_column=self.fairness_column,\n        domain_column=self.domain_column,\n    )\n</code></pre> <code></code> sample \u00b6 <pre><code>sample(count, random_state=0)\n</code></pre> <p>Returns a new DataLoader that contains a random subset of N samples.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def sample(self, count: int, random_state: int = 0) -&gt; DataLoader:\n    \"\"\"Returns a new DataLoader that contains a random subset of N samples.\"\"\"\n    return self.decorate(self.data.sample(count, random_state=random_state))\n</code></pre> <code></code> drop \u00b6 <pre><code>drop(columns=None)\n</code></pre> <p>Returns a new DataLoader with a list of columns dropped.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def drop(self, columns: list | None = None) -&gt; DataLoader:\n    \"\"\"Returns a new DataLoader with a list of columns dropped.\"\"\"\n    return self.decorate(self.data.drop(columns=(columns if columns else [])))\n</code></pre> <code></code> from_info <code>staticmethod</code> \u00b6 <pre><code>from_info(data, info)\n</code></pre> <p>A static method that creates a DataLoader from the data and the information dictionary.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>@staticmethod\ndef from_info(data: pd.DataFrame, info: dict) -&gt; GenericDataLoader:\n    \"\"\"A static method that creates a DataLoader from the data and the information dictionary.\"\"\"\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(f\"Invalid data type {type(data)}\")\n\n    return GenericDataLoader(\n        data,\n        sensitive_features=info[\"sensitive_features\"],\n        important_features=info[\"important_features\"],\n        target_column=info[\"target_column\"],\n        fairness_column=info[\"fairness_column\"],\n        domain_column=info[\"domain_column\"],\n        train_size=info[\"train_size\"],\n    )\n</code></pre> <code></code> __getitem__ \u00b6 <pre><code>__getitem__(feature)\n</code></pre> <p>Get an item from the dataloader.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def __getitem__(self, feature: str | list | int) -&gt; Any:\n    \"\"\"Get an item from the dataloader.\"\"\"\n    return self.data[feature]\n</code></pre> <code></code> __setitem__ \u00b6 <pre><code>__setitem__(feature, val)\n</code></pre> <p>Replace an item in the dataloader with the specified value.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def __setitem__(self, feature: str, val: Any) -&gt; None:\n    \"\"\"Replace an item in the dataloader with the specified value.\"\"\"\n    self.data[feature] = val\n</code></pre> <code></code> train \u00b6 <pre><code>train()\n</code></pre> <p>Returns a DataLoader containing the training set.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def train(self) -&gt; DataLoader:\n    \"\"\"Returns a DataLoader containing the training set.\"\"\"\n    train_data, _ = self._train_test_split()\n    return self.decorate(train_data.reset_index(drop=True))\n</code></pre> <code></code> test \u00b6 <pre><code>test()\n</code></pre> <p>Returns a DataLoader containing the training set.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def test(self) -&gt; DataLoader:\n    \"\"\"Returns a DataLoader containing the training set.\"\"\"\n    _, test_data = self._train_test_split()\n    return self.decorate(test_data.reset_index(drop=True))\n</code></pre> <code></code> fillna \u00b6 <pre><code>fillna(value)\n</code></pre> <p>Returns a DataLoader with NaN filled by the provided number(s).</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def fillna(self, value: Any) -&gt; DataLoader:\n    \"\"\"Returns a DataLoader with NaN filled by the provided number(s).\"\"\"\n    self.data = self.data.fillna(value)\n    return self\n</code></pre> <code></code> is_tabular \u00b6 <pre><code>is_tabular()\n</code></pre> <p>Always represents a tabular dataset.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def is_tabular(self) -&gt; bool:\n    \"\"\"Always represents a tabular dataset.\"\"\"\n    return True\n</code></pre> <code></code> create_from_info \u00b6 <pre><code>create_from_info(data, info)\n</code></pre> <p>Helper for creating a DataLoader from existing information.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/dataloader.py</code> <pre><code>def create_from_info(data: pd.DataFrame | torch.utils.data.Dataset, info: dict) -&gt; DataLoader:\n    \"\"\"Helper for creating a DataLoader from existing information.\"\"\"\n    if info[\"data_type\"] == \"generic\":\n        return GenericDataLoader.from_info(data, info)\n    raise RuntimeError(f\"invalid datatype {info}\")\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.quality.synthcity.feature_encoder","title":"feature_encoder","text":"FeatureEncoder \u00b6 <p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/feature_encoder.py</code> <pre><code>class FeatureEncoder(TransformerMixin, BaseEstimator):  # type: ignore\n    def __init__(self, n_dim_in: int = 1, n_dim_out: int = 2) -&gt; None:\n        \"\"\"\n        Base feature encoder with sklearn-style API.\n\n        Args:\n            n_dim_in: Size of the input to the feature encoder. Defaults to 1.\n            n_dim_out: Size of the output from the feature encoder. Defaults to 2.\n        \"\"\"\n        super().__init__()\n        self.n_dim_in = n_dim_in\n        self.n_dim_out = n_dim_out\n        self.n_features_out: int\n        self.feature_name_in: str\n        self.feature_names_out: list[str]\n        self.feature_types_out: list[str]\n        self.categorical: bool = False\n\n    def fit(self, x: pd.Series, y: Any = None, **kwargs: Any) -&gt; FeatureEncoder:\n        \"\"\"\n        Fit the feature encoder using the input Pandas series x and possibly the layers in the form of y.\n\n        Args:\n            x: Input for fitting the encoder\n            y: Optional labels that might be used in fitting the encoder. Defaults to None.\n            kwargs: Other settings to be processed\n\n        Returns:\n            The fitted FeatureEncoder object.\n        \"\"\"\n        self.feature_name_in = x.name\n        self.feature_type_in = self._get_feature_type(x)\n        input = validate_shape(x.values, self.n_dim_in)\n        output = self._fit(input, **kwargs)._transform(input)\n        self._out_shape = (-1, *output.shape[1:])  # for inverse_transform\n        output = validate_shape(output, self.n_dim_out)\n        if self.n_dim_out == 1:\n            self.n_features_out = 1\n        else:\n            self.n_features_out = output.shape[1]\n        self.feature_names_out = self.get_feature_names_out()\n        self.feature_types_out = self.get_feature_types_out(output)\n        return self\n\n    def _fit(self, x: np.ndarray, **kwargs: Any) -&gt; FeatureEncoder:\n        return self\n\n    def transform(self, x: pd.Series) -&gt; pd.DataFrame | pd.Series:\n        \"\"\"\n        Take in the Series and use the encoder to transform the series after fitting.\n\n        Args:\n            x: The input to be transformed.\n\n        Returns:\n            The transformed input.\n        \"\"\"\n        data = validate_shape(x.values, self.n_dim_in)\n        out = self._transform(data)\n        out = validate_shape(out, self.n_dim_out)\n        if self.n_dim_out == 1:\n            return pd.Series(out, name=self.feature_name_in)\n        return pd.DataFrame(out, columns=self.feature_names_out)\n\n    def _transform(self, x: np.ndarray) -&gt; np.ndarray:\n        return x\n\n    def get_feature_names_out(self) -&gt; list[str]:\n        \"\"\"A list of the names of the features being encoded.\"\"\"\n        n = self.n_features_out\n        if n == 1:\n            return [self.feature_name_in]\n        return [f\"{self.feature_name_in}_{i}\" for i in range(n)]\n\n    def get_feature_types_out(self, output: np.ndarray) -&gt; list[str]:\n        \"\"\"A list of the name of the features produced by the encoder.\"\"\"\n        t = self._get_feature_type(output)\n        return [t] * self.n_features_out\n\n    def _get_feature_type(self, x: Any) -&gt; str:\n        \"\"\"A string indicating the feature type associated with the input.\"\"\"\n        if self.categorical:\n            return \"discrete\"\n        if np.issubdtype(x.dtype, np.floating):\n            return \"continuous\"\n        if np.issubdtype(x.dtype, np.datetime64):\n            return \"datetime\"\n        return \"discrete\"\n\n    def inverse_transform(self, df: pd.DataFrame | pd.Series) -&gt; pd.Series:\n        \"\"\"Reverse the encoder mapping.\"\"\"\n        y = df.values.reshape(self._out_shape)\n        x = self._inverse_transform(y)\n        x = validate_shape(x, 1)\n        return pd.Series(x, name=self.feature_name_in)\n\n    def _inverse_transform(self, data: np.ndarray) -&gt; np.ndarray:\n        return data\n\n    @classmethod\n    def wraps(cls: type, encoder_class: TransformerMixin, **params: Any) -&gt; type[FeatureEncoder]:\n        \"\"\"Wraps sklearn transformer to FeatureEncoder.\"\"\"\n\n        class WrappedEncoder(FeatureEncoder):\n            def __init__(self, n_dim_in: int = 2, *args: Any, **kwargs: Any) -&gt; None:\n                self.encoder = encoder_class(n_dim_in, *args, **kwargs)\n\n            def _fit(self, x: np.ndarray, **kwargs: Any) -&gt; FeatureEncoder:\n                self.encoder.fit(x, **kwargs)\n                return self\n\n            def _transform(self, x: np.ndarray) -&gt; np.ndarray:\n                return self.encoder.transform(x)\n\n            def _inverse_transform(self, data: np.ndarray) -&gt; np.ndarray:\n                return self.encoder.inverse_transform(data)\n\n            def get_feature_names_out(self) -&gt; list[str]:\n                return list(self.encoder.get_feature_names_out([self.feature_name_in]))\n\n        for attr in (\"__name__\", \"__qualname__\", \"__doc__\"):\n            setattr(WrappedEncoder, attr, getattr(encoder_class, attr))\n        for attr, val in params.items():\n            setattr(WrappedEncoder, attr, val)\n\n        return WrappedEncoder\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(n_dim_in=1, n_dim_out=2)\n</code></pre> <p>Base feature encoder with sklearn-style API.</p> <p>Parameters:</p> Name Type Description Default <code>n_dim_in</code> <code>int</code> <p>Size of the input to the feature encoder. Defaults to 1.</p> <code>1</code> <code>n_dim_out</code> <code>int</code> <p>Size of the output from the feature encoder. Defaults to 2.</p> <code>2</code> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/feature_encoder.py</code> <pre><code>def __init__(self, n_dim_in: int = 1, n_dim_out: int = 2) -&gt; None:\n    \"\"\"\n    Base feature encoder with sklearn-style API.\n\n    Args:\n        n_dim_in: Size of the input to the feature encoder. Defaults to 1.\n        n_dim_out: Size of the output from the feature encoder. Defaults to 2.\n    \"\"\"\n    super().__init__()\n    self.n_dim_in = n_dim_in\n    self.n_dim_out = n_dim_out\n    self.n_features_out: int\n    self.feature_name_in: str\n    self.feature_names_out: list[str]\n    self.feature_types_out: list[str]\n    self.categorical: bool = False\n</code></pre> <code></code> fit \u00b6 <pre><code>fit(x, y=None, **kwargs)\n</code></pre> <p>Fit the feature encoder using the input Pandas series x and possibly the layers in the form of y.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>Input for fitting the encoder</p> required <code>y</code> <code>Any</code> <p>Optional labels that might be used in fitting the encoder. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Other settings to be processed</p> <code>{}</code> <p>Returns:</p> Type Description <code>FeatureEncoder</code> <p>The fitted FeatureEncoder object.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/feature_encoder.py</code> <pre><code>def fit(self, x: pd.Series, y: Any = None, **kwargs: Any) -&gt; FeatureEncoder:\n    \"\"\"\n    Fit the feature encoder using the input Pandas series x and possibly the layers in the form of y.\n\n    Args:\n        x: Input for fitting the encoder\n        y: Optional labels that might be used in fitting the encoder. Defaults to None.\n        kwargs: Other settings to be processed\n\n    Returns:\n        The fitted FeatureEncoder object.\n    \"\"\"\n    self.feature_name_in = x.name\n    self.feature_type_in = self._get_feature_type(x)\n    input = validate_shape(x.values, self.n_dim_in)\n    output = self._fit(input, **kwargs)._transform(input)\n    self._out_shape = (-1, *output.shape[1:])  # for inverse_transform\n    output = validate_shape(output, self.n_dim_out)\n    if self.n_dim_out == 1:\n        self.n_features_out = 1\n    else:\n        self.n_features_out = output.shape[1]\n    self.feature_names_out = self.get_feature_names_out()\n    self.feature_types_out = self.get_feature_types_out(output)\n    return self\n</code></pre> <code></code> transform \u00b6 <pre><code>transform(x)\n</code></pre> <p>Take in the Series and use the encoder to transform the series after fitting.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The input to be transformed.</p> required <p>Returns:</p> Type Description <code>DataFrame | Series</code> <p>The transformed input.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/feature_encoder.py</code> <pre><code>def transform(self, x: pd.Series) -&gt; pd.DataFrame | pd.Series:\n    \"\"\"\n    Take in the Series and use the encoder to transform the series after fitting.\n\n    Args:\n        x: The input to be transformed.\n\n    Returns:\n        The transformed input.\n    \"\"\"\n    data = validate_shape(x.values, self.n_dim_in)\n    out = self._transform(data)\n    out = validate_shape(out, self.n_dim_out)\n    if self.n_dim_out == 1:\n        return pd.Series(out, name=self.feature_name_in)\n    return pd.DataFrame(out, columns=self.feature_names_out)\n</code></pre> <code></code> get_feature_names_out \u00b6 <pre><code>get_feature_names_out()\n</code></pre> <p>A list of the names of the features being encoded.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/feature_encoder.py</code> <pre><code>def get_feature_names_out(self) -&gt; list[str]:\n    \"\"\"A list of the names of the features being encoded.\"\"\"\n    n = self.n_features_out\n    if n == 1:\n        return [self.feature_name_in]\n    return [f\"{self.feature_name_in}_{i}\" for i in range(n)]\n</code></pre> <code></code> get_feature_types_out \u00b6 <pre><code>get_feature_types_out(output)\n</code></pre> <p>A list of the name of the features produced by the encoder.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/feature_encoder.py</code> <pre><code>def get_feature_types_out(self, output: np.ndarray) -&gt; list[str]:\n    \"\"\"A list of the name of the features produced by the encoder.\"\"\"\n    t = self._get_feature_type(output)\n    return [t] * self.n_features_out\n</code></pre> <code></code> inverse_transform \u00b6 <pre><code>inverse_transform(df)\n</code></pre> <p>Reverse the encoder mapping.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/feature_encoder.py</code> <pre><code>def inverse_transform(self, df: pd.DataFrame | pd.Series) -&gt; pd.Series:\n    \"\"\"Reverse the encoder mapping.\"\"\"\n    y = df.values.reshape(self._out_shape)\n    x = self._inverse_transform(y)\n    x = validate_shape(x, 1)\n    return pd.Series(x, name=self.feature_name_in)\n</code></pre> <code></code> wraps <code>classmethod</code> \u00b6 <pre><code>wraps(encoder_class, **params)\n</code></pre> <p>Wraps sklearn transformer to FeatureEncoder.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/feature_encoder.py</code> <pre><code>@classmethod\ndef wraps(cls: type, encoder_class: TransformerMixin, **params: Any) -&gt; type[FeatureEncoder]:\n    \"\"\"Wraps sklearn transformer to FeatureEncoder.\"\"\"\n\n    class WrappedEncoder(FeatureEncoder):\n        def __init__(self, n_dim_in: int = 2, *args: Any, **kwargs: Any) -&gt; None:\n            self.encoder = encoder_class(n_dim_in, *args, **kwargs)\n\n        def _fit(self, x: np.ndarray, **kwargs: Any) -&gt; FeatureEncoder:\n            self.encoder.fit(x, **kwargs)\n            return self\n\n        def _transform(self, x: np.ndarray) -&gt; np.ndarray:\n            return self.encoder.transform(x)\n\n        def _inverse_transform(self, data: np.ndarray) -&gt; np.ndarray:\n            return self.encoder.inverse_transform(data)\n\n        def get_feature_names_out(self) -&gt; list[str]:\n            return list(self.encoder.get_feature_names_out([self.feature_name_in]))\n\n    for attr in (\"__name__\", \"__qualname__\", \"__doc__\"):\n        setattr(WrappedEncoder, attr, getattr(encoder_class, attr))\n    for attr, val in params.items():\n        setattr(WrappedEncoder, attr, val)\n\n    return WrappedEncoder\n</code></pre> <code></code> DatetimeEncoder \u00b6 <p>               Bases: <code>FeatureEncoder</code></p> <p>Datetime variables encoder.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/feature_encoder.py</code> <pre><code>class DatetimeEncoder(FeatureEncoder):\n    \"\"\"Datetime variables encoder.\"\"\"\n\n    def __init__(self, n_dim_in: int = 1, n_dim_out: int = 1):\n        \"\"\"\n        Datetime variables encoder.\n\n        Args:\n            n_dim_in: Size of the input to the feature encoder. Defaults to 1.\n            n_dim_out: Size of the output from the feature encoder. Defaults to 1.\n        \"\"\"\n        super().__init__(n_dim_in, n_dim_out)\n\n    def _transform(self, x: np.ndarray) -&gt; np.ndarray:\n        return pd.to_numeric(x).astype(float)\n\n    def _inverse_transform(self, data: np.ndarray) -&gt; np.ndarray:\n        return pd.to_datetime(data)\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(n_dim_in=1, n_dim_out=1)\n</code></pre> <p>Datetime variables encoder.</p> <p>Parameters:</p> Name Type Description Default <code>n_dim_in</code> <code>int</code> <p>Size of the input to the feature encoder. Defaults to 1.</p> <code>1</code> <code>n_dim_out</code> <code>int</code> <p>Size of the output from the feature encoder. Defaults to 1.</p> <code>1</code> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/feature_encoder.py</code> <pre><code>def __init__(self, n_dim_in: int = 1, n_dim_out: int = 1):\n    \"\"\"\n    Datetime variables encoder.\n\n    Args:\n        n_dim_in: Size of the input to the feature encoder. Defaults to 1.\n        n_dim_out: Size of the output from the feature encoder. Defaults to 1.\n    \"\"\"\n    super().__init__(n_dim_in, n_dim_out)\n</code></pre> <code></code> validate_shape \u00b6 <pre><code>validate_shape(x, n_dim)\n</code></pre> <p>Perform validation of the shape of x against the specified <code>n_dim</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Numpy array to be validated</p> required <code>n_dim</code> <code>int</code> <p>value that determines the validation</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Thrown when there is a dissonance between the <code>n_dim</code> and data shape of x)</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Returns the data, as long as it has the right shape.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/feature_encoder.py</code> <pre><code>def validate_shape(x: np.ndarray, n_dim: int) -&gt; np.ndarray:\n    \"\"\"\n    Perform validation of the shape of x against the specified ``n_dim``.\n\n    Args:\n        x: Numpy array to be validated\n        n_dim: value that determines the validation\n\n    Raises:\n        ValueError: Thrown when there is a dissonance between the ``n_dim`` and data shape of x)\n\n    Returns:\n        Returns the data, as long as it has the right shape.\n    \"\"\"\n    if n_dim == 1:\n        if x.ndim == 2:\n            x = np.squeeze(x, axis=1)\n        if x.ndim != 1:\n            raise ValueError(\"array must be 1D\")\n        return x\n    if n_dim == 2:\n        if x.ndim == 1:\n            x = x.reshape(-1, 1)\n        if x.ndim != 2:\n            raise ValueError(\"array must be 2D\")\n        return x\n    raise ValueError(\"n_dim must be 1 or 2\")\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.quality.synthcity.metric","title":"metric","text":"MetricEvaluator \u00b6 Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/metric.py</code> <pre><code>class MetricEvaluator(metaclass=ABCMeta):\n    def __init__(\n        self,\n        reduction: str = \"mean\",\n        n_histogram_bins: int = 10,\n        n_folds: int = 3,\n        task_type: str = \"classification\",\n        random_state: int = 0,\n        workspace: Path = Path(\"workspace\"),\n        use_cache: bool = True,\n        default_metric: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Base class for all metrics.\n\n        If any method implementation is missing, the class constructor will fail.\n\n        Args:\n            reduction: The way to aggregate metrics across folds. Defaults to \"mean\".\n            n_histogram_bins: The number of bins used in histogram calculation. Defaults to 10.\n            n_folds: The number of folds in cross validation. Defaults to 3.\n            task_type: The type of downstream task.. Defaults to \"classification\".\n            random_state: Random state seed. Defaults to 0.\n            workspace: The directory to save intermediate models or results.. Defaults to Path(\"workspace\").\n            use_cache:  Whether to use cache. If True, it will try to load saved results in workspace directory\n                where possible. Defaults to True.\n            default_metric: Type of metric to be used if one not specified. Defaults to None.\n        \"\"\"\n        self._reduction = reduction\n        self._n_histogram_bins = n_histogram_bins\n        self._n_folds = n_folds\n\n        self._task_type = task_type\n        self._random_state = random_state\n        self._workspace = workspace\n        self._use_cache = use_cache\n        if default_metric is None:\n            default_metric = reduction\n        self._default_metric = default_metric\n\n        workspace.mkdir(parents=True, exist_ok=True)\n\n    @abstractmethod\n    def evaluate(self, x_gt: DataLoader, x_syn: DataLoader) -&gt; dict:\n        \"\"\"Compare two datasets and return a dictionary of metrics.\"\"\"\n        ...\n\n    @abstractmethod\n    def evaluate_default(self, x_gt: DataLoader, x_syn: DataLoader) -&gt; float:\n        \"\"\"Default evaluation.\"\"\"\n        ...\n\n    @staticmethod\n    @abstractmethod\n    def direction() -&gt; str:\n        \"\"\"Direction of metric (bigger better or smaller better).\"\"\"\n        ...\n\n    @staticmethod\n    @abstractmethod\n    def type() -&gt; str:\n        \"\"\"Type of metric.\"\"\"\n        ...\n\n    @staticmethod\n    @abstractmethod\n    def name() -&gt; str:\n        \"\"\"Name of the metric.\"\"\"\n        ...\n\n    @classmethod\n    def fqdn(cls) -&gt; str:\n        \"\"\"No idea.\"\"\"\n        return f\"{cls.type()}.{cls.name()}\"\n\n    def reduction(self) -&gt; Callable:\n        \"\"\"The way in which the input should be reduced if necessary.\"\"\"\n        if self._reduction == \"mean\":\n            return np.mean\n        if self._reduction == \"max\":\n            return np.max\n        if self._reduction == \"min\":\n            return np.min\n        raise ValueError(f\"Unknown reduction {self._reduction}\")\n\n    def _get_oneclass_model(self, x_gt: np.ndarray) -&gt; OneClassLayer:\n        model = OneClassLayer(\n            input_dim=x_gt.shape[1],\n            rep_dim=x_gt.shape[1],\n            center=torch.ones(x_gt.shape[1]) * 10,\n        )\n        model.fit(torch.from_numpy(x_gt))\n\n        return model.to(DEVICE)\n\n    def _oneclass_predict(self, model: OneClassLayer, X: np.ndarray) -&gt; np.ndarray:\n        with torch.no_grad():\n            return model(torch.from_numpy(X).float().to(DEVICE)).cpu().detach().numpy()\n\n    def use_cache(self, path: Path) -&gt; bool:\n        \"\"\"Whether to save information to the provided path.\"\"\"\n        return path.exists() and self._use_cache\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(\n    reduction=\"mean\",\n    n_histogram_bins=10,\n    n_folds=3,\n    task_type=\"classification\",\n    random_state=0,\n    workspace=Path(\"workspace\"),\n    use_cache=True,\n    default_metric=None,\n)\n</code></pre> <p>Base class for all metrics.</p> <p>If any method implementation is missing, the class constructor will fail.</p> <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>str</code> <p>The way to aggregate metrics across folds. Defaults to \"mean\".</p> <code>'mean'</code> <code>n_histogram_bins</code> <code>int</code> <p>The number of bins used in histogram calculation. Defaults to 10.</p> <code>10</code> <code>n_folds</code> <code>int</code> <p>The number of folds in cross validation. Defaults to 3.</p> <code>3</code> <code>task_type</code> <code>str</code> <p>The type of downstream task.. Defaults to \"classification\".</p> <code>'classification'</code> <code>random_state</code> <code>int</code> <p>Random state seed. Defaults to 0.</p> <code>0</code> <code>workspace</code> <code>Path</code> <p>The directory to save intermediate models or results.. Defaults to Path(\"workspace\").</p> <code>Path('workspace')</code> <code>use_cache</code> <code>bool</code> <p>Whether to use cache. If True, it will try to load saved results in workspace directory where possible. Defaults to True.</p> <code>True</code> <code>default_metric</code> <code>str | None</code> <p>Type of metric to be used if one not specified. Defaults to None.</p> <code>None</code> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/metric.py</code> <pre><code>def __init__(\n    self,\n    reduction: str = \"mean\",\n    n_histogram_bins: int = 10,\n    n_folds: int = 3,\n    task_type: str = \"classification\",\n    random_state: int = 0,\n    workspace: Path = Path(\"workspace\"),\n    use_cache: bool = True,\n    default_metric: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Base class for all metrics.\n\n    If any method implementation is missing, the class constructor will fail.\n\n    Args:\n        reduction: The way to aggregate metrics across folds. Defaults to \"mean\".\n        n_histogram_bins: The number of bins used in histogram calculation. Defaults to 10.\n        n_folds: The number of folds in cross validation. Defaults to 3.\n        task_type: The type of downstream task.. Defaults to \"classification\".\n        random_state: Random state seed. Defaults to 0.\n        workspace: The directory to save intermediate models or results.. Defaults to Path(\"workspace\").\n        use_cache:  Whether to use cache. If True, it will try to load saved results in workspace directory\n            where possible. Defaults to True.\n        default_metric: Type of metric to be used if one not specified. Defaults to None.\n    \"\"\"\n    self._reduction = reduction\n    self._n_histogram_bins = n_histogram_bins\n    self._n_folds = n_folds\n\n    self._task_type = task_type\n    self._random_state = random_state\n    self._workspace = workspace\n    self._use_cache = use_cache\n    if default_metric is None:\n        default_metric = reduction\n    self._default_metric = default_metric\n\n    workspace.mkdir(parents=True, exist_ok=True)\n</code></pre> <code></code> evaluate <code>abstractmethod</code> \u00b6 <pre><code>evaluate(x_gt, x_syn)\n</code></pre> <p>Compare two datasets and return a dictionary of metrics.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/metric.py</code> <pre><code>@abstractmethod\ndef evaluate(self, x_gt: DataLoader, x_syn: DataLoader) -&gt; dict:\n    \"\"\"Compare two datasets and return a dictionary of metrics.\"\"\"\n    ...\n</code></pre> <code></code> evaluate_default <code>abstractmethod</code> \u00b6 <pre><code>evaluate_default(x_gt, x_syn)\n</code></pre> <p>Default evaluation.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/metric.py</code> <pre><code>@abstractmethod\ndef evaluate_default(self, x_gt: DataLoader, x_syn: DataLoader) -&gt; float:\n    \"\"\"Default evaluation.\"\"\"\n    ...\n</code></pre> <code></code> direction <code>abstractmethod</code> <code>staticmethod</code> \u00b6 <pre><code>direction()\n</code></pre> <p>Direction of metric (bigger better or smaller better).</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/metric.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef direction() -&gt; str:\n    \"\"\"Direction of metric (bigger better or smaller better).\"\"\"\n    ...\n</code></pre> <code></code> type <code>abstractmethod</code> <code>staticmethod</code> \u00b6 <pre><code>type()\n</code></pre> <p>Type of metric.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/metric.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef type() -&gt; str:\n    \"\"\"Type of metric.\"\"\"\n    ...\n</code></pre> <code></code> name <code>abstractmethod</code> <code>staticmethod</code> \u00b6 <pre><code>name()\n</code></pre> <p>Name of the metric.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/metric.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef name() -&gt; str:\n    \"\"\"Name of the metric.\"\"\"\n    ...\n</code></pre> <code></code> fqdn <code>classmethod</code> \u00b6 <pre><code>fqdn()\n</code></pre> <p>No idea.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/metric.py</code> <pre><code>@classmethod\ndef fqdn(cls) -&gt; str:\n    \"\"\"No idea.\"\"\"\n    return f\"{cls.type()}.{cls.name()}\"\n</code></pre> <code></code> reduction \u00b6 <pre><code>reduction()\n</code></pre> <p>The way in which the input should be reduced if necessary.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/metric.py</code> <pre><code>def reduction(self) -&gt; Callable:\n    \"\"\"The way in which the input should be reduced if necessary.\"\"\"\n    if self._reduction == \"mean\":\n        return np.mean\n    if self._reduction == \"max\":\n        return np.max\n    if self._reduction == \"min\":\n        return np.min\n    raise ValueError(f\"Unknown reduction {self._reduction}\")\n</code></pre> <code></code> use_cache \u00b6 <pre><code>use_cache(path)\n</code></pre> <p>Whether to save information to the provided path.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/metric.py</code> <pre><code>def use_cache(self, path: Path) -&gt; bool:\n    \"\"\"Whether to save information to the provided path.\"\"\"\n    return path.exists() and self._use_cache\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.quality.synthcity.networks","title":"networks","text":"build_network \u00b6 <pre><code>build_network(network_name, params)\n</code></pre> <p>Placeholder for now. Would be a factory type method if there where more than one option.</p> <p>Parameters:</p> Name Type Description Default <code>network_name</code> <code>str</code> <p>Name of the network to be generated.</p> required <code>params</code> <code>dict</code> <p>Parameters/configuration used to create the network in a custom way.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>A torch module to be trained.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/networks.py</code> <pre><code>def build_network(network_name: str, params: dict) -&gt; nn.Module:\n    \"\"\"\n    Placeholder for now. Would be a factory type method if there where more than one option.\n\n    Args:\n        network_name: Name of the network to be generated.\n        params: Parameters/configuration used to create the network in a custom way.\n\n    Returns:\n        A torch module to be trained.\n    \"\"\"\n    if network_name == \"feedforward\":\n        return feedforward_network(params)\n    raise ValueError(\"Network name not recognized.\")\n</code></pre> <code></code> feedforward_network \u00b6 <pre><code>feedforward_network(params)\n</code></pre> <p>Architecture for a Feedforward Neural Network.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Has keys [\"input_dim\", \"rep_dim\", \"num_hidden\", \"activation\", \"num_layers\", \"dropout_prob\", \"dropout_active\", \"LossFn\". These determine the architecture structure</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The constructed network.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/networks.py</code> <pre><code>def feedforward_network(params: dict) -&gt; nn.Module:\n    \"\"\"\n    Architecture for a Feedforward Neural Network.\n\n    Args:\n        params: Has keys [\"input_dim\", \"rep_dim\", \"num_hidden\", \"activation\", \"num_layers\", \"dropout_prob\",\n            \"dropout_active\", \"LossFn\". These determine the architecture structure\n\n    Returns:\n        The constructed network.\n    \"\"\"\n    modules: list[nn.Module] = []\n\n    if params[\"dropout_active\"]:\n        modules.append(torch.nn.Dropout(p=params[\"dropout_prob\"]))\n\n    # Input layer\n\n    modules.append(torch.nn.Linear(params[\"input_dim\"], params[\"num_hidden\"], bias=False))\n    modules.append(ACTIVATION_DICT[params[\"activation\"]])\n\n    # Intermediate layers\n\n    for _ in range(params[\"num_layers\"] - 1):\n        if params[\"dropout_active\"]:\n            modules.append(torch.nn.Dropout(p=params[\"dropout_prob\"]))\n\n        modules.append(torch.nn.Linear(params[\"num_hidden\"], params[\"num_hidden\"], bias=False))\n        modules.append(ACTIVATION_DICT[params[\"activation\"]])\n\n    # Output layer\n\n    modules.append(torch.nn.Linear(params[\"num_hidden\"], params[\"rep_dim\"], bias=False))\n\n    return nn.Sequential(*modules)\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.quality.synthcity.one_class","title":"one_class","text":"BaseNet \u00b6 <p>               Bases: <code>Module</code></p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/one_class.py</code> <pre><code>class BaseNet(nn.Module):\n    def __init__(self) -&gt; None:\n        \"\"\"Base class for all neural networks.\"\"\"\n        super().__init__()\n\n    def forward(self, X: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Abstract forward pass through the network.\n\n        Args:\n            X: input to the network\n\n        Raises:\n            NotImplementedError: Must be implemented by the inheriting network\n\n        Returns:\n            Output of the network\n        \"\"\"\n        raise NotImplementedError\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__()\n</code></pre> <p>Base class for all neural networks.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/one_class.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Base class for all neural networks.\"\"\"\n    super().__init__()\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(X)\n</code></pre> <p>Abstract forward pass through the network.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>input to the network</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented by the inheriting network</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output of the network</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/one_class.py</code> <pre><code>def forward(self, X: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Abstract forward pass through the network.\n\n    Args:\n        X: input to the network\n\n    Raises:\n        NotImplementedError: Must be implemented by the inheriting network\n\n    Returns:\n        Output of the network\n    \"\"\"\n    raise NotImplementedError\n</code></pre> <code></code> OneClassLayer \u00b6 <p>               Bases: <code>BaseNet</code></p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/one_class.py</code> <pre><code>class OneClassLayer(BaseNet):\n    def __init__(\n        self,\n        input_dim: int,\n        rep_dim: int,\n        center: torch.Tensor,\n        num_layers: int = 4,\n        num_hidden: int = 32,\n        activation: str = \"ReLU\",\n        dropout_prob: float = 0.2,\n        dropout_active: bool = False,\n        lr: float = 2e-3,\n        epochs: int = 1000,\n        warm_up_epochs: int = 20,\n        train_prop: float = 1.0,\n        weight_decay: float = 2e-3,\n        radius: float = 1,\n        nu: float = 1e-2,\n    ):\n        \"\"\"Neural network.\"\"\"\n        super().__init__()\n\n        self.rep_dim = rep_dim\n        self.input_dim = input_dim\n        self.num_layers = num_layers\n        self.num_hidden = num_hidden\n        self.activation = activation\n        self.dropout_prob = dropout_prob\n        self.dropout_active = dropout_active\n        self.train_prop = train_prop\n        self.learningRate = lr\n        self.epochs = epochs\n        self.warm_up_epochs = warm_up_epochs\n        self.weight_decay = weight_decay\n        if torch.cuda.is_available():\n            self.device = torch.device(\"cuda\")  # Make this an option\n        else:\n            self.device = torch.device(\"cpu\")\n        # set up the network\n\n        self.model = build_network(\n            network_name=\"feedforward\",\n            params={\n                \"input_dim\": input_dim,\n                \"rep_dim\": rep_dim,\n                \"num_hidden\": num_hidden,\n                \"activation\": activation,\n                \"num_layers\": num_layers,\n                \"dropout_prob\": dropout_prob,\n                \"dropout_active\": dropout_active,\n                \"LossFn\": \"SoftBoundary\",\n            },\n        ).to(self.device)\n\n        # create the loss function\n\n        self.c = center.to(self.device)\n        self.r = radius\n        self.nu = nu\n\n        self.loss_fn = soft_boundary_loss\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Pass the input through the network for a forward pass.\n\n        Args:\n            x: Input tensor\n\n        Returns:\n            Output tensors.\n        \"\"\"\n        return self.model(x)\n\n    def fit(self, x_train: torch.Tensor) -&gt; None:\n        \"\"\"\n        Perform a training step using the ``x_train`` tensor.\n\n        Args:\n            x_train: Batch of training data.\n        \"\"\"\n        self.optimizer = torch.optim.AdamW(\n            self.model.parameters(),\n            lr=self.learningRate,\n            weight_decay=self.weight_decay,\n        )\n        self.X = torch.tensor(x_train.reshape((-1, self.input_dim))).float()\n\n        if self.train_prop != 1:\n            x_train, x_val = (\n                x_train[: int(self.train_prop * len(x_train))],\n                x_train[int(self.train_prop * len(x_train)) :],\n            )\n            inputs_val = Variable(x_val.to(self.device)).float()\n\n        self.losses = []\n        self.loss_vals = []\n\n        for epoch in range(self.epochs):\n            # Converting inputs and labels to Variable\n            inputs = Variable(x_train).to(self.device).float()\n\n            self.model.zero_grad()\n\n            self.optimizer.zero_grad()\n\n            # get output from the model, given the inputs\n            outputs = self.model(inputs)\n\n            # get loss for the predicted output\n            self.loss = self.loss_fn(outputs=outputs, r=torch.Tensor([self.r]), c=self.c, nu=torch.Tensor([self.nu]))\n\n            # get gradients w.r.t to parameters\n            self.loss.backward(retain_graph=True)\n            self.losses.append(self.loss.detach().cpu().numpy())\n\n            # update parameters\n            self.optimizer.step()\n\n            if self.train_prop != 1.0:\n                with torch.no_grad():\n                    # get output from the model, given the inputs\n                    outputs = self.model(inputs_val)\n\n                    # get loss for the predicted output\n                    loss_val = self.loss_fn(\n                        outputs=outputs, r=torch.Tensor([self.r]), c=self.c, nu=torch.Tensor([self.nu])\n                    )\n\n                    self.loss_vals.append(loss_val)\n\n            if self.train_prop == 1:\n                log(DEBUG, \"epoch {}, loss {}\".format(epoch, self.loss.item()))\n            else:\n                log(DEBUG, \"epoch {:4}, train loss {:.4e}, val loss {:.4e}\".format(epoch, self.loss.item(), loss_val))\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(\n    input_dim,\n    rep_dim,\n    center,\n    num_layers=4,\n    num_hidden=32,\n    activation=\"ReLU\",\n    dropout_prob=0.2,\n    dropout_active=False,\n    lr=0.002,\n    epochs=1000,\n    warm_up_epochs=20,\n    train_prop=1.0,\n    weight_decay=0.002,\n    radius=1,\n    nu=0.01,\n)\n</code></pre> <p>Neural network.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/one_class.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    rep_dim: int,\n    center: torch.Tensor,\n    num_layers: int = 4,\n    num_hidden: int = 32,\n    activation: str = \"ReLU\",\n    dropout_prob: float = 0.2,\n    dropout_active: bool = False,\n    lr: float = 2e-3,\n    epochs: int = 1000,\n    warm_up_epochs: int = 20,\n    train_prop: float = 1.0,\n    weight_decay: float = 2e-3,\n    radius: float = 1,\n    nu: float = 1e-2,\n):\n    \"\"\"Neural network.\"\"\"\n    super().__init__()\n\n    self.rep_dim = rep_dim\n    self.input_dim = input_dim\n    self.num_layers = num_layers\n    self.num_hidden = num_hidden\n    self.activation = activation\n    self.dropout_prob = dropout_prob\n    self.dropout_active = dropout_active\n    self.train_prop = train_prop\n    self.learningRate = lr\n    self.epochs = epochs\n    self.warm_up_epochs = warm_up_epochs\n    self.weight_decay = weight_decay\n    if torch.cuda.is_available():\n        self.device = torch.device(\"cuda\")  # Make this an option\n    else:\n        self.device = torch.device(\"cpu\")\n    # set up the network\n\n    self.model = build_network(\n        network_name=\"feedforward\",\n        params={\n            \"input_dim\": input_dim,\n            \"rep_dim\": rep_dim,\n            \"num_hidden\": num_hidden,\n            \"activation\": activation,\n            \"num_layers\": num_layers,\n            \"dropout_prob\": dropout_prob,\n            \"dropout_active\": dropout_active,\n            \"LossFn\": \"SoftBoundary\",\n        },\n    ).to(self.device)\n\n    # create the loss function\n\n    self.c = center.to(self.device)\n    self.r = radius\n    self.nu = nu\n\n    self.loss_fn = soft_boundary_loss\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(x)\n</code></pre> <p>Pass the input through the network for a forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensors.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/one_class.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Pass the input through the network for a forward pass.\n\n    Args:\n        x: Input tensor\n\n    Returns:\n        Output tensors.\n    \"\"\"\n    return self.model(x)\n</code></pre> <code></code> fit \u00b6 <pre><code>fit(x_train)\n</code></pre> <p>Perform a training step using the <code>x_train</code> tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>Tensor</code> <p>Batch of training data.</p> required Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/one_class.py</code> <pre><code>def fit(self, x_train: torch.Tensor) -&gt; None:\n    \"\"\"\n    Perform a training step using the ``x_train`` tensor.\n\n    Args:\n        x_train: Batch of training data.\n    \"\"\"\n    self.optimizer = torch.optim.AdamW(\n        self.model.parameters(),\n        lr=self.learningRate,\n        weight_decay=self.weight_decay,\n    )\n    self.X = torch.tensor(x_train.reshape((-1, self.input_dim))).float()\n\n    if self.train_prop != 1:\n        x_train, x_val = (\n            x_train[: int(self.train_prop * len(x_train))],\n            x_train[int(self.train_prop * len(x_train)) :],\n        )\n        inputs_val = Variable(x_val.to(self.device)).float()\n\n    self.losses = []\n    self.loss_vals = []\n\n    for epoch in range(self.epochs):\n        # Converting inputs and labels to Variable\n        inputs = Variable(x_train).to(self.device).float()\n\n        self.model.zero_grad()\n\n        self.optimizer.zero_grad()\n\n        # get output from the model, given the inputs\n        outputs = self.model(inputs)\n\n        # get loss for the predicted output\n        self.loss = self.loss_fn(outputs=outputs, r=torch.Tensor([self.r]), c=self.c, nu=torch.Tensor([self.nu]))\n\n        # get gradients w.r.t to parameters\n        self.loss.backward(retain_graph=True)\n        self.losses.append(self.loss.detach().cpu().numpy())\n\n        # update parameters\n        self.optimizer.step()\n\n        if self.train_prop != 1.0:\n            with torch.no_grad():\n                # get output from the model, given the inputs\n                outputs = self.model(inputs_val)\n\n                # get loss for the predicted output\n                loss_val = self.loss_fn(\n                    outputs=outputs, r=torch.Tensor([self.r]), c=self.c, nu=torch.Tensor([self.nu])\n                )\n\n                self.loss_vals.append(loss_val)\n\n        if self.train_prop == 1:\n            log(DEBUG, \"epoch {}, loss {}\".format(epoch, self.loss.item()))\n        else:\n            log(DEBUG, \"epoch {:4}, train loss {:.4e}, val loss {:.4e}\".format(epoch, self.loss.item(), loss_val))\n</code></pre> <code></code> one_class_loss \u00b6 <pre><code>one_class_loss(outputs, c)\n</code></pre> <p>Computes the sum of the Euclidean distances from the center tensor (c) and then the mean over the sum.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>Tensor</code> <p>Output from the neural network for the batch.</p> required <code>c</code> <code>Tensor</code> <p>center point, from which we're measuring the Euclidean distances.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Mean distances.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/one_class.py</code> <pre><code>def one_class_loss(outputs: torch.Tensor, c: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the sum of the Euclidean distances from the center tensor (c) and then the mean over the sum.\n\n    Args:\n        outputs: Output from the neural network for the batch.\n        c: center point, from which we're measuring the Euclidean distances.\n\n    Returns:\n        Mean distances.\n    \"\"\"\n    dist = torch.sum((outputs - c) ** 2, dim=1)\n    return torch.mean(dist)\n</code></pre> <code></code> soft_boundary_loss \u00b6 <pre><code>soft_boundary_loss(outputs, r, c, nu)\n</code></pre> <p>A similar loss function to the one class loss but with some small modifications.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/one_class.py</code> <pre><code>def soft_boundary_loss(outputs: torch.Tensor, r: torch.Tensor, c: torch.Tensor, nu: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"A similar loss function to the one class loss but with some small modifications.\"\"\"\n    dist = torch.sum((outputs - c) ** 2, dim=1)\n    scores = dist\n    return (1 / nu) * torch.mean(torch.max(torch.zeros_like(scores), scores))\n</code></pre> <code></code> get_radius \u00b6 <pre><code>get_radius(dist, nu)\n</code></pre> <p>Optimally solve for radius R via the (1-nu)-quantile of distances.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <code>Tensor</code> <p>Distances tensor</p> required <code>nu</code> <code>float</code> <p>hyper-parameter for the quantile</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Radii</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/one_class.py</code> <pre><code>def get_radius(dist: torch.Tensor, nu: float) -&gt; np.ndarray:\n    \"\"\"\n    Optimally solve for radius R via the (1-nu)-quantile of distances.\n\n    Args:\n        dist: Distances tensor\n        nu: hyper-parameter for the quantile\n\n    Returns:\n        Radii\n    \"\"\"\n    return np.quantile(np.sqrt(dist.clone().data.float().cpu().numpy()), 1 - nu)\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.quality.synthcity.statistical_eval","title":"statistical_eval","text":"StatisticalEvaluator \u00b6 <p>               Bases: <code>MetricEvaluator</code></p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/statistical_eval.py</code> <pre><code>class StatisticalEvaluator(MetricEvaluator):\n    def __init__(self, **kwargs: Any) -&gt; None:\n        \"\"\"Base class for statistical evaluators to inherit from.\"\"\"\n        super().__init__(**kwargs)\n\n    @staticmethod\n    def type() -&gt; str:\n        \"\"\"Type of evaluator.\"\"\"\n        return \"stats\"\n\n    @abstractmethod\n    def _evaluate(self, X_gt: DataLoader, X_syn: DataLoader) -&gt; dict: ...\n\n    def evaluate(self, X_gt: DataLoader, X_syn: DataLoader) -&gt; dict:\n        \"\"\"\n        Performs evaluation using the ground truth and synthetic datasets as dataloaders by calling the internal\n        ``_evaluate`` function of inheriting classes.\n\n        Args:\n            X_gt: Dataloader with ground truth (real) data.\n            X_syn: Dataloader with synthetically generated data.\n\n        Returns:\n            A dictionary of results from the evaluation\n        \"\"\"\n        return self._evaluate(X_gt, X_syn)\n\n    def evaluate_default(\n        self,\n        x_gt: DataLoader,\n        x_syn: DataLoader,\n    ) -&gt; float:\n        \"\"\"Perform a default evaluation if one is not specified.\"\"\"\n        return self.evaluate(x_gt, x_syn)[self._default_metric]\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(**kwargs)\n</code></pre> <p>Base class for statistical evaluators to inherit from.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/statistical_eval.py</code> <pre><code>def __init__(self, **kwargs: Any) -&gt; None:\n    \"\"\"Base class for statistical evaluators to inherit from.\"\"\"\n    super().__init__(**kwargs)\n</code></pre> <code></code> type <code>staticmethod</code> \u00b6 <pre><code>type()\n</code></pre> <p>Type of evaluator.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/statistical_eval.py</code> <pre><code>@staticmethod\ndef type() -&gt; str:\n    \"\"\"Type of evaluator.\"\"\"\n    return \"stats\"\n</code></pre> <code></code> evaluate \u00b6 <pre><code>evaluate(X_gt, X_syn)\n</code></pre> <p>Performs evaluation using the ground truth and synthetic datasets as dataloaders by calling the internal <code>_evaluate</code> function of inheriting classes.</p> <p>Parameters:</p> Name Type Description Default <code>X_gt</code> <code>DataLoader</code> <p>Dataloader with ground truth (real) data.</p> required <code>X_syn</code> <code>DataLoader</code> <p>Dataloader with synthetically generated data.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary of results from the evaluation</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/statistical_eval.py</code> <pre><code>def evaluate(self, X_gt: DataLoader, X_syn: DataLoader) -&gt; dict:\n    \"\"\"\n    Performs evaluation using the ground truth and synthetic datasets as dataloaders by calling the internal\n    ``_evaluate`` function of inheriting classes.\n\n    Args:\n        X_gt: Dataloader with ground truth (real) data.\n        X_syn: Dataloader with synthetically generated data.\n\n    Returns:\n        A dictionary of results from the evaluation\n    \"\"\"\n    return self._evaluate(X_gt, X_syn)\n</code></pre> <code></code> evaluate_default \u00b6 <pre><code>evaluate_default(x_gt, x_syn)\n</code></pre> <p>Perform a default evaluation if one is not specified.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/statistical_eval.py</code> <pre><code>def evaluate_default(\n    self,\n    x_gt: DataLoader,\n    x_syn: DataLoader,\n) -&gt; float:\n    \"\"\"Perform a default evaluation if one is not specified.\"\"\"\n    return self.evaluate(x_gt, x_syn)[self._default_metric]\n</code></pre> <code></code> AlphaPrecision \u00b6 <p>               Bases: <code>StatisticalEvaluator</code></p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/statistical_eval.py</code> <pre><code>class AlphaPrecision(StatisticalEvaluator):\n    def __init__(self, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Evaluates the alpha-precision, beta-recall, and authenticity scores.\n\n        The class evaluates the synthetic data using a tuple of three metrics:\n\n            alpha-precision, beta-recall, and authenticity.\n\n        Note that these metrics can be evaluated for each synthetic data point (which are useful for auditing and\n        post-processing). Here we average the scores to reflect the overall quality of the data.\n        The formal definitions can be found in the reference below:\n\n        Alaa, Ahmed, Boris Van Breugel, Evgeny S. Saveliev, and Mihaela van der Schaar. \"How faithful is your synthetic\n        data? sample-level metrics for evaluating and auditing generative models.\"\n        In International Conference on Machine Learning, pp. 290-306. PMLR, 2022.\n        \"\"\"\n        super().__init__(default_metric=\"authenticity_OC\", **kwargs)\n\n    @staticmethod\n    def name() -&gt; str:\n        \"\"\"Return name.\"\"\"\n        return \"alpha_precision\"\n\n    @staticmethod\n    def direction() -&gt; str:\n        \"\"\"Return optimization direction.\"\"\"\n        return \"maximize\"\n\n    def metrics(\n        self,\n        x: np.ndarray,\n        x_syn: np.ndarray,\n        emb_center: np.ndarray | None = None,\n    ) -&gt; tuple[list[float], list[float], list[float], float, float, float]:\n        \"\"\"\n        Compute the alpha-precision, beta-recall, and authenticity scores provided real data (x) and synthetic\n        data (x_syn). If ``emb_center`` is provided this are \"non-naive\" metrics. If it is none, these constitute\n        \"naive\" scores.\n\n        Args:\n            x: Real data\n            x_syn: Synthetically generated data.\n            emb_center: Center for the embeddings of the data. If None, we just use the mean of the features of x).\n                Defaults to None.\n\n        Raises:\n            RuntimeError: Raised if the datasets are not the same sizes.\n            RuntimeError: Raised if there is an invalid score for delta_precision_alpha.\n            RuntimeError: Raised if there is an invalid score for delta_coverage_beta.\n\n        Returns:\n            alphas, alpha_precision_curve, beta_coverage_curve, delta_precision_alpha, delta_coverage_beta,\n            authenticity.\n        \"\"\"\n        if len(x) != len(x_syn):\n            raise RuntimeError(\"The real and synthetic data must have the same length\")\n\n        if emb_center is None:\n            emb_center = np.mean(x, axis=0)\n\n        n_steps = 30\n        alphas = np.linspace(0, 1, n_steps)\n\n        radii = np.quantile(np.sqrt(np.sum((x - emb_center) ** 2, axis=1)), alphas)\n\n        synth_center = np.mean(x_syn, axis=0)\n\n        alpha_precision_curve: list[float] = []\n        beta_coverage_curve: list[float] = []\n\n        synth_to_center = np.sqrt(np.sum((x_syn - emb_center) ** 2, axis=1))\n\n        nbrs_real = NearestNeighbors(n_neighbors=2, n_jobs=-1, p=2).fit(x)\n        k_neighbors_real = nbrs_real.kneighbors(x)\n        assert isinstance(k_neighbors_real, tuple)\n        real_to_real, _ = k_neighbors_real\n\n        nbrs_synth = NearestNeighbors(n_neighbors=1, n_jobs=-1, p=2).fit(x_syn)\n        k_neighbors_synth = nbrs_synth.kneighbors(x)\n        assert isinstance(k_neighbors_synth, tuple)\n        real_to_synth, real_to_synth_args = k_neighbors_synth\n\n        # Let us find closest real point to any real point, excluding itself (therefore 1 instead of 0)\n        real_to_real = real_to_real[:, 1].squeeze()\n        real_to_synth = real_to_synth.squeeze()\n        real_to_synth_args = real_to_synth_args.squeeze()\n\n        real_synth_closest = x_syn[real_to_synth_args]\n\n        real_synth_closest_d = np.sqrt(np.sum((real_synth_closest - synth_center) ** 2, axis=1))\n        closest_synth_radii = np.quantile(real_synth_closest_d, alphas)\n\n        for k in range(len(radii)):\n            precision_audit_mask = synth_to_center &lt;= radii[k]\n            alpha_precision = np.mean(precision_audit_mask)\n\n            beta_coverage = np.mean(\n                ((real_to_synth &lt;= real_to_real) * (real_synth_closest_d &lt;= closest_synth_radii[k]))\n            )\n\n            alpha_precision_curve.append(alpha_precision)\n            beta_coverage_curve.append(beta_coverage)\n\n        # See which one is bigger\n\n        authen = real_to_real[real_to_synth_args] &lt; real_to_synth\n        authenticity = np.mean(authen)\n\n        delta_precision_alpha = 1.0 - np.sum(np.abs(np.array(alphas) - np.array(alpha_precision_curve))) / np.sum(\n            alphas\n        )\n\n        if delta_precision_alpha &lt; 0:\n            raise RuntimeError(\"negative value detected for Delta_precision_alpha\")\n\n        delta_coverage_beta = 1.0 - np.sum(np.abs(np.array(alphas) - np.array(beta_coverage_curve))) / np.sum(alphas)\n\n        if delta_coverage_beta &lt; 0:\n            raise RuntimeError(\"negative value detected for Delta_coverage_beta\")\n\n        return (\n            alphas.tolist(),\n            alpha_precision_curve,\n            beta_coverage_curve,\n            delta_precision_alpha,\n            delta_coverage_beta,\n            authenticity.astype(float),\n        )\n\n    def _normalize_covariates(\n        self,\n        x: DataLoader,\n        x_syn: DataLoader,\n    ) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"\n        This is an internal method to replicate the old, naive method for evaluating AlphaPrecision.\n\n        Args:\n            x (DataLoader): The ground truth dataset.\n            x_syn (DataLoader): The synthetic dataset.\n\n        Returns:\n            Tuple[pd.DataFrame, pd.DataFrame]: normalized version of the datasets\n        \"\"\"\n        x_gt_norm = x.dataframe().copy()\n        x_syn_norm = x_syn.dataframe().copy()\n        if self._task_type != \"survival_analysis\":\n            if hasattr(x, \"target_column\"):\n                x_gt_norm = x_gt_norm.drop(columns=[x.target_column])\n            if hasattr(x_syn, \"target_column\"):\n                x_syn_norm = x_syn_norm.drop(columns=[x_syn.target_column])\n        scaler = MinMaxScaler().fit(x_gt_norm)\n        if hasattr(x, \"target_column\"):\n            x_gt_norm_df = pd.DataFrame(\n                scaler.transform(x_gt_norm),\n                columns=[col for col in x.train().dataframe().columns if col != x.target_column],\n            )\n        else:\n            x_gt_norm_df = pd.DataFrame(scaler.transform(x_gt_norm), columns=x.train().dataframe().columns)\n\n        if hasattr(x_syn, \"target_column\"):\n            x_syn_norm_df = pd.DataFrame(\n                scaler.transform(x_syn_norm),\n                columns=[col for col in x_syn.dataframe().columns if col != x_syn.target_column],\n            )\n        else:\n            x_syn_norm_df = pd.DataFrame(scaler.transform(x_syn_norm), columns=x_syn.dataframe().columns)\n\n        return x_gt_norm_df, x_syn_norm_df\n\n    def _evaluate(\n        self,\n        x: DataLoader,\n        x_syn: DataLoader,\n    ) -&gt; dict:\n        \"\"\"\n        Run the full evaluation pipeline, including both naive and non-naive metrics.\n\n        Args:\n            x (DataLoader): The ground truth dataset.\n            x_syn (DataLoader): The synthetic dataset.\n\n        Returns:\n            Dictionary of metric type and value\n        \"\"\"\n        results = {}\n\n        x_ = x.numpy().reshape(len(x), -1)\n        x_syn_ = x_syn.numpy().reshape(len(x_syn), -1)\n\n        # OneClass representation\n        emb = \"_OC\"\n        oneclass_model = self._get_oneclass_model(x_)\n        x_ = self._oneclass_predict(oneclass_model, x_)\n        x_syn_ = self._oneclass_predict(oneclass_model, x_syn_)\n        emb_center = oneclass_model.c.detach().cpu().numpy()\n\n        (\n            _,\n            _,\n            _,\n            delta_precision_alpha,\n            delta_coverage_beta,\n            authenticity,\n        ) = self.metrics(x_, x_syn_, emb_center=emb_center)\n\n        results[f\"delta_precision_alpha{emb}\"] = delta_precision_alpha\n        results[f\"delta_coverage_beta{emb}\"] = delta_coverage_beta\n        results[f\"authenticity{emb}\"] = authenticity\n\n        X_df, X_syn_df = self._normalize_covariates(x, x_syn)\n        (\n            _,\n            _,\n            _,\n            delta_precision_alpha_naive,\n            delta_coverage_beta_naive,\n            authenticity_naive,\n        ) = self.metrics(X_df.to_numpy(), X_syn_df.to_numpy(), emb_center=None)\n\n        results[\"delta_precision_alpha_naive\"] = delta_precision_alpha_naive\n        results[\"delta_coverage_beta_naive\"] = delta_coverage_beta_naive\n        results[\"authenticity_naive\"] = authenticity_naive\n\n        return results\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(**kwargs)\n</code></pre> <p>Evaluates the alpha-precision, beta-recall, and authenticity scores.</p> <p>The class evaluates the synthetic data using a tuple of three metrics:</p> <pre><code>alpha-precision, beta-recall, and authenticity.\n</code></pre> <p>Note that these metrics can be evaluated for each synthetic data point (which are useful for auditing and post-processing). Here we average the scores to reflect the overall quality of the data. The formal definitions can be found in the reference below:</p> <p>Alaa, Ahmed, Boris Van Breugel, Evgeny S. Saveliev, and Mihaela van der Schaar. \"How faithful is your synthetic data? sample-level metrics for evaluating and auditing generative models.\" In International Conference on Machine Learning, pp. 290-306. PMLR, 2022.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/statistical_eval.py</code> <pre><code>def __init__(self, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Evaluates the alpha-precision, beta-recall, and authenticity scores.\n\n    The class evaluates the synthetic data using a tuple of three metrics:\n\n        alpha-precision, beta-recall, and authenticity.\n\n    Note that these metrics can be evaluated for each synthetic data point (which are useful for auditing and\n    post-processing). Here we average the scores to reflect the overall quality of the data.\n    The formal definitions can be found in the reference below:\n\n    Alaa, Ahmed, Boris Van Breugel, Evgeny S. Saveliev, and Mihaela van der Schaar. \"How faithful is your synthetic\n    data? sample-level metrics for evaluating and auditing generative models.\"\n    In International Conference on Machine Learning, pp. 290-306. PMLR, 2022.\n    \"\"\"\n    super().__init__(default_metric=\"authenticity_OC\", **kwargs)\n</code></pre> <code></code> name <code>staticmethod</code> \u00b6 <pre><code>name()\n</code></pre> <p>Return name.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/statistical_eval.py</code> <pre><code>@staticmethod\ndef name() -&gt; str:\n    \"\"\"Return name.\"\"\"\n    return \"alpha_precision\"\n</code></pre> <code></code> direction <code>staticmethod</code> \u00b6 <pre><code>direction()\n</code></pre> <p>Return optimization direction.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/statistical_eval.py</code> <pre><code>@staticmethod\ndef direction() -&gt; str:\n    \"\"\"Return optimization direction.\"\"\"\n    return \"maximize\"\n</code></pre> <code></code> metrics \u00b6 <pre><code>metrics(x, x_syn, emb_center=None)\n</code></pre> <p>Compute the alpha-precision, beta-recall, and authenticity scores provided real data (x) and synthetic data (x_syn). If <code>emb_center</code> is provided this are \"non-naive\" metrics. If it is none, these constitute \"naive\" scores.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Real data</p> required <code>x_syn</code> <code>ndarray</code> <p>Synthetically generated data.</p> required <code>emb_center</code> <code>ndarray | None</code> <p>Center for the embeddings of the data. If None, we just use the mean of the features of x). Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Raised if the datasets are not the same sizes.</p> <code>RuntimeError</code> <p>Raised if there is an invalid score for delta_precision_alpha.</p> <code>RuntimeError</code> <p>Raised if there is an invalid score for delta_coverage_beta.</p> <p>Returns:</p> Type Description <code>list[float]</code> <p>alphas, alpha_precision_curve, beta_coverage_curve, delta_precision_alpha, delta_coverage_beta,</p> <code>list[float]</code> <p>authenticity.</p> Source code in <code>src/midst_toolkit/evaluation/quality/synthcity/statistical_eval.py</code> <pre><code>def metrics(\n    self,\n    x: np.ndarray,\n    x_syn: np.ndarray,\n    emb_center: np.ndarray | None = None,\n) -&gt; tuple[list[float], list[float], list[float], float, float, float]:\n    \"\"\"\n    Compute the alpha-precision, beta-recall, and authenticity scores provided real data (x) and synthetic\n    data (x_syn). If ``emb_center`` is provided this are \"non-naive\" metrics. If it is none, these constitute\n    \"naive\" scores.\n\n    Args:\n        x: Real data\n        x_syn: Synthetically generated data.\n        emb_center: Center for the embeddings of the data. If None, we just use the mean of the features of x).\n            Defaults to None.\n\n    Raises:\n        RuntimeError: Raised if the datasets are not the same sizes.\n        RuntimeError: Raised if there is an invalid score for delta_precision_alpha.\n        RuntimeError: Raised if there is an invalid score for delta_coverage_beta.\n\n    Returns:\n        alphas, alpha_precision_curve, beta_coverage_curve, delta_precision_alpha, delta_coverage_beta,\n        authenticity.\n    \"\"\"\n    if len(x) != len(x_syn):\n        raise RuntimeError(\"The real and synthetic data must have the same length\")\n\n    if emb_center is None:\n        emb_center = np.mean(x, axis=0)\n\n    n_steps = 30\n    alphas = np.linspace(0, 1, n_steps)\n\n    radii = np.quantile(np.sqrt(np.sum((x - emb_center) ** 2, axis=1)), alphas)\n\n    synth_center = np.mean(x_syn, axis=0)\n\n    alpha_precision_curve: list[float] = []\n    beta_coverage_curve: list[float] = []\n\n    synth_to_center = np.sqrt(np.sum((x_syn - emb_center) ** 2, axis=1))\n\n    nbrs_real = NearestNeighbors(n_neighbors=2, n_jobs=-1, p=2).fit(x)\n    k_neighbors_real = nbrs_real.kneighbors(x)\n    assert isinstance(k_neighbors_real, tuple)\n    real_to_real, _ = k_neighbors_real\n\n    nbrs_synth = NearestNeighbors(n_neighbors=1, n_jobs=-1, p=2).fit(x_syn)\n    k_neighbors_synth = nbrs_synth.kneighbors(x)\n    assert isinstance(k_neighbors_synth, tuple)\n    real_to_synth, real_to_synth_args = k_neighbors_synth\n\n    # Let us find closest real point to any real point, excluding itself (therefore 1 instead of 0)\n    real_to_real = real_to_real[:, 1].squeeze()\n    real_to_synth = real_to_synth.squeeze()\n    real_to_synth_args = real_to_synth_args.squeeze()\n\n    real_synth_closest = x_syn[real_to_synth_args]\n\n    real_synth_closest_d = np.sqrt(np.sum((real_synth_closest - synth_center) ** 2, axis=1))\n    closest_synth_radii = np.quantile(real_synth_closest_d, alphas)\n\n    for k in range(len(radii)):\n        precision_audit_mask = synth_to_center &lt;= radii[k]\n        alpha_precision = np.mean(precision_audit_mask)\n\n        beta_coverage = np.mean(\n            ((real_to_synth &lt;= real_to_real) * (real_synth_closest_d &lt;= closest_synth_radii[k]))\n        )\n\n        alpha_precision_curve.append(alpha_precision)\n        beta_coverage_curve.append(beta_coverage)\n\n    # See which one is bigger\n\n    authen = real_to_real[real_to_synth_args] &lt; real_to_synth\n    authenticity = np.mean(authen)\n\n    delta_precision_alpha = 1.0 - np.sum(np.abs(np.array(alphas) - np.array(alpha_precision_curve))) / np.sum(\n        alphas\n    )\n\n    if delta_precision_alpha &lt; 0:\n        raise RuntimeError(\"negative value detected for Delta_precision_alpha\")\n\n    delta_coverage_beta = 1.0 - np.sum(np.abs(np.array(alphas) - np.array(beta_coverage_curve))) / np.sum(alphas)\n\n    if delta_coverage_beta &lt; 0:\n        raise RuntimeError(\"negative value detected for Delta_coverage_beta\")\n\n    return (\n        alphas.tolist(),\n        alpha_precision_curve,\n        beta_coverage_curve,\n        delta_precision_alpha,\n        delta_coverage_beta,\n        authenticity.astype(float),\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.utils","title":"utils","text":""},{"location":"api/#midst_toolkit.evaluation.utils.create_quality_metrics_directory","title":"create_quality_metrics_directory","text":"<pre><code>create_quality_metrics_directory(save_directory)\n</code></pre> <p>Helper function for creating a directory at the specified path to whole metrics results. If the directory already exists, this function will log a warning and no-op.</p> <p>Parameters:</p> Name Type Description Default <code>save_directory</code> <code>Path</code> <p>Path of the directory to create.</p> required Source code in <code>src/midst_toolkit/evaluation/utils.py</code> <pre><code>def create_quality_metrics_directory(save_directory: Path) -&gt; None:\n    \"\"\"\n    Helper function for creating a directory at the specified path to whole metrics results. If the directory already\n    exists, this function will log a warning and no-op.\n\n    Args:\n        save_directory: Path of the directory to create.\n    \"\"\"\n    if not os.path.exists(save_directory):\n        os.makedirs(save_directory)\n    else:\n        log(WARNING, f\"Path: {save_directory} already exists. Make sure this is intended.\")\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.utils.dump_metrics_dict","title":"dump_metrics_dict","text":"<pre><code>dump_metrics_dict(metrics_dict, file_path)\n</code></pre> <p>Write the provided metrics dictionary to the provided <code>file_path</code> argument. The metrics dictionary is written in a specific format.</p> <p>Parameters:</p> Name Type Description Default <code>metrics_dict</code> <code>dict[str, float]</code> <p>Dictionary of metrics with string key values and associated floats representing metrics calculations</p> required <code>file_path</code> <code>Path</code> <p>Path to which the metrics are written. The file will be created or overwritten if it exists</p> required Source code in <code>src/midst_toolkit/evaluation/utils.py</code> <pre><code>def dump_metrics_dict(metrics_dict: dict[str, float], file_path: Path) -&gt; None:\n    \"\"\"\n    Write the provided metrics dictionary to the provided ``file_path`` argument. The metrics dictionary is written\n    in a specific format.\n\n    Args:\n        metrics_dict: Dictionary of metrics with string key values and associated floats representing metrics\n            calculations\n        file_path: Path to which the metrics are written. The file will be created or overwritten if it exists\n    \"\"\"\n    if os.path.exists(file_path):\n        log(WARNING, f\"File at path {file_path} already exists.\")\n    with open(file_path, \"w\") as f:\n        for metric_key, metric_value in metrics_dict.items():\n            f.write(f\"Metric Name: {metric_key}\\t Metric Value: {metric_value}\\n\")\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.utils.extract_columns_based_on_meta_info","title":"extract_columns_based_on_meta_info","text":"<pre><code>extract_columns_based_on_meta_info(data, meta_info)\n</code></pre> <p>Given a set of meta information, which should be in JSON format with keys 'num_col_idx', 'cat_col_idx', 'target_col_idx', and 'task_type', the provided dataframe is filtered to the correct set of columns for evaluation using the meta information.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be filtered using the meta information</p> required <code>meta_info</code> <code>dict[str, Any]</code> <p>JSON with meta information about the columns and their corresponding types that should be considered. At minimum, it should have the keys 'num_col_idx', 'cat_col_idx', 'target_col_idx', and 'task_type'</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Filtered dataframes. The first dataframe is the filtered set of columns associated with numerical data. The</p> <code>DataFrame</code> <p>second is the filtered set of columns associated with categorical data.</p> Source code in <code>src/midst_toolkit/evaluation/utils.py</code> <pre><code>def extract_columns_based_on_meta_info(\n    data: pd.DataFrame, meta_info: dict[str, Any]\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Given a set of meta information, which should be in JSON format with keys 'num_col_idx', 'cat_col_idx',\n    'target_col_idx', and 'task_type', the provided dataframe is filtered to the correct set of columns for evaluation\n    using the meta information.\n\n    Args:\n        data: Dataframe to be filtered using the meta information\n        meta_info: JSON with meta information about the columns and their corresponding types that should be\n            considered. At minimum, it should have the keys 'num_col_idx', 'cat_col_idx', 'target_col_idx', and\n            'task_type'\n\n    Returns:\n        Filtered dataframes. The first dataframe is the filtered set of columns associated with numerical data. The\n        second is the filtered set of columns associated with categorical data.\n    \"\"\"\n    # TODO: Consider creating a meta_info class that formalizes the structure of the meta_info produced when\n    # Training the diffusion generators.\n\n    # Enumerate columns and replace column name with index\n    data.columns = range(len(data.columns))\n\n    # Get numerical and categorical column indices from meta info\n    # NOTE: numerical and categorical columns are the only admissible/generate-able types\"\n    numerical_column_idx = meta_info[\"num_col_idx\"]\n    categorical_column_idx = meta_info[\"cat_col_idx\"]\n\n    # Target columns are also part of the generation, just need to add it to the right \"category\"\n    target_col_idx = meta_info[\"target_col_idx\"]\n    task_type = TaskType(meta_info[\"task_type\"])\n    if task_type == TaskType.REGRESSION:\n        numerical_column_idx = numerical_column_idx + target_col_idx\n    else:\n        categorical_column_idx = categorical_column_idx + target_col_idx\n\n    numerical_data = data[numerical_column_idx]\n    categorical_data = data[categorical_column_idx]\n\n    return numerical_data, categorical_data\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.utils.one_hot_encode_categoricals_and_merge_with_numerical","title":"one_hot_encode_categoricals_and_merge_with_numerical","text":"<pre><code>one_hot_encode_categoricals_and_merge_with_numerical(\n    real_categorical_data,\n    synthetic_categorical_data,\n    real_numerical_data,\n    synthetic_numerical_data,\n)\n</code></pre> <p>Performs one-hot encoding on the real and synthetic data contained in numpy arrays. The <code>real_categorical_data</code> is used to fit the one-hot encoder, which is then applied to the data in <code>synthetic_categorical_data</code>. The resulting, one-hot encoded, numpy arrays are then concatenated together numerical then one-hots for both the synthetic and real data.</p> <p>Parameters:</p> Name Type Description Default <code>real_categorical_data</code> <code>ndarray</code> <p>Categorical data from the real dataset.</p> required <code>synthetic_categorical_data</code> <code>ndarray</code> <p>Categorical data from the synthetically generated dataset.</p> required <code>real_numerical_data</code> <code>ndarray</code> <p>Numerical data from the real dataset.</p> required <code>synthetic_numerical_data</code> <code>ndarray</code> <p>Numerical data from the synthetically generated dataset.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Two pandas dataframes representing the numerical and categorical data concatenated together. First dataframe</p> <code>DataFrame</code> <p>is the real data, second is the synthetic data.</p> Source code in <code>src/midst_toolkit/evaluation/utils.py</code> <pre><code>def one_hot_encode_categoricals_and_merge_with_numerical(\n    real_categorical_data: np.ndarray,\n    synthetic_categorical_data: np.ndarray,\n    real_numerical_data: np.ndarray,\n    synthetic_numerical_data: np.ndarray,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Performs one-hot encoding on the real and synthetic data contained in numpy arrays. The ``real_categorical_data``\n    is used to fit the one-hot encoder, which is then applied to the data in ``synthetic_categorical_data``. The\n    resulting, one-hot encoded, numpy arrays are then concatenated together numerical then one-hots for both the\n    synthetic and real data.\n\n    Args:\n        real_categorical_data: Categorical data from the real dataset.\n        synthetic_categorical_data: Categorical data from the synthetically generated dataset.\n        real_numerical_data: Numerical data from the real dataset.\n        synthetic_numerical_data: Numerical data from the synthetically generated dataset.\n\n    Returns:\n        Two pandas dataframes representing the numerical and categorical data concatenated together. First dataframe\n        is the real data, second is the synthetic data.\n    \"\"\"\n    encoder = OneHotEncoder()\n    one_hot_real_data = encoder.fit_transform(real_categorical_data).toarray()\n    one_hot_synthetic_data = encoder.transform(synthetic_categorical_data).toarray()\n\n    real_dataframe = pd.DataFrame(np.concatenate((real_numerical_data, one_hot_real_data), axis=1)).astype(float)\n\n    synthetic_dataframe = pd.DataFrame(\n        np.concatenate((synthetic_numerical_data, one_hot_synthetic_data), axis=1)\n    ).astype(float)\n\n    return real_dataframe, synthetic_dataframe\n</code></pre>"},{"location":"api/#midst_toolkit.models","title":"models","text":"<p>PLACEHOLDER.</p>"},{"location":"api/#midst_toolkit.models.clavaddpm","title":"clavaddpm","text":"<p>PLACEHOLDER.</p>"},{"location":"api/#midst_toolkit.models.clavaddpm.clustering","title":"clustering","text":"<p>Clustering functions for the multi-table ClavaDDPM model.</p>"},{"location":"api/#midst_toolkit.models.clavaddpm.clustering.clava_clustering","title":"clava_clustering","text":"<pre><code>clava_clustering(tables, relation_order, save_dir, configs)\n</code></pre> <p>Clustering function for the multi-table function of the ClavaDDPM model.</p> <p>Parameters:</p> Name Type Description Default <code>tables</code> <code>Tables</code> <p>Definition of the tables and their relations. Example: {     \"table1\": {         \"children\": [\"table2\"],         \"parents\": []     },     \"table2\": {         \"children\": [],         \"parents\": [\"table1\"]     } }</p> required <code>relation_order</code> <code>RelationOrder</code> <p>List of tuples of parent and child tables. Example: [(\"table1\", \"table2\"), (\"table1\", \"table3\")]</p> required <code>save_dir</code> <code>Path</code> <p>Directory to save the clustering checkpoint.</p> required <code>configs</code> <code>Configs</code> <p>Dictionary of configurations. The following config keys are required: {     num_clusters = int | dict,     parent_scale = float,     clustering_method = str[\"kmeans\" | \"both\" | \"variational\" | \"gmm\"], }</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any], GroupLengthsProbDicts]</code> <p>A tuple with 2 values: - The tables dictionary. - The dictionary with the group lengths probability for all the parent-child pairs.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/clustering.py</code> <pre><code>def clava_clustering(\n    tables: Tables,\n    relation_order: RelationOrder,\n    save_dir: Path,\n    configs: Configs,\n) -&gt; tuple[dict[str, Any], GroupLengthsProbDicts]:\n    \"\"\"\n    Clustering function for the multi-table function of the ClavaDDPM model.\n\n    Args:\n        tables: Definition of the tables and their relations. Example:\n            {\n                \"table1\": {\n                    \"children\": [\"table2\"],\n                    \"parents\": []\n                },\n                \"table2\": {\n                    \"children\": [],\n                    \"parents\": [\"table1\"]\n                }\n            }\n        relation_order: List of tuples of parent and child tables. Example:\n            [(\"table1\", \"table2\"), (\"table1\", \"table3\")]\n        save_dir: Directory to save the clustering checkpoint.\n        configs: Dictionary of configurations. The following config keys are required:\n            {\n                num_clusters = int | dict,\n                parent_scale = float,\n                clustering_method = str[\"kmeans\" | \"both\" | \"variational\" | \"gmm\"],\n            }\n\n    Returns:\n        A tuple with 2 values:\n            - The tables dictionary.\n            - The dictionary with the group lengths probability for all the parent-child pairs.\n    \"\"\"\n    cluster_ckpt = _load_clustering_info_from_checkpoint(save_dir)\n    if cluster_ckpt is not None:\n        tables = cluster_ckpt[\"tables\"]\n        all_group_lengths_prob_dicts = cluster_ckpt[\"all_group_lengths_prob_dicts\"]\n\n    else:\n        tables, all_group_lengths_prob_dicts = _run_clustering(tables, relation_order, configs)\n\n        # saving the clustering information in the checkpoint file\n        cluster_ckpt = {\n            \"tables\": tables,\n            \"all_group_lengths_prob_dicts\": all_group_lengths_prob_dicts,\n        }\n        with open(save_dir / \"cluster_ckpt.pkl\", \"wb\") as f:\n            pickle.dump(cluster_ckpt, f)\n\n    # adding a placeholder for the top level tables (i.e. tables with no parent)\n    for parent, child in relation_order:\n        if parent is None:\n            tables[child][\"df\"][\"placeholder\"] = list(range(len(tables[child][\"df\"])))\n\n    return tables, all_group_lengths_prob_dicts\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils","title":"diffusion_utils","text":"<p>PLACEHOLDER.</p>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.FoundNANsError","title":"FoundNANsError","text":"<p>               Bases: <code>BaseException</code></p> <p>Found NANs during sampling.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>class FoundNANsError(BaseException):\n    \"\"\"Found NANs during sampling.\"\"\"\n\n    def __init__(self, message=\"Found NANs during sampling.\"):\n        # ruff: noqa: D107\n        super(FoundNANsError, self).__init__(message)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.normal_kl","title":"normal_kl","text":"<pre><code>normal_kl(mean1, logvar1, mean2, logvar2)\n</code></pre> <p>Compute the KL divergence between two gaussians.</p> <p>Shapes are automatically broadcasted, so batches can be compared to scalars, among other use cases.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def normal_kl(\n    mean1: Tensor | float,\n    logvar1: Tensor | float,\n    mean2: Tensor | float,\n    logvar2: Tensor | float,\n) -&gt; Tensor:\n    \"\"\"\n    Compute the KL divergence between two gaussians.\n\n    Shapes are automatically broadcasted, so batches can be compared to\n    scalars, among other use cases.\n    \"\"\"\n    tensor = None\n    for obj in (mean1, logvar1, mean2, logvar2):\n        if isinstance(obj, torch.Tensor):\n            tensor = obj\n            break\n    assert tensor is not None, \"at least one argument must be a Tensor\"\n\n    # Force variances to be Tensors. Broadcasting helps convert scalars to\n    # Tensors, but it does not work for torch.exp().\n    logvar1, logvar2 = [x if isinstance(x, torch.Tensor) else torch.tensor(x).to(tensor) for x in (logvar1, logvar2)]\n\n    return 0.5 * (\n        -1.0 + logvar2 - logvar1 + torch.exp(logvar1 - logvar2) + ((mean1 - mean2) ** 2) * torch.exp(-logvar2)\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.approx_standard_normal_cdf","title":"approx_standard_normal_cdf","text":"<pre><code>approx_standard_normal_cdf(x)\n</code></pre> <p>A fast approximation of the cumulative distribution function of the standard normal.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def approx_standard_normal_cdf(x: Tensor) -&gt; Tensor:\n    \"\"\"\n    A fast approximation of the cumulative distribution function of the\n    standard normal.\n    \"\"\"\n    return 0.5 * (1.0 + torch.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * torch.pow(x, 3))))\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.discretized_gaussian_log_likelihood","title":"discretized_gaussian_log_likelihood","text":"<pre><code>discretized_gaussian_log_likelihood(\n    x, *, means, log_scales\n)\n</code></pre> <p>Compute the log-likelihood of a Gaussian distribution discretizing to a given image.</p> <p>:param x: the target images. It is assumed that this was uint8 values,           rescaled to the range [-1, 1]. :param means: the Gaussian mean Tensor. :param log_scales: the Gaussian log stddev Tensor. :return: a tensor like x of log probabilities (in nats).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def discretized_gaussian_log_likelihood(x: Tensor, *, means: Tensor, log_scales: Tensor) -&gt; Tensor:\n    \"\"\"\n    Compute the log-likelihood of a Gaussian distribution discretizing to a\n    given image.\n\n    :param x: the target images. It is assumed that this was uint8 values,\n              rescaled to the range [-1, 1].\n    :param means: the Gaussian mean Tensor.\n    :param log_scales: the Gaussian log stddev Tensor.\n    :return: a tensor like x of log probabilities (in nats).\n    \"\"\"\n    assert x.shape == means.shape == log_scales.shape\n    centered_x = x - means\n    inv_stdv = torch.exp(-log_scales)\n    plus_in = inv_stdv * (centered_x + 1.0 / 255.0)\n    cdf_plus = approx_standard_normal_cdf(plus_in)\n    min_in = inv_stdv * (centered_x - 1.0 / 255.0)\n    cdf_min = approx_standard_normal_cdf(min_in)\n    log_cdf_plus = torch.log(cdf_plus.clamp(min=1e-12))\n    log_one_minus_cdf_min = torch.log((1.0 - cdf_min).clamp(min=1e-12))\n    cdf_delta = cdf_plus - cdf_min\n    log_probs = torch.where(\n        x &lt; -0.999,\n        log_cdf_plus,\n        torch.where(x &gt; 0.999, log_one_minus_cdf_min, torch.log(cdf_delta.clamp(min=1e-12))),\n    )\n    assert log_probs.shape == x.shape\n    return log_probs\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.sum_except_batch","title":"sum_except_batch","text":"<pre><code>sum_except_batch(x, num_dims=1)\n</code></pre> <p>Sums all dimensions except the first.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Tensor, shape (batch_size, ...)</p> required <code>num_dims</code> <code>int</code> <p>int, number of batch dims (default=1)</p> <code>1</code> <p>Returns:</p> Name Type Description <code>x_sum</code> <code>Tensor</code> <p>Tensor, shape (batch_size,)</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def sum_except_batch(x: Tensor, num_dims: int = 1) -&gt; Tensor:\n    \"\"\"\n    Sums all dimensions except the first.\n\n    Args:\n        x: Tensor, shape (batch_size, ...)\n        num_dims: int, number of batch dims (default=1)\n\n    Returns:\n        x_sum: Tensor, shape (batch_size,)\n    \"\"\"\n    return x.reshape(*x.shape[:num_dims], -1).sum(-1)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.mean_flat","title":"mean_flat","text":"<pre><code>mean_flat(tensor)\n</code></pre> <p>Take the mean over all non-batch dimensions.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def mean_flat(tensor: Tensor) -&gt; Tensor:\n    \"\"\"Take the mean over all non-batch dimensions.\"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion","title":"gaussian_multinomial_diffusion","text":"<p>Based on the code below.</p> <p>https://github.com/openai/guided-diffusion/blob/main/guided_diffusion https://github.com/ehoogeboom/multinomial_diffusion</p>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.GaussianMultinomialDiffusion","title":"GaussianMultinomialDiffusion","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>class GaussianMultinomialDiffusion(torch.nn.Module):\n    def __init__(\n        # ruff: noqa: PLR0915\n        self,\n        num_classes: np.ndarray,\n        num_numerical_features: int,\n        denoise_fn: torch.nn.Module,\n        num_timesteps: int = 1000,\n        gaussian_loss_type: str = \"mse\",\n        gaussian_parametrization: str = \"eps\",\n        multinomial_loss_type: str = \"vb_stochastic\",\n        parametrization: str = \"x0\",\n        scheduler: str = \"cosine\",\n        device: torch.device | None = None,\n    ):\n        # ruff: noqa: D107\n        if device is None:\n            device = torch.device(\"cpu\")\n\n        super(GaussianMultinomialDiffusion, self).__init__()\n        assert multinomial_loss_type in (\"vb_stochastic\", \"vb_all\")\n        assert parametrization in (\"x0\", \"direct\")\n\n        if multinomial_loss_type == \"vb_all\":\n            print(\n                \"Computing the loss using the bound on _all_ timesteps.\"\n                \" This is expensive both in terms of memory and computation.\"\n            )\n\n        self.num_numerical_features = num_numerical_features\n        self.num_classes = num_classes  # it as a vector [K1, K2, ..., Km]\n        self.num_classes_expanded = torch.from_numpy(\n            np.concatenate([num_classes[i].repeat(num_classes[i]) for i in range(len(num_classes))])\n        ).to(device)\n\n        self.slices_for_classes = [np.arange(self.num_classes[0])]\n        offsets: np.ndarray = np.cumsum(self.num_classes)\n        for i in range(1, len(offsets)):\n            self.slices_for_classes.append(np.arange(offsets[i - 1], offsets[i]))\n        self.offsets = torch.from_numpy(np.append([0], offsets)).to(device)\n\n        self._denoise_fn = denoise_fn\n        self.gaussian_loss_type = gaussian_loss_type\n        self.gaussian_parametrization = gaussian_parametrization\n        self.multinomial_loss_type = multinomial_loss_type\n        self.num_timesteps = num_timesteps\n        self.parametrization = parametrization\n        self.scheduler = scheduler\n        self.device = device\n        self.alphas: Tensor\n        self.alphas_cumprod: Tensor\n        self.alphas_cumprod_next: Tensor\n        self.alphas_cumprod_prev: Tensor\n        self.sqrt_alphas_cumprod: Tensor\n        self.sqrt_one_minus_alphas_cumprod: Tensor\n        self.log_cumprod_alpha: Tensor\n        self.log_alpha: Tensor\n        self.log_1_min_alpha: Tensor\n        self.log_1_min_cumprod_alpha: Tensor\n        self.sqrt_recipm1_alphas_cumprod: Tensor\n        self.sqrt_recip_alphas_cumprod: Tensor\n        self.Lt_history: Tensor\n        self.Lt_count: Tensor\n\n        a = 1.0 - get_named_beta_schedule(scheduler, num_timesteps)\n        alphas = torch.tensor(a.astype(\"float64\"))\n        betas = 1.0 - alphas\n\n        log_alpha: Tensor = np.log(alphas)  # type: ignore[assignment]\n        log_cumprod_alpha: Tensor = np.cumsum(log_alpha)  # type: ignore[assignment]\n\n        log_1_min_alpha: Tensor = log_1_min_a(log_alpha)\n        log_1_min_cumprod_alpha: Tensor = log_1_min_a(log_cumprod_alpha)\n\n        alphas_cumprod: Tensor = np.cumprod(alphas, axis=0)  # type: ignore[assignment]\n        alphas_cumprod_prev = torch.tensor(np.append(1.0, alphas_cumprod[:-1]))\n        alphas_cumprod_next = torch.tensor(np.append(alphas_cumprod[1:], 0.0))\n        sqrt_alphas_cumprod: Tensor = np.sqrt(alphas_cumprod)  # type: ignore[assignment]\n        sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - alphas_cumprod)\n        sqrt_recip_alphas_cumprod: Tensor = np.sqrt(1.0 / alphas_cumprod)\n        sqrt_recipm1_alphas_cumprod: Tensor = np.sqrt(1.0 / alphas_cumprod - 1)\n\n        # Gaussian diffusion\n\n        self.posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n        self.posterior_log_variance_clipped = (\n            torch.from_numpy(np.log(np.append(self.posterior_variance[1], self.posterior_variance[1:])))\n            .float()\n            .to(device)\n        )\n        self.posterior_mean_coef1 = (betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)).float().to(device)\n        self.posterior_mean_coef2 = (\n            ((1.0 - alphas_cumprod_prev) * np.sqrt(alphas.numpy()) / (1.0 - alphas_cumprod)).float().to(device)\n        )\n\n        assert log_add_exp(log_alpha, log_1_min_alpha).abs().sum().item() &lt; 1.0e-5\n        assert log_add_exp(log_cumprod_alpha, log_1_min_cumprod_alpha).abs().sum().item() &lt; 1e-5\n        diff: Tensor = cast(Tensor, np.cumsum(log_alpha) - log_cumprod_alpha)\n        assert diff.abs().sum().item() &lt; 1.0e-5\n\n        # Convert to float32 and register buffers.\n        self.register_buffer(\"alphas\", alphas.float().to(device))\n        self.register_buffer(\"log_alpha\", log_alpha.float().to(device))\n        self.register_buffer(\"log_1_min_alpha\", log_1_min_alpha.float().to(device))\n        self.register_buffer(\"log_1_min_cumprod_alpha\", log_1_min_cumprod_alpha.float().to(device))\n        self.register_buffer(\"log_cumprod_alpha\", log_cumprod_alpha.float().to(device))\n        self.register_buffer(\"alphas_cumprod\", alphas_cumprod.float().to(device))\n        self.register_buffer(\"alphas_cumprod_prev\", alphas_cumprod_prev.float().to(device))\n        self.register_buffer(\"alphas_cumprod_next\", alphas_cumprod_next.float().to(device))\n        self.register_buffer(\"sqrt_alphas_cumprod\", sqrt_alphas_cumprod.float().to(device))\n        self.register_buffer(\n            \"sqrt_one_minus_alphas_cumprod\",\n            sqrt_one_minus_alphas_cumprod.float().to(device),\n        )\n        self.register_buffer(\"sqrt_recip_alphas_cumprod\", sqrt_recip_alphas_cumprod.float().to(device))\n        self.register_buffer(\n            \"sqrt_recipm1_alphas_cumprod\",\n            sqrt_recipm1_alphas_cumprod.float().to(device),\n        )\n\n        self.register_buffer(\"Lt_history\", torch.zeros(num_timesteps))\n        self.register_buffer(\"Lt_count\", torch.zeros(num_timesteps))\n\n    # Gaussian part\n    def gaussian_q_mean_variance(self, x_start: Tensor, t: Tensor) -&gt; tuple[Tensor, Tensor, Tensor]:\n        mean = extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n        variance = extract(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract(self.log_1_min_cumprod_alpha, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def gaussian_q_sample(self, x_start: Tensor, t: Tensor, noise: Tensor | None = None) -&gt; Tensor:\n        if noise is None:\n            noise = torch.randn_like(x_start)\n        assert noise.shape == x_start.shape\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n            + extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n        )\n\n    def gaussian_q_posterior_mean_variance(\n        self,\n        x_start: Tensor,\n        x_t: Tensor,\n        t: Tensor,\n    ) -&gt; tuple[Tensor, Tensor, Tensor]:\n        assert x_start.shape == x_t.shape\n        posterior_mean = (\n            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start\n            + extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n        assert (\n            posterior_mean.shape[0]\n            == posterior_variance.shape[0]\n            == posterior_log_variance_clipped.shape[0]\n            == x_start.shape[0]\n        )\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def gaussian_p_mean_variance(\n        self,\n        model_output: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        denoised_fn: Callable | None = None,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Tensor]:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        B, C = x.shape[:2]\n        assert t.shape == (B,)\n\n        model_variance = torch.cat(\n            [\n                self.posterior_variance[1].unsqueeze(0).to(x.device),\n                (1.0 - self.alphas)[1:],\n            ],\n            dim=0,\n        )\n        # model_variance = self.posterior_variance.to(x.device)\n        model_log_variance = torch.log(model_variance)\n\n        model_variance = extract(model_variance, t, x.shape)\n        model_log_variance = extract(model_log_variance, t, x.shape)\n\n        if self.gaussian_parametrization == \"eps\":\n            pred_xstart = self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n        elif self.gaussian_parametrization == \"x0\":\n            pred_xstart = model_output\n        else:\n            raise NotImplementedError\n\n        model_mean, _, _ = self.gaussian_q_posterior_mean_variance(x_start=pred_xstart, x_t=x, t=t)\n\n        assert model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape, (\n            f\"{model_mean.shape}, {model_log_variance.shape}, {pred_xstart.shape}, {x.shape}\"\n        )\n\n        return {\n            \"mean\": model_mean,\n            \"variance\": model_variance,\n            \"log_variance\": model_log_variance,\n            \"pred_xstart\": pred_xstart,\n        }\n\n    def _vb_terms_bpd(\n        self,\n        model_output: Tensor,\n        x_start: Tensor,\n        x_t: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Tensor]:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        (\n            true_mean,\n            _,\n            true_log_variance_clipped,\n        ) = self.gaussian_q_posterior_mean_variance(x_start=x_start, x_t=x_t, t=t)\n        out = self.gaussian_p_mean_variance(\n            model_output, x_t, t, clip_denoised=clip_denoised, model_kwargs=model_kwargs\n        )\n        kl = normal_kl(true_mean, true_log_variance_clipped, out[\"mean\"], out[\"log_variance\"])\n        kl = mean_flat(kl) / np.log(2.0)\n\n        decoder_nll = -discretized_gaussian_log_likelihood(\n            x_start, means=out[\"mean\"], log_scales=0.5 * out[\"log_variance\"]\n        )\n        assert decoder_nll.shape == x_start.shape\n        decoder_nll = mean_flat(decoder_nll) / np.log(2.0)\n\n        # At the first timestep return the decoder NLL,\n        # otherwise return KL(q(x_{t-1}|x_t,x_0) || p(x_{t-1}|x_t))\n        output = torch.where((t == 0), decoder_nll, kl)\n        return {\n            \"output\": output,\n            \"pred_xstart\": out[\"pred_xstart\"],\n            \"out_mean\": out[\"mean\"],\n            \"true_mean\": true_mean,\n        }\n\n    def _prior_gaussian(self, x_start: Tensor) -&gt; Tensor:\n        \"\"\"\n        Get the prior KL term for the variational lower-bound, measured in\n        bits-per-dim.\n\n        This term can't be optimized, as it only depends on the encoder.\n\n        :param x_start: the [N x C x ...] tensor of inputs.\n        :return: a batch of [N] KL values (in bits), one per batch element.\n        \"\"\"\n        batch_size = x_start.shape[0]\n        t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n        qt_mean, _, qt_log_variance = self.gaussian_q_mean_variance(x_start, t)\n        kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0)\n        return mean_flat(kl_prior) / np.log(2.0)\n\n    def _gaussian_loss(\n        self,\n        model_out: Tensor,\n        x_start: Tensor,\n        x_t: Tensor,\n        t: Tensor,\n        noise: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; Tensor:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        terms = {}\n        if self.gaussian_loss_type == \"mse\":\n            terms[\"loss\"] = mean_flat((noise - model_out) ** 2)\n        elif self.gaussian_loss_type == \"kl\":\n            terms[\"loss\"] = self._vb_terms_bpd(\n                model_output=model_out,\n                x_start=x_start,\n                x_t=x_t,\n                t=t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n            )[\"output\"]\n\n        return terms[\"loss\"]\n\n    def _predict_xstart_from_eps(self, x_t: Tensor, t: Tensor, eps: Tensor) -&gt; Tensor:\n        assert x_t.shape == eps.shape\n        return (\n            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n            - extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * eps\n        )\n\n    def _predict_eps_from_xstart(self, x_t: Tensor, t: Tensor, pred_xstart: Tensor) -&gt; Tensor:\n        return (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / extract(\n            self.sqrt_recipm1_alphas_cumprod, t, x_t.shape\n        )\n\n    def condition_mean(\n        self,\n        cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n        p_mean_var: dict[str, Tensor],\n        x: Tensor,\n        t: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; Tensor:\n        \"\"\"\n        Compute the mean for the previous step, given a function cond_fn that\n        computes the gradient of a conditional log probability with respect to\n        x. In particular, cond_fn computes grad(log(p(y|x))), and we want to\n        condition on y.\n\n        This uses the conditioning strategy from Sohl-Dickstein et al. (2015).\n        \"\"\"\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        gradient = cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n        return p_mean_var[\"mean\"].float() + p_mean_var[\"variance\"] * gradient.float()\n\n    def condition_score(\n        self,\n        cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n        p_mean_var: dict[str, Tensor],\n        x: Tensor,\n        t: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Tensor]:\n        \"\"\"\n        Compute what the p_mean_variance output would have been, should the\n        model's score function be conditioned by cond_fn.\n\n        See condition_mean() for details on cond_fn.\n\n        Unlike condition_mean(), this instead uses the conditioning strategy\n        from Song et al (2020).\n        \"\"\"\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        alpha_bar = extract(self.alphas_cumprod, t, x.shape)\n\n        eps = self._predict_eps_from_xstart(x, t, p_mean_var[\"pred_xstart\"])\n        eps = eps - (1 - alpha_bar).sqrt() * cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n\n        out = p_mean_var.copy()\n        out[\"pred_xstart\"] = self._predict_xstart_from_eps(x, t, eps)\n        out[\"mean\"], _, _ = self.gaussian_q_posterior_mean_variance(x_start=out[\"pred_xstart\"], x_t=x, t=t)\n        return out\n\n    def gaussian_p_sample(\n        self,\n        model_out: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        denoised_fn: Callable | None = None,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; dict[str, Tensor]:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        out = self.gaussian_p_mean_variance(\n            model_out,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n        )\n        noise = torch.randn_like(x)\n        nonzero_mask = (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))  # no noise when t == 0\n\n        if cond_fn is not None:\n            out[\"mean\"] = self.condition_mean(cond_fn, out, x, t, model_kwargs=model_kwargs)\n\n        sample = out[\"mean\"] + nonzero_mask * torch.exp(0.5 * out[\"log_variance\"]) * noise\n        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    # Multinomial part\n\n    def multinomial_kl(self, log_prob1: Tensor, log_prob2: Tensor) -&gt; Tensor:\n        return (log_prob1.exp() * (log_prob1 - log_prob2)).sum(dim=1)\n\n    def q_pred_one_timestep(self, log_x_t: Tensor, t: Tensor) -&gt; Tensor:\n        log_alpha_t = extract(self.log_alpha, t, log_x_t.shape)\n        log_1_min_alpha_t = extract(self.log_1_min_alpha, t, log_x_t.shape)\n\n        # alpha_t * E[xt] + (1 - alpha_t) 1 / K\n        return log_add_exp(\n            log_x_t + log_alpha_t,\n            log_1_min_alpha_t - torch.log(self.num_classes_expanded),\n        )\n\n    def q_pred(self, log_x_start: Tensor, t: Tensor) -&gt; Tensor:\n        log_cumprod_alpha_t = extract(self.log_cumprod_alpha, t, log_x_start.shape)\n        log_1_min_cumprod_alpha = extract(self.log_1_min_cumprod_alpha, t, log_x_start.shape)\n\n        return log_add_exp(\n            log_x_start + log_cumprod_alpha_t,\n            log_1_min_cumprod_alpha - torch.log(self.num_classes_expanded),\n        )\n\n    def predict_start(self, model_out: Tensor, log_x_t: Tensor, t: Tensor, out_dict: dict[str, Tensor]) -&gt; Tensor:\n        # model_out = self._denoise_fn(x_t, t.to(x_t.device), **out_dict)\n\n        assert model_out.size(0) == log_x_t.size(0)\n        assert self.num_classes is not None\n        assert model_out.size(1) == self.num_classes.sum(), f\"{model_out.size()}\"\n\n        log_pred = torch.empty_like(model_out)\n        for ix in self.slices_for_classes:\n            log_pred[:, ix] = F.log_softmax(model_out[:, ix], dim=1)\n        return log_pred\n\n    def q_posterior(self, log_x_start: Tensor, log_x_t: Tensor, t: Tensor) -&gt; Tensor:\n        # q(xt-1 | xt, x0) = q(xt | xt-1, x0) * q(xt-1 | x0) / q(xt | x0)\n        # where q(xt | xt-1, x0) = q(xt | xt-1).\n\n        # EV_log_qxt_x0 = self.q_pred(log_x_start, t)\n\n        # print('sum exp', EV_log_qxt_x0.exp().sum(1).mean())\n        # assert False\n\n        # log_qxt_x0 = (log_x_t.exp() * EV_log_qxt_x0).sum(dim=1)\n        t_minus_1 = t - 1\n        # Remove negative values, will not be used anyway for final decoder\n        t_minus_1 = torch.where(t_minus_1 &lt; 0, torch.zeros_like(t_minus_1), t_minus_1)\n        log_EV_qxtmin_x0 = self.q_pred(log_x_start, t_minus_1)\n\n        num_axes = (1,) * (len(log_x_start.size()) - 1)\n        t_broadcast = t.to(log_x_start.device).view(-1, *num_axes) * torch.ones_like(log_x_start)\n        log_EV_qxtmin_x0 = torch.where(t_broadcast == 0, log_x_start, log_EV_qxtmin_x0.to(torch.float32))\n\n        # unnormed_logprobs = log_EV_qxtmin_x0 +\n        #                     log q_pred_one_timestep(x_t, t)\n        # Note: _NOT_ x_tmin1, which is how the formula is typically used!!!\n        # Not very easy to see why this is true. But it is :)\n        unnormed_logprobs = log_EV_qxtmin_x0 + self.q_pred_one_timestep(log_x_t, t)\n\n        return unnormed_logprobs - sliced_logsumexp(unnormed_logprobs, self.offsets)\n\n    def p_pred(self, model_out: Tensor, log_x: Tensor, t: Tensor, out_dict: dict[str, Tensor]) -&gt; Tensor:\n        if self.parametrization == \"x0\":\n            log_x_recon = self.predict_start(model_out, log_x, t=t, out_dict=out_dict)\n            log_model_pred = self.q_posterior(log_x_start=log_x_recon, log_x_t=log_x, t=t)\n        elif self.parametrization == \"direct\":\n            log_model_pred = self.predict_start(model_out, log_x, t=t, out_dict=out_dict)\n        else:\n            raise ValueError\n        return log_model_pred\n\n    @torch.no_grad()\n    def p_sample(self, model_out: Tensor, log_x: Tensor, t: Tensor, out_dict: dict[str, Tensor]) -&gt; Tensor:\n        model_log_prob = self.p_pred(model_out, log_x=log_x, t=t, out_dict=out_dict)\n        return self.log_sample_categorical(model_log_prob)\n\n    # Dead code\n    # @torch.no_grad()\n    # def p_sample_loop(self, shape, out_dict):\n    #     b = shape[0]\n    #     # start with random normal image.\n    #     img = torch.randn(shape, device=device)\n\n    #     for i in reversed(range(1, self.num_timesteps)):\n    #         img = self.p_sample(img, torch.full((b,), i, device=self.device, dtype=torch.long), out_dict)\n    #     return img\n\n    # @torch.no_grad()\n    # def _sample(self, image_size, out_dict, batch_size=16):\n    #     return self.p_sample_loop((batch_size, 3, image_size, image_size), out_dict)\n\n    # Dead code\n    # @torch.no_grad()\n    # def interpolate(self, x1: Tensor, x2: Tensor, t: Tensor | None = None, lam: float = 0.5) -&gt; Tensor:\n    #     b, *_, device = *x1.shape, x1.device\n    #     t = default(t, self.num_timesteps - 1)\n\n    #     assert x1.shape == x2.shape\n\n    #     t_batched = torch.stack([torch.tensor(t, device=device)] * b)\n    #     xt1, xt2 = map(lambda x: self.q_sample(x, t=t_batched), (x1, x2))\n\n    #     img = (1 - lam) * xt1 + lam * xt2\n    #     for i in reversed(range(0, t)):\n    #         img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long))\n\n    #     return img\n\n    def log_sample_categorical(self, logits: Tensor) -&gt; Tensor:\n        full_sample = []\n        for i in range(len(self.num_classes)):\n            one_class_logits = logits[:, self.slices_for_classes[i]]\n            uniform = torch.rand_like(one_class_logits)\n            gumbel_noise = -torch.log(-torch.log(uniform + 1e-30) + 1e-30)\n            sample = (gumbel_noise + one_class_logits).argmax(dim=1)\n            full_sample.append(sample.unsqueeze(1))\n        full_sample_tensor = torch.cat(full_sample, dim=1)\n        return index_to_log_onehot(full_sample_tensor, torch.from_numpy(self.num_classes))\n\n    def q_sample(self, log_x_start: Tensor, t: Tensor) -&gt; Tensor:\n        log_EV_qxt_x0 = self.q_pred(log_x_start, t)\n        # ruff: noqa: N806\n        return self.log_sample_categorical(log_EV_qxt_x0)\n\n    # Dead code\n    # def nll(self, log_x_start, out_dict):\n    #     b = log_x_start.size(0)\n    #     device = log_x_start.device\n    #     loss = 0\n    #     for t in range(0, self.num_timesteps):\n    #         t_array = (torch.ones(b, device=device) * t).long()\n\n    #         kl = self.compute_Lt(\n    #             log_x_start=log_x_start,\n    #             log_x_t=self.q_sample(log_x_start=log_x_start, t=t_array),\n    #             t=t_array,\n    #             out_dict=out_dict,\n    #         )\n\n    #         loss += kl\n\n    #     loss += self.kl_prior(log_x_start)\n\n    #     return loss\n\n    def kl_prior(self, log_x_start: Tensor) -&gt; Tensor:\n        b = log_x_start.size(0)\n        device = log_x_start.device\n        ones = torch.ones(b, device=device).long()\n\n        log_qxT_prob = self.q_pred(log_x_start, t=(self.num_timesteps - 1) * ones)\n        # ruff: noqa: N806\n        log_half_prob = -torch.log(self.num_classes_expanded * torch.ones_like(log_qxT_prob))\n\n        kl_prior = self.multinomial_kl(log_qxT_prob, log_half_prob)\n        return sum_except_batch(kl_prior)\n\n    def compute_Lt(\n        # ruff: noqa: N802\n        self,\n        model_out: Tensor,\n        log_x_start: Tensor,\n        log_x_t: Tensor,\n        t: Tensor,\n        out_dict: dict[str, Tensor],\n        detach_mean: bool = False,\n    ) -&gt; Tensor:\n        log_true_prob = self.q_posterior(log_x_start=log_x_start, log_x_t=log_x_t, t=t)\n        log_model_prob = self.p_pred(model_out, log_x=log_x_t, t=t, out_dict=out_dict)\n\n        if detach_mean:\n            log_model_prob = log_model_prob.detach()\n\n        kl = self.multinomial_kl(log_true_prob, log_model_prob)\n        kl = sum_except_batch(kl)\n\n        decoder_nll = -log_categorical(log_x_start, log_model_prob)\n        decoder_nll = sum_except_batch(decoder_nll)\n\n        mask = (t == torch.zeros_like(t)).float()\n        return mask * decoder_nll + (1.0 - mask) * kl\n\n    def sample_time(self, b: int, device: torch.device, method: str = \"uniform\") -&gt; tuple[Tensor, Tensor]:\n        if method == \"importance\":\n            if not (self.Lt_count &gt; 10).all():\n                return self.sample_time(b, device, method=\"uniform\")\n\n            Lt_sqrt = torch.sqrt(self.Lt_history + 1e-10) + 0.0001\n            # ruff: noqa: N806\n            Lt_sqrt[0] = Lt_sqrt[1]  # Overwrite decoder term with L1.\n            pt_all = (Lt_sqrt / Lt_sqrt.sum()).to(device)\n\n            t = torch.multinomial(pt_all, num_samples=b, replacement=True).to(device)\n\n            pt = pt_all.gather(dim=0, index=t)\n\n            return t, pt\n\n        if method == \"uniform\":\n            t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n\n            pt = torch.ones_like(t).float() / self.num_timesteps\n            return t, pt\n        raise ValueError\n\n    def _multinomial_loss(\n        self,\n        model_out: Tensor,\n        log_x_start: Tensor,\n        log_x_t: Tensor,\n        t: Tensor,\n        pt: Tensor,\n        out_dict: dict[str, Tensor],\n    ) -&gt; Tensor:\n        if self.multinomial_loss_type == \"vb_stochastic\":\n            kl = self.compute_Lt(model_out, log_x_start, log_x_t, t, out_dict)\n            kl_prior = self.kl_prior(log_x_start)\n            # Upweigh loss term of the kl\n            return kl / pt + kl_prior\n\n        if self.multinomial_loss_type == \"vb_all\":\n            # Expensive, dont do it ;).\n            # DEPRECATED\n            # return -self.nll(log_x_start)\n            raise ValueError(\"multinomial_loss_type == 'vb_all' is deprecated.\")\n        raise ValueError\n\n    # Dead code\n    # def log_prob(self, x, out_dict):\n    #     b, device = x.size(0), x.device\n    #     if self.training:\n    #         return self._multinomial_loss(x, out_dict)\n\n    #     log_x_start = index_to_log_onehot(x, self.num_classes)\n\n    #     t, pt = self.sample_time(b, device, \"importance\")\n\n    #     kl = self.compute_Lt(log_x_start, self.q_sample(log_x_start=log_x_start, t=t), t, out_dict)\n\n    #     kl_prior = self.kl_prior(log_x_start)\n\n    #     # Upweigh loss term of the kl\n    #     loss = kl / pt + kl_prior\n\n    #     return -loss\n\n    def mixed_loss(self, x: Tensor, out_dict: dict[str, Tensor]) -&gt; tuple[Tensor, Tensor]:\n        b = x.shape[0]\n        device = x.device\n        t, pt = self.sample_time(b, device, \"uniform\")\n\n        x_num = x[:, : self.num_numerical_features]\n        x_cat = x[:, self.num_numerical_features :]\n\n        x_num_t = x_num\n        log_x_cat_t = x_cat\n        if x_num.shape[1] &gt; 0:\n            noise = torch.randn_like(x_num)\n            x_num_t = self.gaussian_q_sample(x_num, t, noise=noise)\n        if x_cat.shape[1] &gt; 0:\n            log_x_cat = index_to_log_onehot(x_cat.long(), torch.from_numpy(self.num_classes))\n            log_x_cat_t = self.q_sample(log_x_start=log_x_cat, t=t)\n\n        x_in = torch.cat([x_num_t, log_x_cat_t], dim=1)\n\n        model_out = self._denoise_fn(x_in, t, **out_dict)\n\n        model_out_num = model_out[:, : self.num_numerical_features]\n        model_out_cat = model_out[:, self.num_numerical_features :]\n\n        loss_multi = torch.zeros((1,)).float()\n        loss_gauss = torch.zeros((1,)).float()\n        if x_cat.shape[1] &gt; 0:\n            loss_multi = self._multinomial_loss(model_out_cat, log_x_cat, log_x_cat_t, t, pt, out_dict) / len(\n                self.num_classes\n            )\n\n        if x_num.shape[1] &gt; 0:\n            loss_gauss = self._gaussian_loss(model_out_num, x_num, x_num_t, t, noise)\n\n        # loss_multi = torch.where(out_dict['y'] == 1, loss_multi, 2 * loss_multi)\n        # loss_gauss = torch.where(out_dict['y'] == 1, loss_gauss, 2 * loss_gauss)\n\n        return loss_multi.mean(), loss_gauss.mean()\n\n    @torch.no_grad()\n    def mixed_elbo(self, x0, out_dict):\n        b = x0.size(0)\n        device = x0.device\n\n        x_num = x0[:, : self.num_numerical_features]\n        x_cat = x0[:, self.num_numerical_features :]\n        has_cat = x_cat.shape[1] &gt; 0\n        if has_cat:\n            log_x_cat = index_to_log_onehot(x_cat.long(), torch.from_numpy(self.num_classes)).to(device)\n\n        gaussian_loss = []\n        xstart_mse = []\n        mse = []\n        # mu_mse = []\n        out_mean = []\n        true_mean = []\n        multinomial_loss = []\n        for t in range(self.num_timesteps):\n            t_array = (torch.ones(b, device=device) * t).long()\n            noise = torch.randn_like(x_num)\n\n            x_num_t = self.gaussian_q_sample(x_start=x_num, t=t_array, noise=noise)\n            log_x_cat_t = self.q_sample(log_x_start=log_x_cat, t=t_array) if has_cat else x_cat\n\n            model_out = self._denoise_fn(torch.cat([x_num_t, log_x_cat_t], dim=1), t_array, **out_dict)\n\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n\n            kl = torch.tensor([0.0])\n            if has_cat:\n                kl = self.compute_Lt(\n                    model_out=model_out_cat,\n                    log_x_start=log_x_cat,\n                    log_x_t=log_x_cat_t,\n                    t=t_array,\n                    out_dict=out_dict,\n                )\n\n            out = self._vb_terms_bpd(\n                model_out_num,\n                x_start=x_num,\n                x_t=x_num_t,\n                t=t_array,\n                clip_denoised=False,\n            )\n\n            multinomial_loss.append(kl)\n            gaussian_loss.append(out[\"output\"])\n            xstart_mse.append(mean_flat((out[\"pred_xstart\"] - x_num) ** 2))\n            # mu_mse.append(mean_flat(out[\"mean_mse\"]))\n            out_mean.append(mean_flat(out[\"out_mean\"]))\n            true_mean.append(mean_flat(out[\"true_mean\"]))\n\n            eps = self._predict_eps_from_xstart(x_num_t, t_array, out[\"pred_xstart\"])\n            mse.append(mean_flat((eps - noise) ** 2))\n\n        gaussian_loss_tensor = torch.stack(gaussian_loss, dim=1)\n        multinomial_loss_tensor = torch.stack(multinomial_loss, dim=1)\n        xstart_mse_tensor = torch.stack(xstart_mse, dim=1)\n        mse_tensor = torch.stack(mse, dim=1)\n        # mu_mse = torch.stack(mu_mse, dim=1)\n        out_mean_tensor = torch.stack(out_mean, dim=1)\n        true_mean_tensor = torch.stack(true_mean, dim=1)\n\n        prior_gauss = self._prior_gaussian(x_num)\n\n        prior_multin = torch.tensor([0.0])\n        if has_cat:\n            prior_multin = self.kl_prior(log_x_cat)\n\n        total_gauss = gaussian_loss_tensor.sum(dim=1) + prior_gauss\n        total_multin = multinomial_loss_tensor.sum(dim=1) + prior_multin\n        return {\n            \"total_gaussian\": total_gauss,\n            \"total_multinomial\": total_multin,\n            \"losses_gaussian\": gaussian_loss_tensor,\n            \"losses_multinimial\": multinomial_loss_tensor,\n            \"xstart_mse\": xstart_mse_tensor,\n            \"mse\": mse_tensor,\n            # \"mu_mse\": mu_mse\n            \"out_mean\": out_mean_tensor,\n            \"true_mean\": true_mean_tensor,\n        }\n\n    @torch.no_grad()\n    def gaussian_ddim_step(\n        self,\n        model_out_num: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        denoised_fn: Callable | None = None,\n        eta: float = 0.0,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; Tensor:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        out = self.gaussian_p_mean_variance(\n            model_out_num,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=None,\n        )\n\n        if cond_fn is not None:\n            out = self.condition_score(cond_fn, out, x, t, model_kwargs=model_kwargs)\n\n        eps = self._predict_eps_from_xstart(x, t, out[\"pred_xstart\"])\n\n        alpha_bar = extract(self.alphas_cumprod, t, x.shape)\n        alpha_bar_prev = extract(self.alphas_cumprod_prev, t, x.shape)\n        sigma = eta * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar)) * torch.sqrt(1 - alpha_bar / alpha_bar_prev)\n\n        noise = torch.randn_like(x)\n        mean_pred = out[\"pred_xstart\"] * torch.sqrt(alpha_bar_prev) + torch.sqrt(1 - alpha_bar_prev - sigma**2) * eps\n        nonzero_mask = (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))  # no noise when t == 0\n        return mean_pred + nonzero_mask * sigma * noise\n\n    @torch.no_grad()\n    def gaussian_ddim_sample(self, noise, T, out_dict, eta=0.0, model_kwargs=None, cond_fn=None):\n        # ruff: noqa: D102, N803\n        x = noise\n        b = x.shape[0]\n        device = x.device\n        for t in reversed(range(T)):\n            print(f\"Sample timestep {t:4d}\", end=\"\\r\")\n            t_array = (torch.ones(b, device=device) * t).long()\n            out_num = self._denoise_fn(x, t_array, **out_dict)\n            x = self.gaussian_ddim_step(out_num, x, t_array, model_kwargs=model_kwargs, cond_fn=cond_fn)\n        print()\n        return x\n\n    @torch.no_grad()\n    def gaussian_ddim_reverse_step(\n        self,\n        model_out_num: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        eta: float = 0.0,\n    ) -&gt; Tensor:\n        # ruff: noqa: D102\n        assert eta == 0.0, \"Eta must be zero.\"\n        out = self.gaussian_p_mean_variance(\n            model_out_num,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=None,\n            model_kwargs=None,\n        )\n\n        eps = (extract(self.sqrt_recip_alphas_cumprod, t, x.shape) * x - out[\"pred_xstart\"]) / extract(\n            self.sqrt_recipm1_alphas_cumprod, t, x.shape\n        )\n        alpha_bar_next = extract(self.alphas_cumprod_next, t, x.shape)\n\n        return out[\"pred_xstart\"] * torch.sqrt(alpha_bar_next) + torch.sqrt(1 - alpha_bar_next) * eps\n\n    @torch.no_grad()\n    def gaussian_ddim_reverse_sample(\n        self,\n        x,\n        T,\n        # ruff: noqa: N803\n        out_dict,\n    ):\n        # ruff: noqa: D102\n        b = x.shape[0]\n        device = x.device\n        for t in range(T):\n            print(f\"Reverse timestep {t:4d}\", end=\"\\r\")\n            t_array = (torch.ones(b, device=device) * t).long()\n            out_num = self._denoise_fn(x, t_array, **out_dict)\n            x = self.gaussian_ddim_reverse_step(out_num, x, t_array, eta=0.0)\n        print()\n\n        return x\n\n    @torch.no_grad()\n    def multinomial_ddim_step(\n        self,\n        model_out_cat: Tensor,\n        log_x_t: Tensor,\n        t: Tensor,\n        out_dict: dict[str, Tensor],\n        eta: float = 0.0,\n    ) -&gt; Tensor:\n        # ruff: noqa: D102\n        # not ddim, essentially\n        log_x0 = self.predict_start(model_out_cat, log_x_t=log_x_t, t=t, out_dict=out_dict)\n\n        alpha_bar = extract(self.alphas_cumprod, t, log_x_t.shape)\n        alpha_bar_prev = extract(self.alphas_cumprod_prev, t, log_x_t.shape)\n        sigma = eta * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar)) * torch.sqrt(1 - alpha_bar / alpha_bar_prev)\n\n        coef1 = sigma\n        coef2 = alpha_bar_prev - sigma * alpha_bar\n        coef3 = 1 - coef1 - coef2\n\n        log_ps = torch.stack(\n            [\n                torch.log(coef1) + log_x_t,\n                torch.log(coef2) + log_x0,\n                torch.log(coef3) - torch.log(self.num_classes_expanded),\n            ],\n            dim=2,\n        )\n\n        log_prob = torch.logsumexp(log_ps, dim=2)\n\n        return self.log_sample_categorical(log_prob)\n\n    @torch.no_grad()\n    def sample_ddim(\n        self,\n        num_samples: int,\n        y_dist: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; tuple[Tensor, dict[str, Tensor]]:\n        # ruff: noqa: D102\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        b = num_samples\n        z_norm = torch.randn((b, self.num_numerical_features), device=self.device)\n\n        assert self.num_classes is not None\n        has_cat = self.num_classes[0] != 0\n        log_z = torch.zeros((b, 0), device=self.device).float()\n        if has_cat:\n            uniform_logits = torch.zeros((b, len(self.num_classes_expanded)), device=self.device)\n            log_z = self.log_sample_categorical(uniform_logits)\n\n        y = torch.multinomial(y_dist, num_samples=b, replacement=True)\n        out_dict = {\"y\": y.long().to(self.device)}\n        for i in reversed(range(0, self.num_timesteps)):\n            print(f\"Sample timestep {i:4d}\", end=\"\\r\")\n            t = torch.full((b,), i, device=self.device, dtype=torch.long)\n            model_out = self._denoise_fn(torch.cat([z_norm, log_z], dim=1).float(), t, **out_dict)\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n            z_norm = self.gaussian_ddim_step(\n                model_out_num,\n                z_norm,\n                t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n                cond_fn=cond_fn,\n            )\n            if has_cat:\n                log_z = self.multinomial_ddim_step(model_out_cat, log_z, t, out_dict)\n\n        print()\n        z_ohe = torch.exp(log_z).round()\n        z_cat = log_z\n        if has_cat:\n            z_cat = ohe_to_categories(z_ohe, torch.from_numpy(self.num_classes))\n        sample = torch.cat([z_norm, z_cat], dim=1).cpu()\n        return sample, out_dict\n\n    @torch.no_grad()\n    def conditional_sample(\n        self,\n        ys: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; tuple[Tensor, dict[str, Tensor]]:\n        # ruff: noqa: D102\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        b = len(ys)\n        z_norm = torch.randn((b, self.num_numerical_features), device=self.device)\n        assert self.num_classes is not None\n        has_cat = self.num_classes[0] != 0\n        log_z = torch.zeros((b, 0), device=self.device).float()\n\n        out_dict = {\"y\": ys.long().to(self.device)}\n        for i in reversed(range(0, self.num_timesteps)):\n            print(f\"Sample timestep {i:4d}\", end=\"\\r\")\n            t = torch.full((b,), i, device=self.device, dtype=torch.long)\n            model_out = self._denoise_fn(torch.cat([z_norm, log_z], dim=1).float(), t, **out_dict)\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n            z_norm = self.gaussian_p_sample(\n                model_out_num,\n                z_norm,\n                t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n                cond_fn=cond_fn,\n            )[\"sample\"]\n            if has_cat:\n                log_z = self.p_sample(model_out_cat, log_z, t, out_dict)\n\n        print()\n        z_ohe = torch.exp(log_z).round()\n        z_cat = log_z\n        if has_cat:\n            z_cat = ohe_to_categories(z_ohe, torch.from_numpy(self.num_classes))\n        sample = torch.cat([z_norm, z_cat], dim=1).cpu()\n        return sample, out_dict\n\n    @torch.no_grad()\n    def sample(\n        self,\n        num_samples: int,\n        y_dist: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; tuple[Tensor, dict[str, Tensor]]:\n        # ruff: noqa: D102\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        b = num_samples\n        z_norm = torch.randn((b, self.num_numerical_features), device=self.device)\n\n        assert self.num_classes is not None\n        has_cat = self.num_classes[0] != 0\n        log_z = torch.zeros((b, 0), device=self.device).float()\n        if has_cat:\n            uniform_logits = torch.zeros((b, len(self.num_classes_expanded)), device=self.device)\n            log_z = self.log_sample_categorical(uniform_logits)\n\n        y = torch.multinomial(y_dist, num_samples=b, replacement=True)\n        out_dict = {\"y\": y.long().to(self.device)}\n        for i in reversed(range(0, self.num_timesteps)):\n            print(f\"Sample timestep {i:4d}\", end=\"\\r\")\n            t = torch.full((b,), i, device=self.device, dtype=torch.long)\n            model_out = self._denoise_fn(torch.cat([z_norm, log_z], dim=1).float(), t, **out_dict)\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n            z_norm = self.gaussian_p_sample(\n                model_out_num,\n                z_norm,\n                t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n                cond_fn=cond_fn,\n            )[\"sample\"]\n            if has_cat:\n                log_z = self.p_sample(model_out_cat, log_z, t, out_dict)\n\n        print()\n        z_ohe = torch.exp(log_z).round()\n        z_cat = log_z\n        if has_cat:\n            z_cat = ohe_to_categories(z_ohe, torch.from_numpy(self.num_classes))\n        sample = torch.cat([z_norm, z_cat], dim=1).cpu()\n        return sample, out_dict\n\n    def sample_all(\n        self,\n        num_samples: int,\n        batch_size: int,\n        y_dist: Tensor,\n        ddim: bool = False,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; tuple[Tensor, Tensor]:\n        # ruff: noqa: D102\n        if ddim:\n            print(\"Sample using DDIM.\")\n            sample_fn = self.sample_ddim\n        else:\n            sample_fn = self.sample\n\n        b = batch_size\n\n        all_y = []\n        all_samples = []\n        num_generated = 0\n        while num_generated &lt; num_samples:\n            sample, out_dict = sample_fn(b, y_dist, model_kwargs=model_kwargs, cond_fn=cond_fn)\n            mask_nan = torch.any(sample.isnan(), dim=1)\n            sample = sample[~mask_nan]\n            out_dict[\"y\"] = out_dict[\"y\"][~mask_nan]\n\n            all_samples.append(sample)\n            all_y.append(out_dict[\"y\"].cpu())\n            if sample.shape[0] != b:\n                raise FoundNANsError\n            num_generated += sample.shape[0]\n\n        x_gen = torch.cat(all_samples, dim=0)[:num_samples]\n        y_gen = torch.cat(all_y, dim=0)[:num_samples]\n\n        return x_gen, y_gen\n</code></pre> <code></code> condition_mean \u00b6 <pre><code>condition_mean(\n    cond_fn, p_mean_var, x, t, model_kwargs=None\n)\n</code></pre> <p>Compute the mean for the previous step, given a function cond_fn that computes the gradient of a conditional log probability with respect to x. In particular, cond_fn computes grad(log(p(y|x))), and we want to condition on y.</p> <p>This uses the conditioning strategy from Sohl-Dickstein et al. (2015).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def condition_mean(\n    self,\n    cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n    p_mean_var: dict[str, Tensor],\n    x: Tensor,\n    t: Tensor,\n    model_kwargs: dict[str, Any] | None = None,\n) -&gt; Tensor:\n    \"\"\"\n    Compute the mean for the previous step, given a function cond_fn that\n    computes the gradient of a conditional log probability with respect to\n    x. In particular, cond_fn computes grad(log(p(y|x))), and we want to\n    condition on y.\n\n    This uses the conditioning strategy from Sohl-Dickstein et al. (2015).\n    \"\"\"\n    if model_kwargs is None:\n        model_kwargs = {}\n\n    gradient = cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n    return p_mean_var[\"mean\"].float() + p_mean_var[\"variance\"] * gradient.float()\n</code></pre> <code></code> condition_score \u00b6 <pre><code>condition_score(\n    cond_fn, p_mean_var, x, t, model_kwargs=None\n)\n</code></pre> <p>Compute what the p_mean_variance output would have been, should the model's score function be conditioned by cond_fn.</p> <p>See condition_mean() for details on cond_fn.</p> <p>Unlike condition_mean(), this instead uses the conditioning strategy from Song et al (2020).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def condition_score(\n    self,\n    cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n    p_mean_var: dict[str, Tensor],\n    x: Tensor,\n    t: Tensor,\n    model_kwargs: dict[str, Any] | None = None,\n) -&gt; dict[str, Tensor]:\n    \"\"\"\n    Compute what the p_mean_variance output would have been, should the\n    model's score function be conditioned by cond_fn.\n\n    See condition_mean() for details on cond_fn.\n\n    Unlike condition_mean(), this instead uses the conditioning strategy\n    from Song et al (2020).\n    \"\"\"\n    if model_kwargs is None:\n        model_kwargs = {}\n\n    alpha_bar = extract(self.alphas_cumprod, t, x.shape)\n\n    eps = self._predict_eps_from_xstart(x, t, p_mean_var[\"pred_xstart\"])\n    eps = eps - (1 - alpha_bar).sqrt() * cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n\n    out = p_mean_var.copy()\n    out[\"pred_xstart\"] = self._predict_xstart_from_eps(x, t, eps)\n    out[\"mean\"], _, _ = self.gaussian_q_posterior_mean_variance(x_start=out[\"pred_xstart\"], x_t=x, t=t)\n    return out\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.get_named_beta_schedule","title":"get_named_beta_schedule","text":"<pre><code>get_named_beta_schedule(\n    schedule_name, num_diffusion_timesteps\n)\n</code></pre> <p>Get a pre-defined beta schedule for the given name. The beta schedule library consists of beta schedules which remain similar in the limit of num_diffusion_timesteps. Beta schedules may be added, but should not be removed or changed once they are committed to maintain backwards compatibility.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def get_named_beta_schedule(schedule_name: str, num_diffusion_timesteps: int) -&gt; np.ndarray:\n    \"\"\"\n    Get a pre-defined beta schedule for the given name.\n    The beta schedule library consists of beta schedules which remain similar\n    in the limit of num_diffusion_timesteps.\n    Beta schedules may be added, but should not be removed or changed once\n    they are committed to maintain backwards compatibility.\n    \"\"\"\n    if schedule_name == \"linear\":\n        # Linear schedule from Ho et al, extended to work for any number of\n        # diffusion steps.\n        scale = 1000 / num_diffusion_timesteps\n        beta_start = scale * 0.0001\n        beta_end = scale * 0.02\n        return np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64)\n    if schedule_name == \"cosine\":\n        return betas_for_alpha_bar(\n            num_diffusion_timesteps,\n            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n        )\n    raise NotImplementedError(f\"unknown beta schedule: {schedule_name}\")\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.betas_for_alpha_bar","title":"betas_for_alpha_bar","text":"<pre><code>betas_for_alpha_bar(\n    num_diffusion_timesteps, alpha_bar, max_beta=0.999\n)\n</code></pre> <p>Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of (1-beta) over time from t = [0,1]. :param num_diffusion_timesteps: the number of betas to produce. :param alpha_bar: a lambda that takes an argument t from 0 to 1 and                   produces the cumulative product of (1-beta) up to that                   part of the diffusion process. :param max_beta: the maximum beta to use; use values lower than 1 to                  prevent singularities.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def betas_for_alpha_bar(num_diffusion_timesteps: int, alpha_bar: Callable, max_beta: float = 0.999) -&gt; np.ndarray:\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function,\n    which defines the cumulative product of (1-beta) over time from t = [0,1].\n    :param num_diffusion_timesteps: the number of betas to produce.\n    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n                      produces the cumulative product of (1-beta) up to that\n                      part of the diffusion process.\n    :param max_beta: the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n    \"\"\"\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return np.array(betas)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model","title":"model","text":""},{"location":"api/#midst_toolkit.models.clavaddpm.model.FastTensorDataLoader","title":"FastTensorDataLoader","text":"<p>Defines a faster dataloader for PyTorch tensors.</p> <p>A DataLoader-like object for a set of tensors that can be much faster than TensorDataset + DataLoader because dataloader grabs individual indices of the dataset and calls cat (slow). Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class FastTensorDataLoader:\n    \"\"\"\n    Defines a faster dataloader for PyTorch tensors.\n\n    A DataLoader-like object for a set of tensors that can be much faster than\n    TensorDataset + DataLoader because dataloader grabs individual indices of\n    the dataset and calls cat (slow).\n    Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6\n    \"\"\"\n\n    def __init__(self, *tensors: Tensor, batch_size: int = 32, shuffle: bool = False):\n        \"\"\"\n        Initialize a FastTensorDataLoader.\n        :param *tensors: tensors to store. Must have the same length @ dim 0.\n        :param batch_size: batch size to load.\n        :param shuffle: if True, shuffle the data *in-place* whenever an\n            iterator is created out of this object.\n        :returns: A FastTensorDataLoader.\n        \"\"\"\n        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n        self.tensors = tensors\n\n        self.dataset_len = self.tensors[0].shape[0]\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n\n        # Calculate # batches\n        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n        if remainder &gt; 0:\n            n_batches += 1\n        self.n_batches = n_batches\n\n    def __iter__(self):\n        # ruff: noqa: D105\n        if self.shuffle:\n            r = torch.randperm(self.dataset_len)\n            self.tensors = [t[r] for t in self.tensors]  # type: ignore[assignment]\n        self.i = 0\n        return self\n\n    def __next__(self):\n        # ruff: noqa: D105\n        if self.i &gt;= self.dataset_len:\n            raise StopIteration\n        batch = tuple(t[self.i : self.i + self.batch_size] for t in self.tensors)\n        self.i += self.batch_size\n        return batch\n\n    def __len__(self):\n        # ruff: noqa: D105\n        return self.n_batches\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(*tensors, batch_size=32, shuffle=False)\n</code></pre> <p>Initialize a FastTensorDataLoader. :param tensors: tensors to store. Must have the same length @ dim 0. :param batch_size: batch size to load. :param shuffle: if True, shuffle the data in-place* whenever an     iterator is created out of this object. :returns: A FastTensorDataLoader.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def __init__(self, *tensors: Tensor, batch_size: int = 32, shuffle: bool = False):\n    \"\"\"\n    Initialize a FastTensorDataLoader.\n    :param *tensors: tensors to store. Must have the same length @ dim 0.\n    :param batch_size: batch size to load.\n    :param shuffle: if True, shuffle the data *in-place* whenever an\n        iterator is created out of this object.\n    :returns: A FastTensorDataLoader.\n    \"\"\"\n    assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n    self.tensors = tensors\n\n    self.dataset_len = self.tensors[0].shape[0]\n    self.batch_size = batch_size\n    self.shuffle = shuffle\n\n    # Calculate # batches\n    n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n    if remainder &gt; 0:\n        n_batches += 1\n    self.n_batches = n_batches\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.MLP","title":"MLP","text":"<p>               Bases: <code>Module</code></p> <p>The MLP model used in [gorishniy2021revisiting].</p> <p>The following scheme describes the architecture:</p> <p>.. code-block:: text</p> <pre><code>  MLP: (in) -&gt; Block -&gt; ... -&gt; Block -&gt; Linear -&gt; (out)\nBlock: (in) -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; (out)\n</code></pre> <p>Examples:</p> <p>.. testcode::</p> <pre><code>x = torch.randn(4, 2)\nmodule = MLP.make_baseline(x.shape[1], [3, 5], 0.1, 1)\nassert module(x).shape == (len(x), 1)\n</code></pre> References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class MLP(nn.Module):\n    \"\"\"The MLP model used in [gorishniy2021revisiting].\n\n    The following scheme describes the architecture:\n\n    .. code-block:: text\n\n          MLP: (in) -&gt; Block -&gt; ... -&gt; Block -&gt; Linear -&gt; (out)\n        Block: (in) -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; (out)\n\n    Examples:\n        .. testcode::\n\n            x = torch.randn(4, 2)\n            module = MLP.make_baseline(x.shape[1], [3, 5], 0.1, 1)\n            assert module(x).shape == (len(x), 1)\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n\n    class Block(nn.Module):\n        \"\"\"The main building block of `MLP`.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            d_in: int,\n            d_out: int,\n            bias: bool,\n            activation: ModuleType,\n            dropout: float,\n        ) -&gt; None:\n            super().__init__()\n            self.linear = nn.Linear(d_in, d_out, bias)\n            self.activation = _make_nn_module(activation)\n            self.dropout = nn.Dropout(dropout)\n\n        def forward(self, x: Tensor) -&gt; Tensor:\n            return self.dropout(self.activation(self.linear(x)))\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        d_layers: list[int],\n        dropouts: float | list[float],\n        activation: str | Callable[[], nn.Module],\n        d_out: int,\n    ) -&gt; None:\n        \"\"\"\n        Note:\n            `make_baseline` is the recommended constructor.\n        \"\"\"\n        super().__init__()\n        if isinstance(dropouts, float):\n            dropouts = [dropouts] * len(d_layers)\n        assert len(d_layers) == len(dropouts)\n        assert activation not in [\"ReGLU\", \"GEGLU\"]\n\n        self.blocks = nn.ModuleList(\n            [\n                MLP.Block(\n                    d_in=d_layers[i - 1] if i else d_in,\n                    d_out=d,\n                    bias=True,\n                    activation=activation,\n                    dropout=dropout,\n                )\n                for i, (d, dropout) in enumerate(zip(d_layers, dropouts))\n            ]\n        )\n        self.head = nn.Linear(d_layers[-1] if d_layers else d_in, d_out)\n\n    @classmethod\n    def make_baseline(\n        cls,\n        d_in: int,\n        d_layers: list[int],\n        dropout: float,\n        d_out: int,\n    ) -&gt; Self:\n        \"\"\"Create a \"baseline\" `MLP`.\n\n        This variation of MLP was used in [gorishniy2021revisiting]. Features:\n\n        * all linear layers except for the first one and the last one are of the same dimension\n        * the dropout rate is the same for all dropout layers\n\n        Args:\n            d_in: the input size\n            d_layers: the dimensions of the linear layers. If there are more than two\n                layers, then all of them except for the first and the last ones must\n                have the same dimension.\n            dropout: the dropout rate for all hidden layers\n            d_out: the output size\n        Returns:\n            MLP\n\n        References:\n            * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n            Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n        \"\"\"\n        assert isinstance(dropout, float)\n        if len(d_layers) &gt; 2:\n            assert len(set(d_layers[1:-1])) == 1, (\n                \"if d_layers contains more than two elements, then\"\n                \" all elements except for the first and the last ones must be equal.\"\n            )\n        return cls(\n            d_in=d_in,\n            d_layers=d_layers,\n            dropouts=dropout,\n            activation=\"ReLU\",\n            d_out=d_out,\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = x.float()\n        for block in self.blocks:\n            x = block(x)\n        return self.head(x)\n</code></pre> <code></code> Block \u00b6 <p>               Bases: <code>Module</code></p> <p>The main building block of <code>MLP</code>.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class Block(nn.Module):\n    \"\"\"The main building block of `MLP`.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        d_out: int,\n        bias: bool,\n        activation: ModuleType,\n        dropout: float,\n    ) -&gt; None:\n        super().__init__()\n        self.linear = nn.Linear(d_in, d_out, bias)\n        self.activation = _make_nn_module(activation)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        return self.dropout(self.activation(self.linear(x)))\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(*, d_in, d_layers, dropouts, activation, d_out)\n</code></pre> Note <p><code>make_baseline</code> is the recommended constructor.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def __init__(\n    self,\n    *,\n    d_in: int,\n    d_layers: list[int],\n    dropouts: float | list[float],\n    activation: str | Callable[[], nn.Module],\n    d_out: int,\n) -&gt; None:\n    \"\"\"\n    Note:\n        `make_baseline` is the recommended constructor.\n    \"\"\"\n    super().__init__()\n    if isinstance(dropouts, float):\n        dropouts = [dropouts] * len(d_layers)\n    assert len(d_layers) == len(dropouts)\n    assert activation not in [\"ReGLU\", \"GEGLU\"]\n\n    self.blocks = nn.ModuleList(\n        [\n            MLP.Block(\n                d_in=d_layers[i - 1] if i else d_in,\n                d_out=d,\n                bias=True,\n                activation=activation,\n                dropout=dropout,\n            )\n            for i, (d, dropout) in enumerate(zip(d_layers, dropouts))\n        ]\n    )\n    self.head = nn.Linear(d_layers[-1] if d_layers else d_in, d_out)\n</code></pre> <code></code> make_baseline <code>classmethod</code> \u00b6 <pre><code>make_baseline(d_in, d_layers, dropout, d_out)\n</code></pre> <p>Create a \"baseline\" <code>MLP</code>.</p> <p>This variation of MLP was used in [gorishniy2021revisiting]. Features:</p> <ul> <li>all linear layers except for the first one and the last one are of the same dimension</li> <li>the dropout rate is the same for all dropout layers</li> </ul> <p>Parameters:</p> Name Type Description Default <code>d_in</code> <code>int</code> <p>the input size</p> required <code>d_layers</code> <code>list[int]</code> <p>the dimensions of the linear layers. If there are more than two layers, then all of them except for the first and the last ones must have the same dimension.</p> required <code>dropout</code> <code>float</code> <p>the dropout rate for all hidden layers</p> required <code>d_out</code> <code>int</code> <p>the output size</p> required <p>Returns:     MLP</p> References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>@classmethod\ndef make_baseline(\n    cls,\n    d_in: int,\n    d_layers: list[int],\n    dropout: float,\n    d_out: int,\n) -&gt; Self:\n    \"\"\"Create a \"baseline\" `MLP`.\n\n    This variation of MLP was used in [gorishniy2021revisiting]. Features:\n\n    * all linear layers except for the first one and the last one are of the same dimension\n    * the dropout rate is the same for all dropout layers\n\n    Args:\n        d_in: the input size\n        d_layers: the dimensions of the linear layers. If there are more than two\n            layers, then all of them except for the first and the last ones must\n            have the same dimension.\n        dropout: the dropout rate for all hidden layers\n        d_out: the output size\n    Returns:\n        MLP\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n    assert isinstance(dropout, float)\n    if len(d_layers) &gt; 2:\n        assert len(set(d_layers[1:-1])) == 1, (\n            \"if d_layers contains more than two elements, then\"\n            \" all elements except for the first and the last ones must be equal.\"\n        )\n    return cls(\n        d_in=d_in,\n        d_layers=d_layers,\n        dropouts=dropout,\n        activation=\"ReLU\",\n        d_out=d_out,\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ResNet","title":"ResNet","text":"<p>               Bases: <code>Module</code></p> <p>The ResNet model used in [gorishniy2021revisiting].</p> <p>The following scheme describes the architecture: .. code-block:: text     ResNet: (in) -&gt; Linear -&gt; Block -&gt; ... -&gt; Block -&gt; Head -&gt; (out)              |-&gt; Norm -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; Linear -&gt; Dropout -&gt;|              |                                                                  |      Block: (in) ------------------------------------------------------------&gt; Add -&gt; (out)       Head: (in) -&gt; Norm -&gt; Activation -&gt; Linear -&gt; (out)</p> <p>Examples:</p> <p>.. testcode::     x = torch.randn(4, 2)     module = ResNet.make_baseline(         d_in=x.shape[1],         n_blocks=2,         d_main=3,         d_hidden=4,         dropout_first=0.25,         dropout_second=0.0,         d_out=1     )     assert module(x).shape == (len(x), 1)</p> References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class ResNet(nn.Module):\n    \"\"\"\n    The ResNet model used in [gorishniy2021revisiting].\n\n    The following scheme describes the architecture:\n    .. code-block:: text\n        ResNet: (in) -&gt; Linear -&gt; Block -&gt; ... -&gt; Block -&gt; Head -&gt; (out)\n                 |-&gt; Norm -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; Linear -&gt; Dropout -&gt;|\n                 |                                                                  |\n         Block: (in) ------------------------------------------------------------&gt; Add -&gt; (out)\n          Head: (in) -&gt; Norm -&gt; Activation -&gt; Linear -&gt; (out)\n\n    Examples:\n        .. testcode::\n            x = torch.randn(4, 2)\n            module = ResNet.make_baseline(\n                d_in=x.shape[1],\n                n_blocks=2,\n                d_main=3,\n                d_hidden=4,\n                dropout_first=0.25,\n                dropout_second=0.0,\n                d_out=1\n            )\n            assert module(x).shape == (len(x), 1)\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n\n    class Block(nn.Module):\n        \"\"\"The main building block of `ResNet`.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            d_main: int,\n            d_hidden: int,\n            bias_first: bool,\n            bias_second: bool,\n            dropout_first: float,\n            dropout_second: float,\n            normalization: ModuleType,\n            activation: ModuleType,\n            skip_connection: bool,\n        ) -&gt; None:\n            super().__init__()\n            self.normalization = _make_nn_module(normalization, d_main)\n            self.linear_first = nn.Linear(d_main, d_hidden, bias_first)\n            self.activation = _make_nn_module(activation)\n            self.dropout_first = nn.Dropout(dropout_first)\n            self.linear_second = nn.Linear(d_hidden, d_main, bias_second)\n            self.dropout_second = nn.Dropout(dropout_second)\n            self.skip_connection = skip_connection\n\n        def forward(self, x: Tensor) -&gt; Tensor:\n            x_input = x\n            x = self.normalization(x)\n            x = self.linear_first(x)\n            x = self.activation(x)\n            x = self.dropout_first(x)\n            x = self.linear_second(x)\n            x = self.dropout_second(x)\n            if self.skip_connection:\n                x = x_input + x\n            return x\n\n    class Head(nn.Module):\n        \"\"\"The final module of `ResNet`.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            d_in: int,\n            d_out: int,\n            bias: bool,\n            normalization: ModuleType,\n            activation: ModuleType,\n        ) -&gt; None:\n            super().__init__()\n            self.normalization = _make_nn_module(normalization, d_in)\n            self.activation = _make_nn_module(activation)\n            self.linear = nn.Linear(d_in, d_out, bias)\n\n        def forward(self, x: Tensor) -&gt; Tensor:\n            if self.normalization is not None:\n                x = self.normalization(x)\n            x = self.activation(x)\n            return self.linear(x)\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        n_blocks: int,\n        d_main: int,\n        d_hidden: int,\n        dropout_first: float,\n        dropout_second: float,\n        normalization: ModuleType,\n        activation: ModuleType,\n        d_out: int,\n    ) -&gt; None:\n        \"\"\"\n        Note:\n            `make_baseline` is the recommended constructor.\n        \"\"\"\n        super().__init__()\n\n        self.first_layer = nn.Linear(d_in, d_main)\n        if d_main is None:\n            d_main = d_in\n        self.blocks = nn.Sequential(\n            *[\n                ResNet.Block(\n                    d_main=d_main,\n                    d_hidden=d_hidden,\n                    bias_first=True,\n                    bias_second=True,\n                    dropout_first=dropout_first,\n                    dropout_second=dropout_second,\n                    normalization=normalization,\n                    activation=activation,\n                    skip_connection=True,\n                )\n                for _ in range(n_blocks)\n            ]\n        )\n        self.head = ResNet.Head(\n            d_in=d_main,\n            d_out=d_out,\n            bias=True,\n            normalization=normalization,\n            activation=activation,\n        )\n\n    @classmethod\n    def make_baseline(\n        cls,\n        *,\n        d_in: int,\n        n_blocks: int,\n        d_main: int,\n        d_hidden: int,\n        dropout_first: float,\n        dropout_second: float,\n        d_out: int,\n    ) -&gt; Self:\n        \"\"\"\n        Create a \"baseline\" `ResNet`. This variation of ResNet was used in [gorishniy2021revisiting].\n\n        Args:\n            d_in: the input size\n            n_blocks: the number of Blocks\n            d_main: the input size (or, equivalently, the output size) of each Block\n            d_hidden: the output size of the first linear layer in each Block\n            dropout_first: the dropout rate of the first dropout layer in each Block.\n            dropout_second: the dropout rate of the second dropout layer in each Block.\n            d_out: Output dimension.\n\n        References:\n            * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n            Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n        \"\"\"\n        return cls(\n            d_in=d_in,\n            n_blocks=n_blocks,\n            d_main=d_main,\n            d_hidden=d_hidden,\n            dropout_first=dropout_first,\n            dropout_second=dropout_second,\n            normalization=\"BatchNorm1d\",\n            activation=\"ReLU\",\n            d_out=d_out,\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = x.float()\n        x = self.first_layer(x)\n        x = self.blocks(x)\n        return self.head(x)\n</code></pre> <code></code> Block \u00b6 <p>               Bases: <code>Module</code></p> <p>The main building block of <code>ResNet</code>.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class Block(nn.Module):\n    \"\"\"The main building block of `ResNet`.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        d_main: int,\n        d_hidden: int,\n        bias_first: bool,\n        bias_second: bool,\n        dropout_first: float,\n        dropout_second: float,\n        normalization: ModuleType,\n        activation: ModuleType,\n        skip_connection: bool,\n    ) -&gt; None:\n        super().__init__()\n        self.normalization = _make_nn_module(normalization, d_main)\n        self.linear_first = nn.Linear(d_main, d_hidden, bias_first)\n        self.activation = _make_nn_module(activation)\n        self.dropout_first = nn.Dropout(dropout_first)\n        self.linear_second = nn.Linear(d_hidden, d_main, bias_second)\n        self.dropout_second = nn.Dropout(dropout_second)\n        self.skip_connection = skip_connection\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x_input = x\n        x = self.normalization(x)\n        x = self.linear_first(x)\n        x = self.activation(x)\n        x = self.dropout_first(x)\n        x = self.linear_second(x)\n        x = self.dropout_second(x)\n        if self.skip_connection:\n            x = x_input + x\n        return x\n</code></pre> <code></code> Head \u00b6 <p>               Bases: <code>Module</code></p> <p>The final module of <code>ResNet</code>.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class Head(nn.Module):\n    \"\"\"The final module of `ResNet`.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        d_out: int,\n        bias: bool,\n        normalization: ModuleType,\n        activation: ModuleType,\n    ) -&gt; None:\n        super().__init__()\n        self.normalization = _make_nn_module(normalization, d_in)\n        self.activation = _make_nn_module(activation)\n        self.linear = nn.Linear(d_in, d_out, bias)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        if self.normalization is not None:\n            x = self.normalization(x)\n        x = self.activation(x)\n        return self.linear(x)\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(\n    *,\n    d_in,\n    n_blocks,\n    d_main,\n    d_hidden,\n    dropout_first,\n    dropout_second,\n    normalization,\n    activation,\n    d_out,\n)\n</code></pre> Note <p><code>make_baseline</code> is the recommended constructor.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def __init__(\n    self,\n    *,\n    d_in: int,\n    n_blocks: int,\n    d_main: int,\n    d_hidden: int,\n    dropout_first: float,\n    dropout_second: float,\n    normalization: ModuleType,\n    activation: ModuleType,\n    d_out: int,\n) -&gt; None:\n    \"\"\"\n    Note:\n        `make_baseline` is the recommended constructor.\n    \"\"\"\n    super().__init__()\n\n    self.first_layer = nn.Linear(d_in, d_main)\n    if d_main is None:\n        d_main = d_in\n    self.blocks = nn.Sequential(\n        *[\n            ResNet.Block(\n                d_main=d_main,\n                d_hidden=d_hidden,\n                bias_first=True,\n                bias_second=True,\n                dropout_first=dropout_first,\n                dropout_second=dropout_second,\n                normalization=normalization,\n                activation=activation,\n                skip_connection=True,\n            )\n            for _ in range(n_blocks)\n        ]\n    )\n    self.head = ResNet.Head(\n        d_in=d_main,\n        d_out=d_out,\n        bias=True,\n        normalization=normalization,\n        activation=activation,\n    )\n</code></pre> <code></code> make_baseline <code>classmethod</code> \u00b6 <pre><code>make_baseline(\n    *,\n    d_in,\n    n_blocks,\n    d_main,\n    d_hidden,\n    dropout_first,\n    dropout_second,\n    d_out,\n)\n</code></pre> <p>Create a \"baseline\" <code>ResNet</code>. This variation of ResNet was used in [gorishniy2021revisiting].</p> <p>Parameters:</p> Name Type Description Default <code>d_in</code> <code>int</code> <p>the input size</p> required <code>n_blocks</code> <code>int</code> <p>the number of Blocks</p> required <code>d_main</code> <code>int</code> <p>the input size (or, equivalently, the output size) of each Block</p> required <code>d_hidden</code> <code>int</code> <p>the output size of the first linear layer in each Block</p> required <code>dropout_first</code> <code>float</code> <p>the dropout rate of the first dropout layer in each Block.</p> required <code>dropout_second</code> <code>float</code> <p>the dropout rate of the second dropout layer in each Block.</p> required <code>d_out</code> <code>int</code> <p>Output dimension.</p> required References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>@classmethod\ndef make_baseline(\n    cls,\n    *,\n    d_in: int,\n    n_blocks: int,\n    d_main: int,\n    d_hidden: int,\n    dropout_first: float,\n    dropout_second: float,\n    d_out: int,\n) -&gt; Self:\n    \"\"\"\n    Create a \"baseline\" `ResNet`. This variation of ResNet was used in [gorishniy2021revisiting].\n\n    Args:\n        d_in: the input size\n        n_blocks: the number of Blocks\n        d_main: the input size (or, equivalently, the output size) of each Block\n        d_hidden: the output size of the first linear layer in each Block\n        dropout_first: the dropout rate of the first dropout layer in each Block.\n        dropout_second: the dropout rate of the second dropout layer in each Block.\n        d_out: Output dimension.\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n    return cls(\n        d_in=d_in,\n        n_blocks=n_blocks,\n        d_main=d_main,\n        d_hidden=d_hidden,\n        dropout_first=dropout_first,\n        dropout_second=dropout_second,\n        normalization=\"BatchNorm1d\",\n        activation=\"ReLU\",\n        d_out=d_out,\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ReGLU","title":"ReGLU","text":"<p>               Bases: <code>Module</code></p> <p>The ReGLU activation function from [shazeer2020glu].</p> <p>Examples:</p> <p>.. testcode::</p> <pre><code>module = ReGLU()\nx = torch.randn(3, 4)\nassert module(x).shape == (3, 2)\n</code></pre> References <ul> <li>[shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class ReGLU(nn.Module):\n    \"\"\"The ReGLU activation function from [shazeer2020glu].\n\n    Examples:\n        .. testcode::\n\n            module = ReGLU()\n            x = torch.randn(3, 4)\n            assert module(x).shape == (3, 2)\n\n    References:\n        * [shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        # ruff: noqa: D102\n        return reglu(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.GEGLU","title":"GEGLU","text":"<p>               Bases: <code>Module</code></p> <p>The GEGLU activation function from [shazeer2020glu].</p> <p>Examples:</p> <p>.. testcode::</p> <pre><code>module = GEGLU()\nx = torch.randn(3, 4)\nassert module(x).shape == (3, 2)\n</code></pre> References <ul> <li>[shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class GEGLU(nn.Module):\n    \"\"\"The GEGLU activation function from [shazeer2020glu].\n\n    Examples:\n        .. testcode::\n\n            module = GEGLU()\n            x = torch.randn(3, 4)\n            assert module(x).shape == (3, 2)\n\n    References:\n        * [shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        # ruff: noqa: D102\n        return geglu(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.make_dataset_from_df","title":"make_dataset_from_df","text":"<pre><code>make_dataset_from_df(\n    df, T, is_y_cond, df_info, ratios=None, std=0\n)\n</code></pre> <p>The order of the generated dataset: (y, X_num, X_cat).</p> is_y_cond <p>concat: y is concatenated to X, the model learn a joint distribution of (y, X) embedding: y is not concatenated to X. During computations, y is embedded     and added to the latent vector of X none: y column is completely ignored</p> <p>How does is_y_cond affect the generation of y? is_y_cond:     concat: the model synthesizes (y, X) directly, so y is just the first column     embedding: y is first sampled using empirical distribution of y. The model only         synthesizes X. When returning the generated data, we return the generated X         and the sampled y. (y is sampled from empirical distribution, instead of being         generated by the model)         Note that in this way, y is still not independent of X, because the model has been         adding the embedding of y to the latent vector of X during computations.     none:         y is synthesized using y's empirical distribution. X is generated by the model.         In this case, y is completely independent of X.</p> <p>Note: For now, n_classes has to be set to 0. This is because our matrix is the concatenation of (X_num, X_cat). In this case, if we have is_y_cond == 'concat', we can guarantee that y is the first column of the matrix. However, if we have n_classes &gt; 0, then y is not the first column of the matrix.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def make_dataset_from_df(\n    # ruff: noqa: PLR0915, PLR0912\n    df: pd.DataFrame,\n    T: Transformations,\n    is_y_cond: str,\n    df_info: pd.DataFrame,\n    ratios: list[float] | None = None,\n    std: float = 0,\n) -&gt; tuple[Dataset, dict[int, LabelEncoder], list[int]]:\n    \"\"\"\n    The order of the generated dataset: (y, X_num, X_cat).\n\n    is_y_cond:\n        concat: y is concatenated to X, the model learn a joint distribution of (y, X)\n        embedding: y is not concatenated to X. During computations, y is embedded\n            and added to the latent vector of X\n        none: y column is completely ignored\n\n    How does is_y_cond affect the generation of y?\n    is_y_cond:\n        concat: the model synthesizes (y, X) directly, so y is just the first column\n        embedding: y is first sampled using empirical distribution of y. The model only\n            synthesizes X. When returning the generated data, we return the generated X\n            and the sampled y. (y is sampled from empirical distribution, instead of being\n            generated by the model)\n            Note that in this way, y is still not independent of X, because the model has been\n            adding the embedding of y to the latent vector of X during computations.\n        none:\n            y is synthesized using y's empirical distribution. X is generated by the model.\n            In this case, y is completely independent of X.\n\n    Note: For now, n_classes has to be set to 0. This is because our matrix is the concatenation\n    of (X_num, X_cat). In this case, if we have is_y_cond == 'concat', we can guarantee that y\n    is the first column of the matrix.\n    However, if we have n_classes &gt; 0, then y is not the first column of the matrix.\n    \"\"\"\n    if ratios is None:\n        ratios = [0.7, 0.2, 0.1]\n\n    train_val_df, test_df = train_test_split(df, test_size=ratios[2], random_state=42)\n    train_df, val_df = train_test_split(train_val_df, test_size=ratios[1] / (ratios[0] + ratios[1]), random_state=42)\n\n    cat_column_orders = []\n    num_column_orders = []\n    index_to_column = list(df.columns)\n    column_to_index = {col: i for i, col in enumerate(index_to_column)}\n\n    if df_info[\"n_classes\"] &gt; 0:\n        X_cat: dict[str, np.ndarray] | None = {} if df_info[\"cat_cols\"] is not None or is_y_cond == \"concat\" else None\n        X_num: dict[str, np.ndarray] | None = {} if df_info[\"num_cols\"] is not None else None\n        y = {}\n\n        cat_cols_with_y = []\n        if df_info[\"cat_cols\"] is not None:\n            cat_cols_with_y += df_info[\"cat_cols\"]\n        if is_y_cond == \"concat\":\n            cat_cols_with_y = [df_info[\"y_col\"]] + cat_cols_with_y\n\n        if len(cat_cols_with_y) &gt; 0:\n            X_cat[\"train\"] = train_df[cat_cols_with_y].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"val\"] = val_df[cat_cols_with_y].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"test\"] = test_df[cat_cols_with_y].to_numpy(dtype=np.str_)  # type: ignore[index]\n\n        y[\"train\"] = train_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"val\"] = val_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"test\"] = test_df[df_info[\"y_col\"]].values.astype(np.float32)\n\n        if df_info[\"num_cols\"] is not None:\n            X_num[\"train\"] = train_df[df_info[\"num_cols\"]].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"val\"] = val_df[df_info[\"num_cols\"]].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"test\"] = test_df[df_info[\"num_cols\"]].values.astype(np.float32)  # type: ignore[index]\n\n        cat_column_orders = [column_to_index[col] for col in cat_cols_with_y]\n        num_column_orders = [column_to_index[col] for col in df_info[\"num_cols\"]]\n\n    else:\n        X_cat = {} if df_info[\"cat_cols\"] is not None else None\n        X_num = {} if df_info[\"num_cols\"] is not None or is_y_cond == \"concat\" else None\n        y = {}\n\n        num_cols_with_y = []\n        if df_info[\"num_cols\"] is not None:\n            num_cols_with_y += df_info[\"num_cols\"]\n        if is_y_cond == \"concat\":\n            num_cols_with_y = [df_info[\"y_col\"]] + num_cols_with_y\n\n        if len(num_cols_with_y) &gt; 0:\n            X_num[\"train\"] = train_df[num_cols_with_y].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"val\"] = val_df[num_cols_with_y].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"test\"] = test_df[num_cols_with_y].values.astype(np.float32)  # type: ignore[index]\n\n        y[\"train\"] = train_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"val\"] = val_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"test\"] = test_df[df_info[\"y_col\"]].values.astype(np.float32)\n\n        if df_info[\"cat_cols\"] is not None:\n            X_cat[\"train\"] = train_df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"val\"] = val_df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"test\"] = test_df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)  # type: ignore[index]\n\n        cat_column_orders = [column_to_index[col] for col in df_info[\"cat_cols\"]]\n        num_column_orders = [column_to_index[col] for col in num_cols_with_y]\n\n    column_orders = num_column_orders + cat_column_orders\n    column_orders = [index_to_column[index] for index in column_orders]\n\n    label_encoders = {}\n    if X_cat is not None and len(df_info[\"cat_cols\"]) &gt; 0:\n        X_cat_all = np.vstack((X_cat[\"train\"], X_cat[\"val\"], X_cat[\"test\"]))\n        X_cat_converted = []\n        for col_index in range(X_cat_all.shape[1]):\n            label_encoder = LabelEncoder()\n            X_cat_converted.append(label_encoder.fit_transform(X_cat_all[:, col_index]).astype(float))\n            if std &gt; 0:\n                # add noise\n                X_cat_converted[-1] += np.random.normal(0, std, X_cat_converted[-1].shape)\n            label_encoders[col_index] = label_encoder\n\n        X_cat_converted = np.vstack(X_cat_converted).T  # type: ignore[assignment]\n\n        train_num = X_cat[\"train\"].shape[0]\n        val_num = X_cat[\"val\"].shape[0]\n        # test_num = X_cat[\"test\"].shape[0]\n\n        X_cat[\"train\"] = X_cat_converted[:train_num, :]  # type: ignore[call-overload]\n        X_cat[\"val\"] = X_cat_converted[train_num : train_num + val_num, :]  # type: ignore[call-overload]\n        X_cat[\"test\"] = X_cat_converted[train_num + val_num :, :]  # type: ignore[call-overload]\n\n        if X_num and len(X_num) &gt; 0:\n            X_num[\"train\"] = np.concatenate((X_num[\"train\"], X_cat[\"train\"]), axis=1)\n            X_num[\"val\"] = np.concatenate((X_num[\"val\"], X_cat[\"val\"]), axis=1)\n            X_num[\"test\"] = np.concatenate((X_num[\"test\"], X_cat[\"test\"]), axis=1)\n        else:\n            X_num = X_cat\n            X_cat = None\n\n    D = Dataset(\n        # ruff: noqa: N806\n        X_num,\n        None,\n        y,\n        y_info={},\n        task_type=TaskType(df_info[\"task_type\"]),\n        n_classes=df_info[\"n_classes\"],\n    )\n\n    return transform_dataset(D, T, None), label_encoders, column_orders\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.timestep_embedding","title":"timestep_embedding","text":"<pre><code>timestep_embedding(timesteps, dim, max_period=10000)\n</code></pre> <p>Create sinusoidal timestep embeddings.</p> <p>:param timesteps: a 1-D Tensor of N indices, one per batch element.                   These may be fractional. :param dim: the dimension of the output. :param max_period: controls the minimum frequency of the embeddings. :return: an [N x dim] Tensor of positional embeddings.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def timestep_embedding(timesteps: Tensor, dim: int, max_period: int = 10000) -&gt; Tensor:\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2\n    freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(\n        device=timesteps.device\n    )\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.reglu","title":"reglu","text":"<pre><code>reglu(x)\n</code></pre> <p>The ReGLU activation function from [1].</p> References <p>[1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def reglu(x: Tensor) -&gt; Tensor:\n    \"\"\"The ReGLU activation function from [1].\n\n    References:\n        [1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n    assert x.shape[-1] % 2 == 0\n    a, b = x.chunk(2, dim=-1)\n    return a * F.relu(b)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.geglu","title":"geglu","text":"<pre><code>geglu(x)\n</code></pre> <p>The GEGLU activation function from [1].</p> References <p>[1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def geglu(x: Tensor) -&gt; Tensor:\n    \"\"\"The GEGLU activation function from [1].\n\n    References:\n        [1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n    assert x.shape[-1] % 2 == 0\n    a, b = x.chunk(2, dim=-1)\n    return a * F.gelu(b)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.sampler","title":"sampler","text":"<p>Samplers for the ClavaDDPM model.</p>"},{"location":"api/#midst_toolkit.models.clavaddpm.sampler.ScheduleSampler","title":"ScheduleSampler","text":"<p>               Bases: <code>ABC</code></p> <p>A distribution over timesteps in the diffusion process, intended to reduce variance of the objective.</p> <p>By default, samplers perform unbiased importance sampling, in which the objective's mean is unchanged. However, subclasses may override sample() to change how the resampled terms are reweighted, allowing for actual changes in the objective.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/sampler.py</code> <pre><code>class ScheduleSampler(ABC):\n    \"\"\"\n    A distribution over timesteps in the diffusion process, intended to reduce\n    variance of the objective.\n\n    By default, samplers perform unbiased importance sampling, in which the\n    objective's mean is unchanged. However, subclasses may override sample() to\n    change how the resampled terms are reweighted, allowing for actual changes\n    in the objective.\n    \"\"\"\n\n    @abstractmethod\n    def weights(self) -&gt; Tensor:\n        \"\"\"\n        Get a numpy array of weights, one per diffusion step.\n\n        The weights needn't be normalized, but must be positive.\n        \"\"\"\n\n    def sample(self, batch_size: int, device: str) -&gt; tuple[Tensor, Tensor]:\n        # TODO: what's happening with batch_size? Is is also the number of timesteps?\n        # We need to clarify this.\n        \"\"\"\n        Importance-sample timesteps for a batch.\n\n        Args:\n            batch_size: The number of timesteps.\n            device: The torch device to save to.\n\n        Returns:\n            A tuple (timesteps, weights):\n                - timesteps: a tensor of timestep indices.\n                - weights: a tensor of weights to scale the resulting losses.\n        \"\"\"\n        w = self.weights().cpu().numpy()\n        p = w / np.sum(w)\n        indices_np = np.random.choice(len(p), size=(batch_size,), p=p)\n        indices = torch.from_numpy(indices_np).long().to(device)\n        weights_np = 1 / (len(p) * p[indices_np])\n        weights = torch.from_numpy(weights_np).float().to(device)\n        return indices, weights\n</code></pre> <code></code> weights <code>abstractmethod</code> \u00b6 <pre><code>weights()\n</code></pre> <p>Get a numpy array of weights, one per diffusion step.</p> <p>The weights needn't be normalized, but must be positive.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/sampler.py</code> <pre><code>@abstractmethod\ndef weights(self) -&gt; Tensor:\n    \"\"\"\n    Get a numpy array of weights, one per diffusion step.\n\n    The weights needn't be normalized, but must be positive.\n    \"\"\"\n</code></pre> <code></code> sample \u00b6 <pre><code>sample(batch_size, device)\n</code></pre> <p>Importance-sample timesteps for a batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The number of timesteps.</p> required <code>device</code> <code>str</code> <p>The torch device to save to.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>A tuple (timesteps, weights): - timesteps: a tensor of timestep indices. - weights: a tensor of weights to scale the resulting losses.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/sampler.py</code> <pre><code>def sample(self, batch_size: int, device: str) -&gt; tuple[Tensor, Tensor]:\n    # TODO: what's happening with batch_size? Is is also the number of timesteps?\n    # We need to clarify this.\n    \"\"\"\n    Importance-sample timesteps for a batch.\n\n    Args:\n        batch_size: The number of timesteps.\n        device: The torch device to save to.\n\n    Returns:\n        A tuple (timesteps, weights):\n            - timesteps: a tensor of timestep indices.\n            - weights: a tensor of weights to scale the resulting losses.\n    \"\"\"\n    w = self.weights().cpu().numpy()\n    p = w / np.sum(w)\n    indices_np = np.random.choice(len(p), size=(batch_size,), p=p)\n    indices = torch.from_numpy(indices_np).long().to(device)\n    weights_np = 1 / (len(p) * p[indices_np])\n    weights = torch.from_numpy(weights_np).float().to(device)\n    return indices, weights\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.sampler.UniformSampler","title":"UniformSampler","text":"<p>               Bases: <code>ScheduleSampler</code></p> Source code in <code>src/midst_toolkit/models/clavaddpm/sampler.py</code> <pre><code>class UniformSampler(ScheduleSampler):\n    def __init__(self, diffusion: GaussianMultinomialDiffusion):\n        \"\"\"\n        Initialize the UniformSampler.\n\n        Args:\n            diffusion: The diffusion object.\n        \"\"\"\n        self.diffusion = diffusion\n        self._weights = torch.from_numpy(np.ones([diffusion.num_timesteps]))\n\n    def weights(self) -&gt; Tensor:\n        \"\"\"Return the weights.\"\"\"\n        return self._weights\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(diffusion)\n</code></pre> <p>Initialize the UniformSampler.</p> <p>Parameters:</p> Name Type Description Default <code>diffusion</code> <code>GaussianMultinomialDiffusion</code> <p>The diffusion object.</p> required Source code in <code>src/midst_toolkit/models/clavaddpm/sampler.py</code> <pre><code>def __init__(self, diffusion: GaussianMultinomialDiffusion):\n    \"\"\"\n    Initialize the UniformSampler.\n\n    Args:\n        diffusion: The diffusion object.\n    \"\"\"\n    self.diffusion = diffusion\n    self._weights = torch.from_numpy(np.ones([diffusion.num_timesteps]))\n</code></pre> <code></code> weights \u00b6 <pre><code>weights()\n</code></pre> <p>Return the weights.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/sampler.py</code> <pre><code>def weights(self) -&gt; Tensor:\n    \"\"\"Return the weights.\"\"\"\n    return self._weights\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.sampler.LossAwareSampler","title":"LossAwareSampler","text":"<p>               Bases: <code>ScheduleSampler</code></p> Source code in <code>src/midst_toolkit/models/clavaddpm/sampler.py</code> <pre><code>class LossAwareSampler(ScheduleSampler):\n    def update_with_local_losses(self, local_ts: Tensor, local_losses: Tensor) -&gt; None:\n        \"\"\"\n        Update the reweighting using losses from a model.\n\n        Call this method from each rank with a batch of timesteps and the\n        corresponding losses for each of those timesteps.\n        This method will perform synchronization to make sure all of the ranks\n        maintain the exact same reweighting.\n\n        Args:\n            local_ts: An integer Tensor of timesteps.\n            local_losses: A 1D Tensor of losses.\n        \"\"\"\n        batch_sizes = [\n            torch.tensor([0], dtype=torch.int32, device=local_ts.device)\n            for _ in range(torch.distributed.get_world_size())\n        ]\n        torch.distributed.all_gather(\n            batch_sizes,\n            torch.tensor([len(local_ts)], dtype=torch.int32, device=local_ts.device),\n        )\n\n        # Pad all_gather batches to be the maximum batch size.\n        max_bs = max([int(x.item()) for x in batch_sizes])\n\n        timestep_batches = [torch.zeros(max_bs).to(local_ts) for bs in batch_sizes]\n        loss_batches = [torch.zeros(max_bs).to(local_losses) for bs in batch_sizes]\n        torch.distributed.all_gather(timestep_batches, local_ts)\n        torch.distributed.all_gather(loss_batches, local_losses)\n        timesteps = [x.item() for y, bs in zip(timestep_batches, batch_sizes) for x in y[:bs]]\n        losses = [x.item() for y, bs in zip(loss_batches, batch_sizes) for x in y[:bs]]\n        self.update_with_all_losses(timesteps, losses)\n\n    @abstractmethod\n    def update_with_all_losses(self, ts: list[int], losses: list[float]) -&gt; None:\n        \"\"\"\n        Update the reweighting using losses from a model.\n\n        Sub-classes should override this method to update the reweighting\n        using losses from the model.\n\n        This method directly updates the reweighting without synchronizing\n        between workers. It is called by update_with_local_losses from all\n        ranks with identical arguments. Thus, it should have deterministic\n        behavior to maintain state across workers.\n\n        Args:\n            ts: A list of int timesteps.\n            losses: A list of float losses, one per timestep.\n        \"\"\"\n</code></pre> <code></code> update_with_local_losses \u00b6 <pre><code>update_with_local_losses(local_ts, local_losses)\n</code></pre> <p>Update the reweighting using losses from a model.</p> <p>Call this method from each rank with a batch of timesteps and the corresponding losses for each of those timesteps. This method will perform synchronization to make sure all of the ranks maintain the exact same reweighting.</p> <p>Parameters:</p> Name Type Description Default <code>local_ts</code> <code>Tensor</code> <p>An integer Tensor of timesteps.</p> required <code>local_losses</code> <code>Tensor</code> <p>A 1D Tensor of losses.</p> required Source code in <code>src/midst_toolkit/models/clavaddpm/sampler.py</code> <pre><code>def update_with_local_losses(self, local_ts: Tensor, local_losses: Tensor) -&gt; None:\n    \"\"\"\n    Update the reweighting using losses from a model.\n\n    Call this method from each rank with a batch of timesteps and the\n    corresponding losses for each of those timesteps.\n    This method will perform synchronization to make sure all of the ranks\n    maintain the exact same reweighting.\n\n    Args:\n        local_ts: An integer Tensor of timesteps.\n        local_losses: A 1D Tensor of losses.\n    \"\"\"\n    batch_sizes = [\n        torch.tensor([0], dtype=torch.int32, device=local_ts.device)\n        for _ in range(torch.distributed.get_world_size())\n    ]\n    torch.distributed.all_gather(\n        batch_sizes,\n        torch.tensor([len(local_ts)], dtype=torch.int32, device=local_ts.device),\n    )\n\n    # Pad all_gather batches to be the maximum batch size.\n    max_bs = max([int(x.item()) for x in batch_sizes])\n\n    timestep_batches = [torch.zeros(max_bs).to(local_ts) for bs in batch_sizes]\n    loss_batches = [torch.zeros(max_bs).to(local_losses) for bs in batch_sizes]\n    torch.distributed.all_gather(timestep_batches, local_ts)\n    torch.distributed.all_gather(loss_batches, local_losses)\n    timesteps = [x.item() for y, bs in zip(timestep_batches, batch_sizes) for x in y[:bs]]\n    losses = [x.item() for y, bs in zip(loss_batches, batch_sizes) for x in y[:bs]]\n    self.update_with_all_losses(timesteps, losses)\n</code></pre> <code></code> update_with_all_losses <code>abstractmethod</code> \u00b6 <pre><code>update_with_all_losses(ts, losses)\n</code></pre> <p>Update the reweighting using losses from a model.</p> <p>Sub-classes should override this method to update the reweighting using losses from the model.</p> <p>This method directly updates the reweighting without synchronizing between workers. It is called by update_with_local_losses from all ranks with identical arguments. Thus, it should have deterministic behavior to maintain state across workers.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>list[int]</code> <p>A list of int timesteps.</p> required <code>losses</code> <code>list[float]</code> <p>A list of float losses, one per timestep.</p> required Source code in <code>src/midst_toolkit/models/clavaddpm/sampler.py</code> <pre><code>@abstractmethod\ndef update_with_all_losses(self, ts: list[int], losses: list[float]) -&gt; None:\n    \"\"\"\n    Update the reweighting using losses from a model.\n\n    Sub-classes should override this method to update the reweighting\n    using losses from the model.\n\n    This method directly updates the reweighting without synchronizing\n    between workers. It is called by update_with_local_losses from all\n    ranks with identical arguments. Thus, it should have deterministic\n    behavior to maintain state across workers.\n\n    Args:\n        ts: A list of int timesteps.\n        losses: A list of float losses, one per timestep.\n    \"\"\"\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.sampler.LossSecondMomentResampler","title":"LossSecondMomentResampler","text":"<p>               Bases: <code>LossAwareSampler</code></p> Source code in <code>src/midst_toolkit/models/clavaddpm/sampler.py</code> <pre><code>class LossSecondMomentResampler(LossAwareSampler):\n    def __init__(\n        self,\n        diffusion: GaussianMultinomialDiffusion,\n        history_per_term: int = 10,\n        uniform_prob: float = 0.001,\n    ):\n        \"\"\"\n        Initialize the LossSecondMomentResampler.\n\n        Args:\n            diffusion: The diffusion object.\n            history_per_term: The number of losses to keep for each timestep.\n            uniform_prob: The probability of sampling a uniform timestep.\n        \"\"\"\n        self.diffusion = diffusion\n        self.history_per_term = history_per_term\n        self.uniform_prob = uniform_prob\n        self._loss_history = np.zeros([diffusion.num_timesteps, history_per_term], dtype=np.float64)\n        self._loss_counts = np.zeros([diffusion.num_timesteps], dtype=np.uint)\n\n    def weights(self):\n        \"\"\"\n        Return the weights.\n\n        Warms up the sampler if it's not warmed up.\n        \"\"\"\n        if not self._warmed_up():\n            return np.ones([self.diffusion.num_timesteps], dtype=np.float64)\n        weights = np.sqrt(np.mean(self._loss_history**2, axis=-1))\n        weights /= np.sum(weights)\n        weights *= 1 - self.uniform_prob\n        weights += self.uniform_prob / len(weights)\n        return weights\n\n    def update_with_all_losses(self, ts: list[int], losses: list[float]) -&gt; None:\n        \"\"\"\n        Update the reweighting using losses from the model.\n\n        Args:\n            ts: The timesteps.\n            losses: The losses.\n        \"\"\"\n        for t, loss in zip(ts, losses):\n            if self._loss_counts[t] == self.history_per_term:\n                # Shift out the oldest loss term.\n                self._loss_history[t, :-1] = self._loss_history[t, 1:]\n                self._loss_history[t, -1] = loss\n            else:\n                self._loss_history[t, self._loss_counts[t]] = loss\n                self._loss_counts[t] += 1\n\n    def _warmed_up(self) -&gt; bool:\n        \"\"\"\n        Check if the sampler is warmed up by checking if the loss counts are equal\n        to the history per term.\n\n        Returns:\n            True if the sampler is warmed up, False otherwise.\n        \"\"\"\n        return (self._loss_counts == self.history_per_term).all()\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(\n    diffusion, history_per_term=10, uniform_prob=0.001\n)\n</code></pre> <p>Initialize the LossSecondMomentResampler.</p> <p>Parameters:</p> Name Type Description Default <code>diffusion</code> <code>GaussianMultinomialDiffusion</code> <p>The diffusion object.</p> required <code>history_per_term</code> <code>int</code> <p>The number of losses to keep for each timestep.</p> <code>10</code> <code>uniform_prob</code> <code>float</code> <p>The probability of sampling a uniform timestep.</p> <code>0.001</code> Source code in <code>src/midst_toolkit/models/clavaddpm/sampler.py</code> <pre><code>def __init__(\n    self,\n    diffusion: GaussianMultinomialDiffusion,\n    history_per_term: int = 10,\n    uniform_prob: float = 0.001,\n):\n    \"\"\"\n    Initialize the LossSecondMomentResampler.\n\n    Args:\n        diffusion: The diffusion object.\n        history_per_term: The number of losses to keep for each timestep.\n        uniform_prob: The probability of sampling a uniform timestep.\n    \"\"\"\n    self.diffusion = diffusion\n    self.history_per_term = history_per_term\n    self.uniform_prob = uniform_prob\n    self._loss_history = np.zeros([diffusion.num_timesteps, history_per_term], dtype=np.float64)\n    self._loss_counts = np.zeros([diffusion.num_timesteps], dtype=np.uint)\n</code></pre> <code></code> weights \u00b6 <pre><code>weights()\n</code></pre> <p>Return the weights.</p> <p>Warms up the sampler if it's not warmed up.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/sampler.py</code> <pre><code>def weights(self):\n    \"\"\"\n    Return the weights.\n\n    Warms up the sampler if it's not warmed up.\n    \"\"\"\n    if not self._warmed_up():\n        return np.ones([self.diffusion.num_timesteps], dtype=np.float64)\n    weights = np.sqrt(np.mean(self._loss_history**2, axis=-1))\n    weights /= np.sum(weights)\n    weights *= 1 - self.uniform_prob\n    weights += self.uniform_prob / len(weights)\n    return weights\n</code></pre> <code></code> update_with_all_losses \u00b6 <pre><code>update_with_all_losses(ts, losses)\n</code></pre> <p>Update the reweighting using losses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>list[int]</code> <p>The timesteps.</p> required <code>losses</code> <code>list[float]</code> <p>The losses.</p> required Source code in <code>src/midst_toolkit/models/clavaddpm/sampler.py</code> <pre><code>def update_with_all_losses(self, ts: list[int], losses: list[float]) -&gt; None:\n    \"\"\"\n    Update the reweighting using losses from the model.\n\n    Args:\n        ts: The timesteps.\n        losses: The losses.\n    \"\"\"\n    for t, loss in zip(ts, losses):\n        if self._loss_counts[t] == self.history_per_term:\n            # Shift out the oldest loss term.\n            self._loss_history[t, :-1] = self._loss_history[t, 1:]\n            self._loss_history[t, -1] = loss\n        else:\n            self._loss_history[t, self._loss_counts[t]] = loss\n            self._loss_counts[t] += 1\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.sampler.create_named_schedule_sampler","title":"create_named_schedule_sampler","text":"<pre><code>create_named_schedule_sampler(name, diffusion)\n</code></pre> <p>Create a ScheduleSampler from a library of pre-defined samplers.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Literal['uniform', 'loss-second-moment']</code> <p>The name of the sampler. Should be one of [\"uniform\", \"loss-second-moment\"].</p> required <code>diffusion</code> <code>GaussianMultinomialDiffusion</code> <p>The diffusion object to sample for.</p> required <p>Returns:</p> Type Description <code>ScheduleSampler</code> <p>The UniformSampler if <code>name</code> is \"uniform\", LossSecondMomentResampler if <code>name</code></p> <code>ScheduleSampler</code> <p>is \"loss-second-moment\".</p> Source code in <code>src/midst_toolkit/models/clavaddpm/sampler.py</code> <pre><code>def create_named_schedule_sampler(\n    name: Literal[\"uniform\", \"loss-second-moment\"],\n    diffusion: GaussianMultinomialDiffusion,\n) -&gt; ScheduleSampler:\n    \"\"\"\n    Create a ScheduleSampler from a library of pre-defined samplers.\n\n    Args:\n        name: The name of the sampler. Should be one of [\"uniform\", \"loss-second-moment\"].\n        diffusion: The diffusion object to sample for.\n\n    Returns:\n        The UniformSampler if ``name`` is \"uniform\", LossSecondMomentResampler if ``name``\n        is \"loss-second-moment\".\n    \"\"\"\n    if name == \"uniform\":\n        return UniformSampler(diffusion)\n    if name == \"loss-second-moment\":\n        return LossSecondMomentResampler(diffusion)\n    raise NotImplementedError(f\"unknown schedule sampler: {name}\")\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.train","title":"train","text":"<p>Defines the training functions for the ClavaDDPM model.</p>"},{"location":"api/#midst_toolkit.models.clavaddpm.train.clava_training","title":"clava_training","text":"<pre><code>clava_training(\n    tables,\n    relation_order,\n    save_dir,\n    diffusion_config,\n    classifier_config,\n    device=\"cuda\",\n)\n</code></pre> <p>Training function for the ClavaDDPM model.</p> <p>Parameters:</p> Name Type Description Default <code>tables</code> <code>Tables</code> <p>Definition of the tables and their relations. Example: {     \"table1\": {         \"children\": [\"table2\"],         \"parents\": []     },     \"table2\": {         \"children\": [],         \"parents\": [\"table1\"]     } }</p> required <code>relation_order</code> <code>RelationOrder</code> <p>List of tuples of parent and child tables. Example: [(\"table1\", \"table2\"), (\"table1\", \"table3\")]</p> required <code>save_dir</code> <code>Path</code> <p>Directory to save the ClavaDDPM models.</p> required <code>diffusion_config</code> <code>Configs</code> <p>Dictionary of configurations for the diffusion model. The following config keys are required: {     d_layers = list[int],     dropout = float,     iterations = int,     batch_size = int,     model_type = str[\"mlp\" | \"resnet\"],     gaussian_loss_type = str[\"mse\" | \"cross_entropy\"],     num_timesteps = int,     scheduler = str[\"cosine\" | \"linear\"],     lr = float,     weight_decay = float, }</p> required <code>classifier_config</code> <code>Configs | None</code> <p>Dictionary of configurations for the classifier model. Not required for single table training. The following config keys are required for multi-table training: {     iterations = int,     batch_size = int,     d_layers = list[int],     dim_t = int,     lr = float, }</p> required <code>device</code> <code>str</code> <p>Device to use for training. Default is <code>\"cuda\"</code>.</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>tuple[Tables, dict[tuple[str, str], dict[str, Any]]]</code> <p>A tuple with 2 values: - The tables dictionary. - Dictionary of models for each parent-child pair.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/train.py</code> <pre><code>def clava_training(\n    tables: Tables,\n    relation_order: RelationOrder,\n    save_dir: Path,\n    diffusion_config: Configs,\n    classifier_config: Configs | None,\n    device: str = \"cuda\",\n) -&gt; tuple[Tables, dict[tuple[str, str], dict[str, Any]]]:\n    \"\"\"\n    Training function for the ClavaDDPM model.\n\n    Args:\n        tables: Definition of the tables and their relations. Example:\n            {\n                \"table1\": {\n                    \"children\": [\"table2\"],\n                    \"parents\": []\n                },\n                \"table2\": {\n                    \"children\": [],\n                    \"parents\": [\"table1\"]\n                }\n            }\n        relation_order: List of tuples of parent and child tables. Example:\n            [(\"table1\", \"table2\"), (\"table1\", \"table3\")]\n        save_dir: Directory to save the ClavaDDPM models.\n        diffusion_config: Dictionary of configurations for the diffusion model. The following config keys are required:\n            {\n                d_layers = list[int],\n                dropout = float,\n                iterations = int,\n                batch_size = int,\n                model_type = str[\"mlp\" | \"resnet\"],\n                gaussian_loss_type = str[\"mse\" | \"cross_entropy\"],\n                num_timesteps = int,\n                scheduler = str[\"cosine\" | \"linear\"],\n                lr = float,\n                weight_decay = float,\n            }\n        classifier_config: Dictionary of configurations for the classifier model. Not required for single table\n            training. The following config keys are required for multi-table training:\n            {\n                iterations = int,\n                batch_size = int,\n                d_layers = list[int],\n                dim_t = int,\n                lr = float,\n            }\n        device: Device to use for training. Default is `\"cuda\"`.\n\n    Returns:\n        A tuple with 2 values:\n            - The tables dictionary.\n            - Dictionary of models for each parent-child pair.\n    \"\"\"\n    models = {}\n    for parent, child in relation_order:\n        print(f\"Training {parent} -&gt; {child} model from scratch\")\n        df_with_cluster = tables[child][\"df\"]\n        id_cols = [col for col in df_with_cluster.columns if \"_id\" in col]\n        df_without_id = df_with_cluster.drop(columns=id_cols)\n\n        result = child_training(\n            df_without_id,\n            tables[child][\"domain\"],\n            parent,\n            child,\n            diffusion_config,\n            classifier_config,\n            device,\n        )\n\n        models[(parent, child)] = result\n\n        target_folder = save_dir / \"models\"\n        target_file = target_folder / f\"{parent}_{child}_ckpt.pkl\"\n\n        create_message = f\"Creating {target_folder}. \" if not target_folder.exists() else \"\"\n        log(INFO, f\"{create_message}Saving {parent} -&gt; {child} model to {target_file}\")\n\n        target_folder.mkdir(parents=True, exist_ok=True)\n        with open(target_file, \"wb\") as f:\n            pickle.dump(result, f)\n\n    for parent, child in relation_order:\n        if parent is None:\n            tables[child][\"df\"][\"placeholder\"] = list(range(len(tables[child][\"df\"])))\n\n    save_table_info(tables, relation_order, models, save_dir)\n\n    return tables, models\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.train.child_training","title":"child_training","text":"<pre><code>child_training(\n    child_df_with_cluster,\n    child_domain_dict,\n    parent_name,\n    child_name,\n    diffusion_config,\n    classifier_config,\n    device=\"cuda\",\n)\n</code></pre> <p>Training function for a single child table.</p> <p>Parameters:</p> Name Type Description Default <code>child_df_with_cluster</code> <code>DataFrame</code> <p>DataFrame with the cluster column.</p> required <code>child_domain_dict</code> <code>dict[str, Any]</code> <p>Dictionary of the child table domain. It should contain size and type for each column of the table. For example:     {         \"frequency\": {\"size\": 3, \"type\": \"discrete\"},         \"account_date\": {\"size\": 1535, \"type\": \"continuous\"},     }</p> required <code>parent_name</code> <code>str | None</code> <p>Name of the parent table, or None if there is no parent.</p> required <code>child_name</code> <code>str</code> <p>Name of the child table.</p> required <code>diffusion_config</code> <code>Configs</code> <p>Dictionary of configurations for the diffusion model. The following config keys are required: {     d_layers = list[int],     dropout = float,     iterations = int,     batch_size = int,     model_type = str[\"mlp\" | \"resnet\"],     gaussian_loss_type = str[\"mse\" | \"cross_entropy\"],     num_timesteps = int,     scheduler = str[\"cosine\" | \"linear\"],     lr = float,     weight_decay = float, }</p> required <code>classifier_config</code> <code>Configs | None</code> <p>Dictionary of configurations for the classifier model. Not required for single table training. The following config keys are required for multi-table training: {     iterations = int,     batch_size = int,     d_layers = list[int],     dim_t = int,     lr = float, }</p> required <code>device</code> <code>str</code> <p>Device to use for training. Default is <code>\"cuda\"</code>.</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of the training results.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/train.py</code> <pre><code>def child_training(\n    child_df_with_cluster: pd.DataFrame,\n    child_domain_dict: dict[str, Any],\n    parent_name: str | None,\n    child_name: str,\n    diffusion_config: Configs,\n    classifier_config: Configs | None,\n    device: str = \"cuda\",\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Training function for a single child table.\n\n    Args:\n        child_df_with_cluster: DataFrame with the cluster column.\n        child_domain_dict: Dictionary of the child table domain. It should contain size and type for each\n            column of the table. For example:\n                {\n                    \"frequency\": {\"size\": 3, \"type\": \"discrete\"},\n                    \"account_date\": {\"size\": 1535, \"type\": \"continuous\"},\n                }\n        parent_name: Name of the parent table, or None if there is no parent.\n        child_name: Name of the child table.\n        diffusion_config: Dictionary of configurations for the diffusion model. The following config keys are required:\n            {\n                d_layers = list[int],\n                dropout = float,\n                iterations = int,\n                batch_size = int,\n                model_type = str[\"mlp\" | \"resnet\"],\n                gaussian_loss_type = str[\"mse\" | \"cross_entropy\"],\n                num_timesteps = int,\n                scheduler = str[\"cosine\" | \"linear\"],\n                lr = float,\n                weight_decay = float,\n            }\n        classifier_config: Dictionary of configurations for the classifier model. Not required for single table\n            training. The following config keys are required for multi-table training:\n            {\n                iterations = int,\n                batch_size = int,\n                d_layers = list[int],\n                dim_t = int,\n                lr = float,\n            }\n        device: Device to use for training. Default is `\"cuda\"`.\n\n    Returns:\n        Dictionary of the training results.\n    \"\"\"\n    if parent_name is None:\n        # If there is no parent for this child table, just set a placeholder\n        # for its column name. This can happen on single table training or\n        # when the table is on the top level of the hierarchy.\n        # TODO: find a better name for this variable\n        y_col = \"placeholder\"\n        child_df_with_cluster[\"placeholder\"] = list(range(len(child_df_with_cluster)))\n    else:\n        y_col = f\"{parent_name}_{child_name}_cluster\"\n    child_info = get_table_info(child_df_with_cluster, child_domain_dict, y_col)\n    child_model_params = get_model_params(\n        {\n            \"d_layers\": diffusion_config[\"d_layers\"],\n            \"dropout\": diffusion_config[\"dropout\"],\n        }\n    )\n    child_T_dict = get_T_dict()\n    # ruff: noqa: N806\n\n    child_result = train_model(\n        child_df_with_cluster,\n        child_info,\n        child_model_params,\n        child_T_dict,\n        diffusion_config[\"iterations\"],\n        diffusion_config[\"batch_size\"],\n        diffusion_config[\"model_type\"],\n        diffusion_config[\"gaussian_loss_type\"],\n        diffusion_config[\"num_timesteps\"],\n        diffusion_config[\"scheduler\"],\n        diffusion_config[\"lr\"],\n        diffusion_config[\"weight_decay\"],\n        device=device,\n    )\n\n    if parent_name is None:\n        child_result[\"classifier\"] = None\n    else:\n        assert classifier_config is not None, \"Classifier config is required for multi-table training\"\n        if classifier_config[\"iterations\"] &gt; 0:\n            child_classifier = train_classifier(\n                child_df_with_cluster,\n                child_info,\n                child_model_params,\n                child_T_dict,\n                classifier_config[\"iterations\"],\n                classifier_config[\"batch_size\"],\n                diffusion_config[\"gaussian_loss_type\"],\n                diffusion_config[\"num_timesteps\"],\n                diffusion_config[\"scheduler\"],\n                cluster_col=y_col,\n                d_layers=classifier_config[\"d_layers\"],\n                dim_t=classifier_config[\"dim_t\"],\n                learning_rate=classifier_config[\"lr\"],\n                device=device,\n            )\n            child_result[\"classifier\"] = child_classifier\n        else:\n            log(WARNING, \"Skipping classifier training since classifier_config['iterations'] &lt;= 0\")\n\n    child_result[\"df_info\"] = child_info\n    child_result[\"model_params\"] = child_model_params\n    child_result[\"T_dict\"] = child_T_dict\n    return child_result\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.train.train_model","title":"train_model","text":"<pre><code>train_model(\n    data_frame,\n    data_frame_info,\n    model_params,\n    transformations_dict,\n    steps,\n    batch_size,\n    model_type,\n    gaussian_loss_type,\n    num_timesteps,\n    scheduler,\n    learning_rate,\n    weight_decay,\n    device=\"cuda\",\n)\n</code></pre> <p>Training function for the diffusion model.</p> <p>Parameters:</p> Name Type Description Default <code>data_frame</code> <code>DataFrame</code> <p>DataFrame to train the model on.</p> required <code>data_frame_info</code> <code>DataFrame</code> <p>Dictionary of the table information.</p> required <code>model_params</code> <code>dict[str, Any]</code> <p>Dictionary of the model parameters.</p> required <code>transformations_dict</code> <code>dict[str, Any]</code> <p>Dictionary of the transformations.</p> required <code>steps</code> <code>int</code> <p>Number of steps to train the model.</p> required <code>batch_size</code> <code>int</code> <p>Batch size to use for training.</p> required <code>model_type</code> <code>str</code> <p>Type of the model to use.</p> required <code>gaussian_loss_type</code> <code>str</code> <p>Type of the gaussian loss to use.</p> required <code>num_timesteps</code> <code>int</code> <p>Number of timesteps to use for the diffusion model.</p> required <code>scheduler</code> <code>str</code> <p>Scheduler to use for the diffusion model.</p> required <code>learning_rate</code> <code>float</code> <p>Learning rate to use for the optimizer in the diffusion model.</p> required <code>weight_decay</code> <code>float</code> <p>Weight decay to use for the optimizer in the diffusion model.</p> required <code>device</code> <code>str</code> <p>Device to use for training. Default is <code>\"cuda\"</code>.</p> <code>'cuda'</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of the training results. It will contain the following keys: - diffusion: The diffusion model. - label_encoders: The label encoders. - dataset: The dataset. - column_orders: The column orders.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/train.py</code> <pre><code>def train_model(\n    data_frame: pd.DataFrame,\n    data_frame_info: pd.DataFrame,\n    model_params: dict[str, Any],\n    transformations_dict: dict[str, Any],\n    steps: int,\n    batch_size: int,\n    model_type: str,\n    gaussian_loss_type: str,\n    num_timesteps: int,\n    scheduler: str,\n    learning_rate: float,\n    weight_decay: float,\n    device: str = \"cuda\",\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Training function for the diffusion model.\n\n    Args:\n        data_frame: DataFrame to train the model on.\n        data_frame_info: Dictionary of the table information.\n        model_params: Dictionary of the model parameters.\n        transformations_dict: Dictionary of the transformations.\n        steps: Number of steps to train the model.\n        batch_size: Batch size to use for training.\n        model_type: Type of the model to use.\n        gaussian_loss_type: Type of the gaussian loss to use.\n        num_timesteps: Number of timesteps to use for the diffusion model.\n        scheduler: Scheduler to use for the diffusion model.\n        learning_rate: Learning rate to use for the optimizer in the diffusion model.\n        weight_decay: Weight decay to use for the optimizer in the diffusion model.\n        device: Device to use for training. Default is `\"cuda\"`.\n\n    Returns:\n        Dictionary of the training results. It will contain the following keys:\n            - diffusion: The diffusion model.\n            - label_encoders: The label encoders.\n            - dataset: The dataset.\n            - column_orders: The column orders.\n    \"\"\"\n    transformations = Transformations(**transformations_dict)\n    # ruff: noqa: N806\n    dataset, label_encoders, column_orders = make_dataset_from_df(\n        data_frame,\n        transformations,\n        is_y_cond=model_params[\"is_y_cond\"],\n        ratios=[0.99, 0.005, 0.005],\n        df_info=data_frame_info,\n        std=0,\n    )\n\n    category_sizes = np.array(dataset.get_category_sizes(\"train\"))\n    # ruff: noqa: N806\n    if len(category_sizes) == 0 or transformations_dict[\"cat_encoding\"] == \"one-hot\":\n        category_sizes = np.array([0])\n        # ruff: noqa: N806\n\n    _, empirical_class_dist = torch.unique(torch.from_numpy(dataset.y[\"train\"]), return_counts=True)\n\n    num_numerical_features = dataset.X_num[\"train\"].shape[1] if dataset.X_num is not None else 0\n    d_in = np.sum(category_sizes) + num_numerical_features\n    model_params[\"d_in\"] = d_in\n\n    print(\"Model params: {}\".format(model_params))\n    model = get_model(model_type, model_params)\n    model.to(device)\n\n    train_loader = prepare_fast_dataloader(dataset, split=\"train\", batch_size=batch_size)\n\n    diffusion = GaussianMultinomialDiffusion(\n        num_classes=category_sizes,\n        num_numerical_features=num_numerical_features,\n        denoise_fn=model,\n        gaussian_loss_type=gaussian_loss_type,\n        num_timesteps=num_timesteps,\n        scheduler=scheduler,\n        device=torch.device(device),\n    )\n    diffusion.to(device)\n    diffusion.train()\n\n    trainer = ClavaDDPMTrainer(\n        diffusion,\n        train_loader,\n        lr=learning_rate,\n        weight_decay=weight_decay,\n        steps=steps,\n        device=device,\n    )\n    trainer.train()\n\n    if model_params[\"is_y_cond\"] == \"concat\":\n        column_orders = column_orders[1:] + [column_orders[0]]\n    else:\n        column_orders = column_orders + [data_frame_info[\"y_col\"]]\n\n    return {\n        \"diffusion\": diffusion,\n        \"label_encoders\": label_encoders,\n        \"dataset\": dataset,\n        \"column_orders\": column_orders,\n        \"num_numerical_features\": num_numerical_features,\n        \"K\": category_sizes,\n        \"empirical_class_dist\": empirical_class_dist,\n        \"is_regression\": dataset.is_regression,\n        \"inverse_transform\": dataset.num_transform.inverse_transform if dataset.num_transform is not None else None,\n    }\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.train.train_classifier","title":"train_classifier","text":"<pre><code>train_classifier(\n    data_frame,\n    data_frame_info,\n    model_params,\n    transformations_dict,\n    classifier_steps,\n    batch_size,\n    gaussian_loss_type,\n    num_timesteps,\n    scheduler,\n    d_layers,\n    device=\"cuda\",\n    cluster_col=\"cluster\",\n    dim_t=128,\n    learning_rate=0.0001,\n    classifier_evaluation_interval=5,\n)\n</code></pre> <p>Training function for the classifier model.</p> <p>Parameters:</p> Name Type Description Default <code>data_frame</code> <code>DataFrame</code> <p>DataFrame to train the model on.</p> required <code>data_frame_info</code> <code>DataFrame</code> <p>Dictionary of the table information.</p> required <code>model_params</code> <code>dict[str, Any]</code> <p>Dictionary of the model parameters.</p> required <code>transformations_dict</code> <code>dict[str, Any]</code> <p>Dictionary of the transformations.</p> required <code>classifier_steps</code> <code>int</code> <p>Number of steps to train the classifier.</p> required <code>batch_size</code> <code>int</code> <p>Batch size to use for training.</p> required <code>gaussian_loss_type</code> <code>str</code> <p>Type of the gaussian loss to use.</p> required <code>num_timesteps</code> <code>int</code> <p>Number of timesteps to use for the diffusion model.</p> required <code>scheduler</code> <code>str</code> <p>Scheduler to use for the diffusion model.</p> required <code>d_layers</code> <code>list[int]</code> <p>List of the hidden sizes of the classifier.</p> required <code>device</code> <code>str</code> <p>Device to use for training. Default is <code>\"cuda\"</code>.</p> <code>'cuda'</code> <code>cluster_col</code> <code>str</code> <p>Name of the cluster column. Default is <code>\"cluster\"</code>.</p> <code>'cluster'</code> <code>dim_t</code> <code>int</code> <p>Dimension of the timestamp. Default is 128.</p> <code>128</code> <code>learning_rate</code> <code>float</code> <p>Learning rate to use for the optimizer in the classifier. Default is 0.0001.</p> <code>0.0001</code> <code>classifier_evaluation_interval</code> <code>int</code> <p>The number of classifier training steps to wait until the next evaluation of the classifier. Default is 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>Classifier</code> <p>The trained classifier model.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/train.py</code> <pre><code>def train_classifier(\n    data_frame: pd.DataFrame,\n    data_frame_info: pd.DataFrame,\n    model_params: dict[str, Any],\n    transformations_dict: dict[str, Any],\n    classifier_steps: int,\n    batch_size: int,\n    gaussian_loss_type: str,\n    num_timesteps: int,\n    scheduler: str,\n    d_layers: list[int],\n    device: str = \"cuda\",\n    cluster_col: str = \"cluster\",\n    dim_t: int = 128,\n    learning_rate: float = 0.0001,\n    classifier_evaluation_interval: int = 5,\n) -&gt; Classifier:\n    \"\"\"\n    Training function for the classifier model.\n\n    Args:\n        data_frame: DataFrame to train the model on.\n        data_frame_info: Dictionary of the table information.\n        model_params: Dictionary of the model parameters.\n        transformations_dict: Dictionary of the transformations.\n        classifier_steps: Number of steps to train the classifier.\n        batch_size: Batch size to use for training.\n        gaussian_loss_type: Type of the gaussian loss to use.\n        num_timesteps: Number of timesteps to use for the diffusion model.\n        scheduler: Scheduler to use for the diffusion model.\n        d_layers: List of the hidden sizes of the classifier.\n        device: Device to use for training. Default is `\"cuda\"`.\n        cluster_col: Name of the cluster column. Default is `\"cluster\"`.\n        dim_t: Dimension of the timestamp. Default is 128.\n        learning_rate: Learning rate to use for the optimizer in the classifier. Default is 0.0001.\n        classifier_evaluation_interval: The number of classifier training steps to wait\n            until the next evaluation of the classifier. Default is 5.\n\n    Returns:\n        The trained classifier model.\n    \"\"\"\n    transformations = Transformations(**transformations_dict)\n    # ruff: noqa: N806\n    dataset, label_encoders, column_orders = make_dataset_from_df(\n        data_frame,\n        transformations,\n        is_y_cond=model_params[\"is_y_cond\"],\n        ratios=[0.99, 0.005, 0.005],\n        df_info=data_frame_info,\n        std=0,\n    )\n    print(dataset.n_features)\n    train_loader = prepare_fast_dataloader(dataset, split=\"train\", batch_size=batch_size, y_type=\"long\")\n    val_loader = prepare_fast_dataloader(dataset, split=\"val\", batch_size=batch_size, y_type=\"long\")\n    test_loader = prepare_fast_dataloader(dataset, split=\"test\", batch_size=batch_size, y_type=\"long\")\n\n    category_sizes = np.array(dataset.get_category_sizes(\"train\"))\n    # ruff: noqa: N806\n    if len(category_sizes) == 0 or transformations_dict[\"cat_encoding\"] == \"one-hot\":\n        category_sizes = np.array([0])\n        # ruff: noqa: N806\n    print(category_sizes)\n\n    # TODO: understand what's going on here\n    if dataset.X_num is None:\n        log(WARNING, \"dataset.X_num is None. num_numerical_features will be set to 0\")\n        num_numerical_features = 0\n    else:\n        num_numerical_features = dataset.X_num[\"train\"].shape[1]\n\n    if model_params[\"is_y_cond\"] == \"concat\":\n        num_numerical_features -= 1\n\n    classifier = Classifier(\n        d_in=num_numerical_features,\n        d_out=int(max(data_frame[cluster_col].values) + 1),  # TODO: add a comment why we need to add 1\n        dim_t=dim_t,\n        hidden_sizes=d_layers,\n    ).to(device)\n\n    classifier_optimizer = optim.AdamW(classifier.parameters(), lr=learning_rate)\n\n    empty_diffusion = GaussianMultinomialDiffusion(\n        num_classes=category_sizes,\n        num_numerical_features=num_numerical_features,\n        denoise_fn=None,  # type: ignore[arg-type]\n        gaussian_loss_type=gaussian_loss_type,\n        num_timesteps=num_timesteps,\n        scheduler=scheduler,\n        device=torch.device(device),\n    )\n    empty_diffusion.to(device)\n\n    schedule_sampler = create_named_schedule_sampler(\"uniform\", empty_diffusion)\n\n    classifier.train()\n    for step in range(classifier_steps):\n        logger.logkv(\"step\", step)\n        logger.logkv(\n            \"samples\",\n            (step + 1) * batch_size,\n        )\n        _numerical_forward_backward_log(\n            classifier,\n            classifier_optimizer,\n            train_loader,\n            dataset,\n            schedule_sampler,\n            empty_diffusion,\n            prefix=\"train\",\n            device=device,\n        )\n\n        classifier_optimizer.step()\n        if not step % classifier_evaluation_interval:\n            with torch.no_grad():\n                classifier.eval()\n                _numerical_forward_backward_log(\n                    classifier,\n                    classifier_optimizer,\n                    val_loader,\n                    dataset,\n                    schedule_sampler,\n                    empty_diffusion,\n                    prefix=\"val\",\n                    device=device,\n                )\n                classifier.train()\n\n    # test classifier\n    classifier.eval()\n\n    correct = 0\n    # TODO: why 3000 iterations? Why not just run through the test_loader once? Maybe it's a probabilistic classifier?\n    for _ in range(3000):\n        test_x, test_y = next(test_loader)\n        test_y = test_y.long().to(device)\n        test_x = test_x[:, 1:].to(device) if model_params[\"is_y_cond\"] == \"concat\" else test_x.to(device)\n        with torch.no_grad():\n            pred = classifier(test_x, timesteps=torch.zeros(test_x.shape[0]).to(device))\n            correct += (pred.argmax(dim=1) == test_y).sum().item()\n\n    acc = correct / (3000 * batch_size)\n    print(acc)\n\n    return classifier\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.train.save_table_info","title":"save_table_info","text":"<pre><code>save_table_info(tables, relation_order, models, save_dir)\n</code></pre> <p>Save the table information into the save_dir.</p> <p>Parameters:</p> Name Type Description Default <code>tables</code> <code>Tables</code> <p>Dictionary of the tables by name.</p> required <code>relation_order</code> <code>list[tuple[str, str]]</code> <p>List of tuples of parent and child tables. Example: [(\"table1\", \"table2\"), (\"table1\", \"table3\")]</p> required <code>models</code> <code>dict[tuple[str, str], dict[str, Any]]</code> <p>Dictionary of models for each parent-child pair.</p> required <code>save_dir</code> <code>Path</code> <p>Directory to save the table information.</p> required Source code in <code>src/midst_toolkit/models/clavaddpm/train.py</code> <pre><code>def save_table_info(\n    tables: Tables,\n    relation_order: list[tuple[str, str]],\n    models: dict[tuple[str, str], dict[str, Any]],\n    save_dir: Path,\n) -&gt; None:\n    \"\"\"\n    Save the table information into the save_dir.\n\n    Args:\n        tables: Dictionary of the tables by name.\n        relation_order: List of tuples of parent and child tables. Example:\n            [(\"table1\", \"table2\"), (\"table1\", \"table3\")]\n        models: Dictionary of models for each parent-child pair.\n        save_dir: Directory to save the table information.\n    \"\"\"\n    table_info = {}\n    for parent, child in relation_order:\n        result = models[(parent, child)]\n        df_with_cluster = tables[child][\"df\"]\n        df_without_id = get_df_without_id(df_with_cluster)\n        df_info = result[\"df_info\"]\n        x_num_real = df_without_id[df_info[\"num_cols\"]].to_numpy().astype(float)\n        unique_values_list = []\n        for column in range(x_num_real.shape[1]):\n            unique_values = np.unique(x_num_real[:, column])\n            unique_values_list.append(unique_values)\n        table_info[(parent, child)] = {\n            \"uniq_vals_list\": unique_values_list,\n            \"size\": len(df_with_cluster),\n            \"columns\": tables[child][\"df\"].columns,\n            \"parents\": tables[child][\"parents\"],\n            \"original_cols\": tables[child][\"original_cols\"],\n        }\n        required_keys = [\"num_numerical_features\", \"is_regression\", \"inverse_transform\", \"empirical_class_dist\", \"K\"]\n        filtered_result = {key: result[key] for key in required_keys}\n        table_info[(parent, child)].update(filtered_result)\n\n    for parent, child in relation_order:\n        with open(save_dir / f\"models/{parent}_{child}_ckpt.pkl\", \"rb\") as f:\n            result = pickle.load(f)\n\n        result[\"table_info\"] = table_info\n\n        with open(save_dir / f\"models/{parent}_{child}_ckpt.pkl\", \"wb\") as f:\n            pickle.dump(result, f)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.train.get_df_without_id","title":"get_df_without_id","text":"<pre><code>get_df_without_id(df)\n</code></pre> <p>Get the dataframe without the id columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>the input DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The DataFrame without the id columns.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/train.py</code> <pre><code>def get_df_without_id(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Get the dataframe without the id columns.\n\n    Args:\n        df: the input DataFrame.\n\n    Returns:\n        The DataFrame without the id columns.\n    \"\"\"\n    id_cols = [col for col in df.columns if \"_id\" in col]\n    return df.drop(columns=id_cols)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.trainer","title":"trainer","text":"<p>Trainer class for the ClavaDDPM model.</p>"},{"location":"api/#midst_toolkit.models.clavaddpm.trainer.ClavaDDPMTrainer","title":"ClavaDDPMTrainer","text":"Source code in <code>src/midst_toolkit/models/clavaddpm/trainer.py</code> <pre><code>class ClavaDDPMTrainer:\n    def __init__(\n        self,\n        diffusion_model: GaussianMultinomialDiffusion,\n        train_iter: Generator[tuple[Tensor, ...]],\n        lr: float,\n        weight_decay: float,\n        steps: int,\n        device: str = \"cuda\",\n    ):\n        \"\"\"\n        Trainer class for the ClavaDDPM model.\n\n        Args:\n            diffusion_model: The diffusion model.\n            train_iter: The training iterator. It should yield a tuple of tensors. The first tensor is the input\n                tensor and the second tensor is the output tensor.\n            lr: The learning rate.\n            weight_decay: The weight decay.\n            steps: The number of steps to train.\n            device: The device to use. Default is `\"cuda\"`.\n        \"\"\"\n        self.diffusion_model = diffusion_model\n        self.ema_model = deepcopy(self.diffusion_model._denoise_fn)\n        for param in self.ema_model.parameters():\n            param.detach_()\n\n        self.train_iter = train_iter\n        self.steps = steps\n        self.init_lr = lr\n        self.optimizer = torch.optim.AdamW(self.diffusion_model.parameters(), lr=lr, weight_decay=weight_decay)\n        self.device = device\n        self.loss_history = pd.DataFrame(columns=[\"step\", \"mloss\", \"gloss\", \"loss\"])\n        self.log_every = 100\n        self.print_every = 500\n        self.ema_every = 1000\n\n    def _anneal_lr(self, step: int) -&gt; None:\n        \"\"\"\n        Anneal the learning rate.\n\n        Args:\n            step: The current step.\n        \"\"\"\n        frac_done = step / self.steps\n        lr = self.init_lr * (1 - frac_done)\n        for param_group in self.optimizer.param_groups:\n            param_group[\"lr\"] = lr\n\n    def _train_step(self, x: Tensor, y: Tensor) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"\n        Run a single step of the training loop.\n\n        Args:\n            x: The input tensor.\n            y: The output tensor.\n\n        Returns:\n            A tuple with 2 values:\n                - The multi-class loss.\n                - The Gaussian loss.\n        \"\"\"\n        x = x.to(self.device)\n        target = {\"y\": y}\n        for k, v in target.items():\n            target[k] = v.long().to(self.device)\n        self.optimizer.zero_grad()\n        loss_multi, loss_gauss = self.diffusion_model.mixed_loss(x, target)\n        loss = loss_multi + loss_gauss\n        loss.backward()\n        self.optimizer.step()\n\n        return loss_multi, loss_gauss\n\n    def train(self) -&gt; None:\n        \"\"\"Run the training loop.\"\"\"\n        step = 0\n        curr_loss_multi = 0.0\n        curr_loss_gauss = 0.0\n\n        curr_count = 0\n        while step &lt; self.steps:\n            # TODO: improve this design. If self.steps is larger than self.train_iter,\n            # it will lead to a StopIteration error.\n            x, out = next(self.train_iter)\n            batch_loss_multi, batch_loss_gauss = self._train_step(x, out)\n\n            self._anneal_lr(step)\n\n            curr_count += len(x)\n            curr_loss_multi += batch_loss_multi.item() * len(x)\n            curr_loss_gauss += batch_loss_gauss.item() * len(x)\n\n            # TODO: improve this code, starting by moving it into a function for better readability and modularity.\n            if (step + 1) % self.log_every == 0:\n                mloss = np.around(curr_loss_multi / curr_count, 4)\n                gloss = np.around(curr_loss_gauss / curr_count, 4)\n                if (step + 1) % self.print_every == 0:\n                    print(f\"Step {(step + 1)}/{self.steps} MLoss: {mloss} GLoss: {gloss} Sum: {mloss + gloss}\")\n\n                # TODO: switch this for a concat for better code readability\n                self.loss_history.loc[len(self.loss_history)] = [\n                    step + 1,\n                    mloss,\n                    gloss,\n                    mloss + gloss,\n                ]\n                curr_count = 0\n                curr_loss_gauss = 0.0\n                curr_loss_multi = 0.0\n\n            update_ema(self.ema_model.parameters(), self.diffusion_model._denoise_fn.parameters())\n\n            step += 1\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(\n    diffusion_model,\n    train_iter,\n    lr,\n    weight_decay,\n    steps,\n    device=\"cuda\",\n)\n</code></pre> <p>Trainer class for the ClavaDDPM model.</p> <p>Parameters:</p> Name Type Description Default <code>diffusion_model</code> <code>GaussianMultinomialDiffusion</code> <p>The diffusion model.</p> required <code>train_iter</code> <code>Generator[tuple[Tensor, ...]]</code> <p>The training iterator. It should yield a tuple of tensors. The first tensor is the input tensor and the second tensor is the output tensor.</p> required <code>lr</code> <code>float</code> <p>The learning rate.</p> required <code>weight_decay</code> <code>float</code> <p>The weight decay.</p> required <code>steps</code> <code>int</code> <p>The number of steps to train.</p> required <code>device</code> <code>str</code> <p>The device to use. Default is <code>\"cuda\"</code>.</p> <code>'cuda'</code> Source code in <code>src/midst_toolkit/models/clavaddpm/trainer.py</code> <pre><code>def __init__(\n    self,\n    diffusion_model: GaussianMultinomialDiffusion,\n    train_iter: Generator[tuple[Tensor, ...]],\n    lr: float,\n    weight_decay: float,\n    steps: int,\n    device: str = \"cuda\",\n):\n    \"\"\"\n    Trainer class for the ClavaDDPM model.\n\n    Args:\n        diffusion_model: The diffusion model.\n        train_iter: The training iterator. It should yield a tuple of tensors. The first tensor is the input\n            tensor and the second tensor is the output tensor.\n        lr: The learning rate.\n        weight_decay: The weight decay.\n        steps: The number of steps to train.\n        device: The device to use. Default is `\"cuda\"`.\n    \"\"\"\n    self.diffusion_model = diffusion_model\n    self.ema_model = deepcopy(self.diffusion_model._denoise_fn)\n    for param in self.ema_model.parameters():\n        param.detach_()\n\n    self.train_iter = train_iter\n    self.steps = steps\n    self.init_lr = lr\n    self.optimizer = torch.optim.AdamW(self.diffusion_model.parameters(), lr=lr, weight_decay=weight_decay)\n    self.device = device\n    self.loss_history = pd.DataFrame(columns=[\"step\", \"mloss\", \"gloss\", \"loss\"])\n    self.log_every = 100\n    self.print_every = 500\n    self.ema_every = 1000\n</code></pre> <code></code> train \u00b6 <pre><code>train()\n</code></pre> <p>Run the training loop.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/trainer.py</code> <pre><code>def train(self) -&gt; None:\n    \"\"\"Run the training loop.\"\"\"\n    step = 0\n    curr_loss_multi = 0.0\n    curr_loss_gauss = 0.0\n\n    curr_count = 0\n    while step &lt; self.steps:\n        # TODO: improve this design. If self.steps is larger than self.train_iter,\n        # it will lead to a StopIteration error.\n        x, out = next(self.train_iter)\n        batch_loss_multi, batch_loss_gauss = self._train_step(x, out)\n\n        self._anneal_lr(step)\n\n        curr_count += len(x)\n        curr_loss_multi += batch_loss_multi.item() * len(x)\n        curr_loss_gauss += batch_loss_gauss.item() * len(x)\n\n        # TODO: improve this code, starting by moving it into a function for better readability and modularity.\n        if (step + 1) % self.log_every == 0:\n            mloss = np.around(curr_loss_multi / curr_count, 4)\n            gloss = np.around(curr_loss_gauss / curr_count, 4)\n            if (step + 1) % self.print_every == 0:\n                print(f\"Step {(step + 1)}/{self.steps} MLoss: {mloss} GLoss: {gloss} Sum: {mloss + gloss}\")\n\n            # TODO: switch this for a concat for better code readability\n            self.loss_history.loc[len(self.loss_history)] = [\n                step + 1,\n                mloss,\n                gloss,\n                mloss + gloss,\n            ]\n            curr_count = 0\n            curr_loss_gauss = 0.0\n            curr_loss_multi = 0.0\n\n        update_ema(self.ema_model.parameters(), self.diffusion_model._denoise_fn.parameters())\n\n        step += 1\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.trainer.update_ema","title":"update_ema","text":"<pre><code>update_ema(target_params, source_params, rate=0.999)\n</code></pre> <p>Update target parameters to be closer to those of source parameters using an exponential moving average.</p> <p>Parameters:</p> Name Type Description Default <code>target_params</code> <code>Iterator[Parameter]</code> <p>the target parameter sequence.</p> required <code>source_params</code> <code>Iterator[Parameter]</code> <p>the source parameter sequence.</p> required <code>rate</code> <code>float</code> <p>the EMA rate (closer to 1 means slower).</p> <code>0.999</code> Source code in <code>src/midst_toolkit/models/clavaddpm/trainer.py</code> <pre><code>def update_ema(\n    target_params: Iterator[nn.Parameter],\n    source_params: Iterator[nn.Parameter],\n    rate: float = 0.999,\n) -&gt; None:\n    \"\"\"\n    Update target parameters to be closer to those of source parameters using\n    an exponential moving average.\n\n    Args:\n        target_params: the target parameter sequence.\n        source_params: the source parameter sequence.\n        rate: the EMA rate (closer to 1 means slower).\n    \"\"\"\n    for targ, src in zip(target_params, source_params):\n        # TODO: is this doing anything at all? The detach functions will create new tensors,\n        # so this will not modify the original tensors, and this function does not return anything.\n        targ.detach().mul_(rate).add_(src.detach(), alpha=1 - rate)\n</code></pre>"},{"location":"api/#data-loaders-module","title":"Data Loaders Module","text":""},{"location":"api/#midst_toolkit.core.data_loaders","title":"midst_toolkit.core.data_loaders","text":""},{"location":"api/#logger-module","title":"Logger Module","text":""},{"location":"api/#midst_toolkit.core.logger","title":"midst_toolkit.core.logger","text":"<p>Logger copied from OpenAI baselines to avoid extra RL-based dependencies.</p> <p>https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/logger.py</p>"},{"location":"api/#midst_toolkit.core.logger.TensorBoardOutputFormat","title":"TensorBoardOutputFormat","text":"<p>               Bases: <code>KVWriter</code></p> <p>Dumps key/value pairs into TensorBoard's numeric format.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>class TensorBoardOutputFormat(KVWriter):\n    \"\"\"Dumps key/value pairs into TensorBoard's numeric format.\"\"\"\n\n    def __init__(self, dir: str):\n        os.makedirs(dir, exist_ok=True)\n        self.dir = dir\n        self.step = 1\n        prefix = \"events\"\n        path = osp.join(osp.abspath(dir), prefix)\n        import tensorflow as tf\n        from tensorflow.core.util import event_pb2\n        from tensorflow.python import pywrap_tensorflow\n        from tensorflow.python.util import compat\n\n        self.tf = tf\n        self.event_pb2 = event_pb2\n        self.pywrap_tensorflow = pywrap_tensorflow\n        self.writer = pywrap_tensorflow.EventsWriter(compat.as_bytes(path))\n\n    def writekvs(self, kvs: dict[str, Any]) -&gt; None:\n        def summary_val(k: str, v: Any) -&gt; Any:\n            kwargs = {\"tag\": k, \"simple_value\": float(v)}\n            return self.tf.Summary.Value(**kwargs)\n\n        summary = self.tf.Summary(value=[summary_val(k, v) for k, v in kvs.items()])\n        event = self.event_pb2.Event(wall_time=time.time(), summary=summary)\n        event.step = self.step  # is there any reason why you'd want to specify the step?\n        self.writer.WriteEvent(event)\n        self.writer.Flush()\n        self.step += 1\n\n    def close(self) -&gt; None:\n        if self.writer:\n            self.writer.Close()\n            self.writer = None\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.logkv","title":"logkv","text":"<pre><code>logkv(key, val)\n</code></pre> <p>Log a value of some diagnostic.</p> <p>Call this once for each diagnostic quantity, each iteration If called many times, last value will be used.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def logkv(key: str, val: Any) -&gt; None:\n    \"\"\"\n    Log a value of some diagnostic.\n\n    Call this once for each diagnostic quantity, each iteration\n    If called many times, last value will be used.\n    \"\"\"\n    get_current().logkv(key, val)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.logkv_mean","title":"logkv_mean","text":"<pre><code>logkv_mean(key, val)\n</code></pre> <p>The same as logkv(), but if called many times, values averaged.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def logkv_mean(key: str, val: Any) -&gt; None:\n    \"\"\"The same as logkv(), but if called many times, values averaged.\"\"\"\n    get_current().logkv_mean(key, val)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.logkvs","title":"logkvs","text":"<pre><code>logkvs(d)\n</code></pre> <p>Log a dictionary of key-value pairs.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def logkvs(d: dict[str, Any]) -&gt; None:\n    \"\"\"Log a dictionary of key-value pairs.\"\"\"\n    for k, v in d.items():\n        logkv(k, v)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.dumpkvs","title":"dumpkvs","text":"<pre><code>dumpkvs()\n</code></pre> <p>Write all of the diagnostics from the current iteration.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def dumpkvs() -&gt; dict[str, Any]:\n    \"\"\"Write all of the diagnostics from the current iteration.\"\"\"\n    return get_current().dumpkvs()\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.log","title":"log","text":"<pre><code>log(*args, level=INFO)\n</code></pre> <p>Logs the args in the desired level.</p> <p>Write the sequence of args, with no separators, to the console and output files (if you've configured an output file).</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def log(*args: Iterable[Any], level: int = INFO) -&gt; None:\n    \"\"\"\n    Logs the args in the desired level.\n\n    Write the sequence of args, with no separators, to the console and output\n    files (if you've configured an output file).\n    \"\"\"\n    get_current().log(*args, level=level)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.set_level","title":"set_level","text":"<pre><code>set_level(level)\n</code></pre> <p>Set logging threshold on current logger.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def set_level(level: int) -&gt; None:\n    \"\"\"Set logging threshold on current logger.\"\"\"\n    get_current().set_level(level)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.get_dir","title":"get_dir","text":"<pre><code>get_dir()\n</code></pre> <p>Get directory that log files are being written to.</p> <p>will be None if there is no output directory (i.e., if you didn't call start)</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def get_dir() -&gt; str:\n    \"\"\"\n    Get directory that log files are being written to.\n\n    will be None if there is no output directory (i.e., if you didn't call start)\n    \"\"\"\n    return get_current().get_dir()\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.profile","title":"profile","text":"<pre><code>profile(n)\n</code></pre> <p>Usage.</p> <p>@profile(\"my_func\") def my_func(): code</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def profile(n):\n    \"\"\"\n    Usage.\n\n    @profile(\"my_func\")\n    def my_func(): code\n    \"\"\"\n\n    def decorator_with_name(func):\n        def func_wrapper(*args, **kwargs):\n            with profile_kv(n):\n                return func(*args, **kwargs)\n\n        return func_wrapper\n\n    return decorator_with_name\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.mpi_weighted_mean","title":"mpi_weighted_mean","text":"<pre><code>mpi_weighted_mean(comm, local_name2valcount)\n</code></pre> <p>Copied from below.</p> <p>https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/common/mpi_util.py#L110 Perform a weighted average over dicts that are each on a different node Input: local_name2valcount: dict mapping key -&gt; (value, count) Returns: key -&gt; mean</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def mpi_weighted_mean(comm: Any, local_name2valcount: dict[str, tuple[float, float]]) -&gt; dict[str, float]:\n    \"\"\"\n    Copied from below.\n\n    https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/common/mpi_util.py#L110\n    Perform a weighted average over dicts that are each on a different node\n    Input: local_name2valcount: dict mapping key -&gt; (value, count)\n    Returns: key -&gt; mean\n    \"\"\"\n    all_name2valcount = comm.gather(local_name2valcount)\n    if comm.rank == 0:\n        name2sum: defaultdict[str, float] = defaultdict(float)\n        name2count: defaultdict[str, float] = defaultdict(float)\n        for n2vc in all_name2valcount:\n            for name, (val, count) in n2vc.items():\n                try:\n                    val = float(val)\n                except ValueError:\n                    if comm.rank == 0:\n                        warnings.warn(\"WARNING: tried to compute mean on non-float {}={}\".format(name, val))\n                        # ruff: noqa: B028\n                else:\n                    name2sum[name] += val * count\n                    name2count[name] += count\n        return {name: name2sum[name] / name2count[name] for name in name2sum}\n    return {}\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.configure","title":"configure","text":"<pre><code>configure(\n    dir=None, format_strs=None, comm=None, log_suffix=\"\"\n)\n</code></pre> <p>If comm is provided, average all numerical stats across that comm.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def configure(\n    dir: str | None = None,\n    format_strs: list[str] | None = None,\n    comm: Any | None = None,\n    log_suffix: str = \"\",\n) -&gt; None:\n    \"\"\"If comm is provided, average all numerical stats across that comm.\"\"\"\n    if dir is None:\n        dir = os.getenv(\"OPENAI_LOGDIR\")\n    if dir is None:\n        dir = osp.join(\n            tempfile.gettempdir(),\n            datetime.datetime.now().strftime(\"openai-%Y-%m-%d-%H-%M-%S-%f\"),\n        )\n    assert isinstance(dir, str)\n    dir = os.path.expanduser(dir)\n    os.makedirs(os.path.expanduser(dir), exist_ok=True)\n\n    rank = get_rank_without_mpi_import()\n    if rank &gt; 0:\n        log_suffix = log_suffix + \"-rank%03i\" % rank\n\n    if format_strs is None:\n        if rank == 0:\n            format_strs = os.getenv(\"OPENAI_LOG_FORMAT\", \"stdout,log,csv\").split(\",\")\n        else:\n            format_strs = os.getenv(\"OPENAI_LOG_FORMAT_MPI\", \"log\").split(\",\")\n    format_strs_filter = filter(None, format_strs)\n    output_formats = [make_output_format(f, dir, log_suffix) for f in format_strs_filter]\n\n    Logger.CURRENT = Logger(dir=dir, output_formats=output_formats, comm=comm)  # type: ignore[assignment]\n    if output_formats:\n        log(\"Logging to %s\" % dir)\n</code></pre>"},{"location":"api/#diffusion-utils-module","title":"Diffusion Utils Module","text":""},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils","title":"midst_toolkit.models.clavaddpm.diffusion_utils","text":"<p>PLACEHOLDER.</p>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.FoundNANsError","title":"FoundNANsError","text":"<p>               Bases: <code>BaseException</code></p> <p>Found NANs during sampling.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>class FoundNANsError(BaseException):\n    \"\"\"Found NANs during sampling.\"\"\"\n\n    def __init__(self, message=\"Found NANs during sampling.\"):\n        # ruff: noqa: D107\n        super(FoundNANsError, self).__init__(message)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.normal_kl","title":"normal_kl","text":"<pre><code>normal_kl(mean1, logvar1, mean2, logvar2)\n</code></pre> <p>Compute the KL divergence between two gaussians.</p> <p>Shapes are automatically broadcasted, so batches can be compared to scalars, among other use cases.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def normal_kl(\n    mean1: Tensor | float,\n    logvar1: Tensor | float,\n    mean2: Tensor | float,\n    logvar2: Tensor | float,\n) -&gt; Tensor:\n    \"\"\"\n    Compute the KL divergence between two gaussians.\n\n    Shapes are automatically broadcasted, so batches can be compared to\n    scalars, among other use cases.\n    \"\"\"\n    tensor = None\n    for obj in (mean1, logvar1, mean2, logvar2):\n        if isinstance(obj, torch.Tensor):\n            tensor = obj\n            break\n    assert tensor is not None, \"at least one argument must be a Tensor\"\n\n    # Force variances to be Tensors. Broadcasting helps convert scalars to\n    # Tensors, but it does not work for torch.exp().\n    logvar1, logvar2 = [x if isinstance(x, torch.Tensor) else torch.tensor(x).to(tensor) for x in (logvar1, logvar2)]\n\n    return 0.5 * (\n        -1.0 + logvar2 - logvar1 + torch.exp(logvar1 - logvar2) + ((mean1 - mean2) ** 2) * torch.exp(-logvar2)\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.approx_standard_normal_cdf","title":"approx_standard_normal_cdf","text":"<pre><code>approx_standard_normal_cdf(x)\n</code></pre> <p>A fast approximation of the cumulative distribution function of the standard normal.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def approx_standard_normal_cdf(x: Tensor) -&gt; Tensor:\n    \"\"\"\n    A fast approximation of the cumulative distribution function of the\n    standard normal.\n    \"\"\"\n    return 0.5 * (1.0 + torch.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * torch.pow(x, 3))))\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.discretized_gaussian_log_likelihood","title":"discretized_gaussian_log_likelihood","text":"<pre><code>discretized_gaussian_log_likelihood(\n    x, *, means, log_scales\n)\n</code></pre> <p>Compute the log-likelihood of a Gaussian distribution discretizing to a given image.</p> <p>:param x: the target images. It is assumed that this was uint8 values,           rescaled to the range [-1, 1]. :param means: the Gaussian mean Tensor. :param log_scales: the Gaussian log stddev Tensor. :return: a tensor like x of log probabilities (in nats).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def discretized_gaussian_log_likelihood(x: Tensor, *, means: Tensor, log_scales: Tensor) -&gt; Tensor:\n    \"\"\"\n    Compute the log-likelihood of a Gaussian distribution discretizing to a\n    given image.\n\n    :param x: the target images. It is assumed that this was uint8 values,\n              rescaled to the range [-1, 1].\n    :param means: the Gaussian mean Tensor.\n    :param log_scales: the Gaussian log stddev Tensor.\n    :return: a tensor like x of log probabilities (in nats).\n    \"\"\"\n    assert x.shape == means.shape == log_scales.shape\n    centered_x = x - means\n    inv_stdv = torch.exp(-log_scales)\n    plus_in = inv_stdv * (centered_x + 1.0 / 255.0)\n    cdf_plus = approx_standard_normal_cdf(plus_in)\n    min_in = inv_stdv * (centered_x - 1.0 / 255.0)\n    cdf_min = approx_standard_normal_cdf(min_in)\n    log_cdf_plus = torch.log(cdf_plus.clamp(min=1e-12))\n    log_one_minus_cdf_min = torch.log((1.0 - cdf_min).clamp(min=1e-12))\n    cdf_delta = cdf_plus - cdf_min\n    log_probs = torch.where(\n        x &lt; -0.999,\n        log_cdf_plus,\n        torch.where(x &gt; 0.999, log_one_minus_cdf_min, torch.log(cdf_delta.clamp(min=1e-12))),\n    )\n    assert log_probs.shape == x.shape\n    return log_probs\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.sum_except_batch","title":"sum_except_batch","text":"<pre><code>sum_except_batch(x, num_dims=1)\n</code></pre> <p>Sums all dimensions except the first.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Tensor, shape (batch_size, ...)</p> required <code>num_dims</code> <code>int</code> <p>int, number of batch dims (default=1)</p> <code>1</code> <p>Returns:</p> Name Type Description <code>x_sum</code> <code>Tensor</code> <p>Tensor, shape (batch_size,)</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def sum_except_batch(x: Tensor, num_dims: int = 1) -&gt; Tensor:\n    \"\"\"\n    Sums all dimensions except the first.\n\n    Args:\n        x: Tensor, shape (batch_size, ...)\n        num_dims: int, number of batch dims (default=1)\n\n    Returns:\n        x_sum: Tensor, shape (batch_size,)\n    \"\"\"\n    return x.reshape(*x.shape[:num_dims], -1).sum(-1)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.mean_flat","title":"mean_flat","text":"<pre><code>mean_flat(tensor)\n</code></pre> <p>Take the mean over all non-batch dimensions.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def mean_flat(tensor: Tensor) -&gt; Tensor:\n    \"\"\"Take the mean over all non-batch dimensions.\"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n</code></pre>"},{"location":"api/#gaussian-multinomial-diffusion-module","title":"Gaussian Multinomial Diffusion Module","text":""},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion","title":"midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion","text":"<p>Based on the code below.</p> <p>https://github.com/openai/guided-diffusion/blob/main/guided_diffusion https://github.com/ehoogeboom/multinomial_diffusion</p>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.GaussianMultinomialDiffusion","title":"GaussianMultinomialDiffusion","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>class GaussianMultinomialDiffusion(torch.nn.Module):\n    def __init__(\n        # ruff: noqa: PLR0915\n        self,\n        num_classes: np.ndarray,\n        num_numerical_features: int,\n        denoise_fn: torch.nn.Module,\n        num_timesteps: int = 1000,\n        gaussian_loss_type: str = \"mse\",\n        gaussian_parametrization: str = \"eps\",\n        multinomial_loss_type: str = \"vb_stochastic\",\n        parametrization: str = \"x0\",\n        scheduler: str = \"cosine\",\n        device: torch.device | None = None,\n    ):\n        # ruff: noqa: D107\n        if device is None:\n            device = torch.device(\"cpu\")\n\n        super(GaussianMultinomialDiffusion, self).__init__()\n        assert multinomial_loss_type in (\"vb_stochastic\", \"vb_all\")\n        assert parametrization in (\"x0\", \"direct\")\n\n        if multinomial_loss_type == \"vb_all\":\n            print(\n                \"Computing the loss using the bound on _all_ timesteps.\"\n                \" This is expensive both in terms of memory and computation.\"\n            )\n\n        self.num_numerical_features = num_numerical_features\n        self.num_classes = num_classes  # it as a vector [K1, K2, ..., Km]\n        self.num_classes_expanded = torch.from_numpy(\n            np.concatenate([num_classes[i].repeat(num_classes[i]) for i in range(len(num_classes))])\n        ).to(device)\n\n        self.slices_for_classes = [np.arange(self.num_classes[0])]\n        offsets: np.ndarray = np.cumsum(self.num_classes)\n        for i in range(1, len(offsets)):\n            self.slices_for_classes.append(np.arange(offsets[i - 1], offsets[i]))\n        self.offsets = torch.from_numpy(np.append([0], offsets)).to(device)\n\n        self._denoise_fn = denoise_fn\n        self.gaussian_loss_type = gaussian_loss_type\n        self.gaussian_parametrization = gaussian_parametrization\n        self.multinomial_loss_type = multinomial_loss_type\n        self.num_timesteps = num_timesteps\n        self.parametrization = parametrization\n        self.scheduler = scheduler\n        self.device = device\n        self.alphas: Tensor\n        self.alphas_cumprod: Tensor\n        self.alphas_cumprod_next: Tensor\n        self.alphas_cumprod_prev: Tensor\n        self.sqrt_alphas_cumprod: Tensor\n        self.sqrt_one_minus_alphas_cumprod: Tensor\n        self.log_cumprod_alpha: Tensor\n        self.log_alpha: Tensor\n        self.log_1_min_alpha: Tensor\n        self.log_1_min_cumprod_alpha: Tensor\n        self.sqrt_recipm1_alphas_cumprod: Tensor\n        self.sqrt_recip_alphas_cumprod: Tensor\n        self.Lt_history: Tensor\n        self.Lt_count: Tensor\n\n        a = 1.0 - get_named_beta_schedule(scheduler, num_timesteps)\n        alphas = torch.tensor(a.astype(\"float64\"))\n        betas = 1.0 - alphas\n\n        log_alpha: Tensor = np.log(alphas)  # type: ignore[assignment]\n        log_cumprod_alpha: Tensor = np.cumsum(log_alpha)  # type: ignore[assignment]\n\n        log_1_min_alpha: Tensor = log_1_min_a(log_alpha)\n        log_1_min_cumprod_alpha: Tensor = log_1_min_a(log_cumprod_alpha)\n\n        alphas_cumprod: Tensor = np.cumprod(alphas, axis=0)  # type: ignore[assignment]\n        alphas_cumprod_prev = torch.tensor(np.append(1.0, alphas_cumprod[:-1]))\n        alphas_cumprod_next = torch.tensor(np.append(alphas_cumprod[1:], 0.0))\n        sqrt_alphas_cumprod: Tensor = np.sqrt(alphas_cumprod)  # type: ignore[assignment]\n        sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - alphas_cumprod)\n        sqrt_recip_alphas_cumprod: Tensor = np.sqrt(1.0 / alphas_cumprod)\n        sqrt_recipm1_alphas_cumprod: Tensor = np.sqrt(1.0 / alphas_cumprod - 1)\n\n        # Gaussian diffusion\n\n        self.posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n        self.posterior_log_variance_clipped = (\n            torch.from_numpy(np.log(np.append(self.posterior_variance[1], self.posterior_variance[1:])))\n            .float()\n            .to(device)\n        )\n        self.posterior_mean_coef1 = (betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)).float().to(device)\n        self.posterior_mean_coef2 = (\n            ((1.0 - alphas_cumprod_prev) * np.sqrt(alphas.numpy()) / (1.0 - alphas_cumprod)).float().to(device)\n        )\n\n        assert log_add_exp(log_alpha, log_1_min_alpha).abs().sum().item() &lt; 1.0e-5\n        assert log_add_exp(log_cumprod_alpha, log_1_min_cumprod_alpha).abs().sum().item() &lt; 1e-5\n        diff: Tensor = cast(Tensor, np.cumsum(log_alpha) - log_cumprod_alpha)\n        assert diff.abs().sum().item() &lt; 1.0e-5\n\n        # Convert to float32 and register buffers.\n        self.register_buffer(\"alphas\", alphas.float().to(device))\n        self.register_buffer(\"log_alpha\", log_alpha.float().to(device))\n        self.register_buffer(\"log_1_min_alpha\", log_1_min_alpha.float().to(device))\n        self.register_buffer(\"log_1_min_cumprod_alpha\", log_1_min_cumprod_alpha.float().to(device))\n        self.register_buffer(\"log_cumprod_alpha\", log_cumprod_alpha.float().to(device))\n        self.register_buffer(\"alphas_cumprod\", alphas_cumprod.float().to(device))\n        self.register_buffer(\"alphas_cumprod_prev\", alphas_cumprod_prev.float().to(device))\n        self.register_buffer(\"alphas_cumprod_next\", alphas_cumprod_next.float().to(device))\n        self.register_buffer(\"sqrt_alphas_cumprod\", sqrt_alphas_cumprod.float().to(device))\n        self.register_buffer(\n            \"sqrt_one_minus_alphas_cumprod\",\n            sqrt_one_minus_alphas_cumprod.float().to(device),\n        )\n        self.register_buffer(\"sqrt_recip_alphas_cumprod\", sqrt_recip_alphas_cumprod.float().to(device))\n        self.register_buffer(\n            \"sqrt_recipm1_alphas_cumprod\",\n            sqrt_recipm1_alphas_cumprod.float().to(device),\n        )\n\n        self.register_buffer(\"Lt_history\", torch.zeros(num_timesteps))\n        self.register_buffer(\"Lt_count\", torch.zeros(num_timesteps))\n\n    # Gaussian part\n    def gaussian_q_mean_variance(self, x_start: Tensor, t: Tensor) -&gt; tuple[Tensor, Tensor, Tensor]:\n        mean = extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n        variance = extract(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract(self.log_1_min_cumprod_alpha, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def gaussian_q_sample(self, x_start: Tensor, t: Tensor, noise: Tensor | None = None) -&gt; Tensor:\n        if noise is None:\n            noise = torch.randn_like(x_start)\n        assert noise.shape == x_start.shape\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n            + extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n        )\n\n    def gaussian_q_posterior_mean_variance(\n        self,\n        x_start: Tensor,\n        x_t: Tensor,\n        t: Tensor,\n    ) -&gt; tuple[Tensor, Tensor, Tensor]:\n        assert x_start.shape == x_t.shape\n        posterior_mean = (\n            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start\n            + extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n        assert (\n            posterior_mean.shape[0]\n            == posterior_variance.shape[0]\n            == posterior_log_variance_clipped.shape[0]\n            == x_start.shape[0]\n        )\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def gaussian_p_mean_variance(\n        self,\n        model_output: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        denoised_fn: Callable | None = None,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Tensor]:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        B, C = x.shape[:2]\n        assert t.shape == (B,)\n\n        model_variance = torch.cat(\n            [\n                self.posterior_variance[1].unsqueeze(0).to(x.device),\n                (1.0 - self.alphas)[1:],\n            ],\n            dim=0,\n        )\n        # model_variance = self.posterior_variance.to(x.device)\n        model_log_variance = torch.log(model_variance)\n\n        model_variance = extract(model_variance, t, x.shape)\n        model_log_variance = extract(model_log_variance, t, x.shape)\n\n        if self.gaussian_parametrization == \"eps\":\n            pred_xstart = self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n        elif self.gaussian_parametrization == \"x0\":\n            pred_xstart = model_output\n        else:\n            raise NotImplementedError\n\n        model_mean, _, _ = self.gaussian_q_posterior_mean_variance(x_start=pred_xstart, x_t=x, t=t)\n\n        assert model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape, (\n            f\"{model_mean.shape}, {model_log_variance.shape}, {pred_xstart.shape}, {x.shape}\"\n        )\n\n        return {\n            \"mean\": model_mean,\n            \"variance\": model_variance,\n            \"log_variance\": model_log_variance,\n            \"pred_xstart\": pred_xstart,\n        }\n\n    def _vb_terms_bpd(\n        self,\n        model_output: Tensor,\n        x_start: Tensor,\n        x_t: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Tensor]:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        (\n            true_mean,\n            _,\n            true_log_variance_clipped,\n        ) = self.gaussian_q_posterior_mean_variance(x_start=x_start, x_t=x_t, t=t)\n        out = self.gaussian_p_mean_variance(\n            model_output, x_t, t, clip_denoised=clip_denoised, model_kwargs=model_kwargs\n        )\n        kl = normal_kl(true_mean, true_log_variance_clipped, out[\"mean\"], out[\"log_variance\"])\n        kl = mean_flat(kl) / np.log(2.0)\n\n        decoder_nll = -discretized_gaussian_log_likelihood(\n            x_start, means=out[\"mean\"], log_scales=0.5 * out[\"log_variance\"]\n        )\n        assert decoder_nll.shape == x_start.shape\n        decoder_nll = mean_flat(decoder_nll) / np.log(2.0)\n\n        # At the first timestep return the decoder NLL,\n        # otherwise return KL(q(x_{t-1}|x_t,x_0) || p(x_{t-1}|x_t))\n        output = torch.where((t == 0), decoder_nll, kl)\n        return {\n            \"output\": output,\n            \"pred_xstart\": out[\"pred_xstart\"],\n            \"out_mean\": out[\"mean\"],\n            \"true_mean\": true_mean,\n        }\n\n    def _prior_gaussian(self, x_start: Tensor) -&gt; Tensor:\n        \"\"\"\n        Get the prior KL term for the variational lower-bound, measured in\n        bits-per-dim.\n\n        This term can't be optimized, as it only depends on the encoder.\n\n        :param x_start: the [N x C x ...] tensor of inputs.\n        :return: a batch of [N] KL values (in bits), one per batch element.\n        \"\"\"\n        batch_size = x_start.shape[0]\n        t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n        qt_mean, _, qt_log_variance = self.gaussian_q_mean_variance(x_start, t)\n        kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0)\n        return mean_flat(kl_prior) / np.log(2.0)\n\n    def _gaussian_loss(\n        self,\n        model_out: Tensor,\n        x_start: Tensor,\n        x_t: Tensor,\n        t: Tensor,\n        noise: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; Tensor:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        terms = {}\n        if self.gaussian_loss_type == \"mse\":\n            terms[\"loss\"] = mean_flat((noise - model_out) ** 2)\n        elif self.gaussian_loss_type == \"kl\":\n            terms[\"loss\"] = self._vb_terms_bpd(\n                model_output=model_out,\n                x_start=x_start,\n                x_t=x_t,\n                t=t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n            )[\"output\"]\n\n        return terms[\"loss\"]\n\n    def _predict_xstart_from_eps(self, x_t: Tensor, t: Tensor, eps: Tensor) -&gt; Tensor:\n        assert x_t.shape == eps.shape\n        return (\n            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n            - extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * eps\n        )\n\n    def _predict_eps_from_xstart(self, x_t: Tensor, t: Tensor, pred_xstart: Tensor) -&gt; Tensor:\n        return (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / extract(\n            self.sqrt_recipm1_alphas_cumprod, t, x_t.shape\n        )\n\n    def condition_mean(\n        self,\n        cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n        p_mean_var: dict[str, Tensor],\n        x: Tensor,\n        t: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; Tensor:\n        \"\"\"\n        Compute the mean for the previous step, given a function cond_fn that\n        computes the gradient of a conditional log probability with respect to\n        x. In particular, cond_fn computes grad(log(p(y|x))), and we want to\n        condition on y.\n\n        This uses the conditioning strategy from Sohl-Dickstein et al. (2015).\n        \"\"\"\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        gradient = cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n        return p_mean_var[\"mean\"].float() + p_mean_var[\"variance\"] * gradient.float()\n\n    def condition_score(\n        self,\n        cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n        p_mean_var: dict[str, Tensor],\n        x: Tensor,\n        t: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Tensor]:\n        \"\"\"\n        Compute what the p_mean_variance output would have been, should the\n        model's score function be conditioned by cond_fn.\n\n        See condition_mean() for details on cond_fn.\n\n        Unlike condition_mean(), this instead uses the conditioning strategy\n        from Song et al (2020).\n        \"\"\"\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        alpha_bar = extract(self.alphas_cumprod, t, x.shape)\n\n        eps = self._predict_eps_from_xstart(x, t, p_mean_var[\"pred_xstart\"])\n        eps = eps - (1 - alpha_bar).sqrt() * cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n\n        out = p_mean_var.copy()\n        out[\"pred_xstart\"] = self._predict_xstart_from_eps(x, t, eps)\n        out[\"mean\"], _, _ = self.gaussian_q_posterior_mean_variance(x_start=out[\"pred_xstart\"], x_t=x, t=t)\n        return out\n\n    def gaussian_p_sample(\n        self,\n        model_out: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        denoised_fn: Callable | None = None,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; dict[str, Tensor]:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        out = self.gaussian_p_mean_variance(\n            model_out,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n        )\n        noise = torch.randn_like(x)\n        nonzero_mask = (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))  # no noise when t == 0\n\n        if cond_fn is not None:\n            out[\"mean\"] = self.condition_mean(cond_fn, out, x, t, model_kwargs=model_kwargs)\n\n        sample = out[\"mean\"] + nonzero_mask * torch.exp(0.5 * out[\"log_variance\"]) * noise\n        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    # Multinomial part\n\n    def multinomial_kl(self, log_prob1: Tensor, log_prob2: Tensor) -&gt; Tensor:\n        return (log_prob1.exp() * (log_prob1 - log_prob2)).sum(dim=1)\n\n    def q_pred_one_timestep(self, log_x_t: Tensor, t: Tensor) -&gt; Tensor:\n        log_alpha_t = extract(self.log_alpha, t, log_x_t.shape)\n        log_1_min_alpha_t = extract(self.log_1_min_alpha, t, log_x_t.shape)\n\n        # alpha_t * E[xt] + (1 - alpha_t) 1 / K\n        return log_add_exp(\n            log_x_t + log_alpha_t,\n            log_1_min_alpha_t - torch.log(self.num_classes_expanded),\n        )\n\n    def q_pred(self, log_x_start: Tensor, t: Tensor) -&gt; Tensor:\n        log_cumprod_alpha_t = extract(self.log_cumprod_alpha, t, log_x_start.shape)\n        log_1_min_cumprod_alpha = extract(self.log_1_min_cumprod_alpha, t, log_x_start.shape)\n\n        return log_add_exp(\n            log_x_start + log_cumprod_alpha_t,\n            log_1_min_cumprod_alpha - torch.log(self.num_classes_expanded),\n        )\n\n    def predict_start(self, model_out: Tensor, log_x_t: Tensor, t: Tensor, out_dict: dict[str, Tensor]) -&gt; Tensor:\n        # model_out = self._denoise_fn(x_t, t.to(x_t.device), **out_dict)\n\n        assert model_out.size(0) == log_x_t.size(0)\n        assert self.num_classes is not None\n        assert model_out.size(1) == self.num_classes.sum(), f\"{model_out.size()}\"\n\n        log_pred = torch.empty_like(model_out)\n        for ix in self.slices_for_classes:\n            log_pred[:, ix] = F.log_softmax(model_out[:, ix], dim=1)\n        return log_pred\n\n    def q_posterior(self, log_x_start: Tensor, log_x_t: Tensor, t: Tensor) -&gt; Tensor:\n        # q(xt-1 | xt, x0) = q(xt | xt-1, x0) * q(xt-1 | x0) / q(xt | x0)\n        # where q(xt | xt-1, x0) = q(xt | xt-1).\n\n        # EV_log_qxt_x0 = self.q_pred(log_x_start, t)\n\n        # print('sum exp', EV_log_qxt_x0.exp().sum(1).mean())\n        # assert False\n\n        # log_qxt_x0 = (log_x_t.exp() * EV_log_qxt_x0).sum(dim=1)\n        t_minus_1 = t - 1\n        # Remove negative values, will not be used anyway for final decoder\n        t_minus_1 = torch.where(t_minus_1 &lt; 0, torch.zeros_like(t_minus_1), t_minus_1)\n        log_EV_qxtmin_x0 = self.q_pred(log_x_start, t_minus_1)\n\n        num_axes = (1,) * (len(log_x_start.size()) - 1)\n        t_broadcast = t.to(log_x_start.device).view(-1, *num_axes) * torch.ones_like(log_x_start)\n        log_EV_qxtmin_x0 = torch.where(t_broadcast == 0, log_x_start, log_EV_qxtmin_x0.to(torch.float32))\n\n        # unnormed_logprobs = log_EV_qxtmin_x0 +\n        #                     log q_pred_one_timestep(x_t, t)\n        # Note: _NOT_ x_tmin1, which is how the formula is typically used!!!\n        # Not very easy to see why this is true. But it is :)\n        unnormed_logprobs = log_EV_qxtmin_x0 + self.q_pred_one_timestep(log_x_t, t)\n\n        return unnormed_logprobs - sliced_logsumexp(unnormed_logprobs, self.offsets)\n\n    def p_pred(self, model_out: Tensor, log_x: Tensor, t: Tensor, out_dict: dict[str, Tensor]) -&gt; Tensor:\n        if self.parametrization == \"x0\":\n            log_x_recon = self.predict_start(model_out, log_x, t=t, out_dict=out_dict)\n            log_model_pred = self.q_posterior(log_x_start=log_x_recon, log_x_t=log_x, t=t)\n        elif self.parametrization == \"direct\":\n            log_model_pred = self.predict_start(model_out, log_x, t=t, out_dict=out_dict)\n        else:\n            raise ValueError\n        return log_model_pred\n\n    @torch.no_grad()\n    def p_sample(self, model_out: Tensor, log_x: Tensor, t: Tensor, out_dict: dict[str, Tensor]) -&gt; Tensor:\n        model_log_prob = self.p_pred(model_out, log_x=log_x, t=t, out_dict=out_dict)\n        return self.log_sample_categorical(model_log_prob)\n\n    # Dead code\n    # @torch.no_grad()\n    # def p_sample_loop(self, shape, out_dict):\n    #     b = shape[0]\n    #     # start with random normal image.\n    #     img = torch.randn(shape, device=device)\n\n    #     for i in reversed(range(1, self.num_timesteps)):\n    #         img = self.p_sample(img, torch.full((b,), i, device=self.device, dtype=torch.long), out_dict)\n    #     return img\n\n    # @torch.no_grad()\n    # def _sample(self, image_size, out_dict, batch_size=16):\n    #     return self.p_sample_loop((batch_size, 3, image_size, image_size), out_dict)\n\n    # Dead code\n    # @torch.no_grad()\n    # def interpolate(self, x1: Tensor, x2: Tensor, t: Tensor | None = None, lam: float = 0.5) -&gt; Tensor:\n    #     b, *_, device = *x1.shape, x1.device\n    #     t = default(t, self.num_timesteps - 1)\n\n    #     assert x1.shape == x2.shape\n\n    #     t_batched = torch.stack([torch.tensor(t, device=device)] * b)\n    #     xt1, xt2 = map(lambda x: self.q_sample(x, t=t_batched), (x1, x2))\n\n    #     img = (1 - lam) * xt1 + lam * xt2\n    #     for i in reversed(range(0, t)):\n    #         img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long))\n\n    #     return img\n\n    def log_sample_categorical(self, logits: Tensor) -&gt; Tensor:\n        full_sample = []\n        for i in range(len(self.num_classes)):\n            one_class_logits = logits[:, self.slices_for_classes[i]]\n            uniform = torch.rand_like(one_class_logits)\n            gumbel_noise = -torch.log(-torch.log(uniform + 1e-30) + 1e-30)\n            sample = (gumbel_noise + one_class_logits).argmax(dim=1)\n            full_sample.append(sample.unsqueeze(1))\n        full_sample_tensor = torch.cat(full_sample, dim=1)\n        return index_to_log_onehot(full_sample_tensor, torch.from_numpy(self.num_classes))\n\n    def q_sample(self, log_x_start: Tensor, t: Tensor) -&gt; Tensor:\n        log_EV_qxt_x0 = self.q_pred(log_x_start, t)\n        # ruff: noqa: N806\n        return self.log_sample_categorical(log_EV_qxt_x0)\n\n    # Dead code\n    # def nll(self, log_x_start, out_dict):\n    #     b = log_x_start.size(0)\n    #     device = log_x_start.device\n    #     loss = 0\n    #     for t in range(0, self.num_timesteps):\n    #         t_array = (torch.ones(b, device=device) * t).long()\n\n    #         kl = self.compute_Lt(\n    #             log_x_start=log_x_start,\n    #             log_x_t=self.q_sample(log_x_start=log_x_start, t=t_array),\n    #             t=t_array,\n    #             out_dict=out_dict,\n    #         )\n\n    #         loss += kl\n\n    #     loss += self.kl_prior(log_x_start)\n\n    #     return loss\n\n    def kl_prior(self, log_x_start: Tensor) -&gt; Tensor:\n        b = log_x_start.size(0)\n        device = log_x_start.device\n        ones = torch.ones(b, device=device).long()\n\n        log_qxT_prob = self.q_pred(log_x_start, t=(self.num_timesteps - 1) * ones)\n        # ruff: noqa: N806\n        log_half_prob = -torch.log(self.num_classes_expanded * torch.ones_like(log_qxT_prob))\n\n        kl_prior = self.multinomial_kl(log_qxT_prob, log_half_prob)\n        return sum_except_batch(kl_prior)\n\n    def compute_Lt(\n        # ruff: noqa: N802\n        self,\n        model_out: Tensor,\n        log_x_start: Tensor,\n        log_x_t: Tensor,\n        t: Tensor,\n        out_dict: dict[str, Tensor],\n        detach_mean: bool = False,\n    ) -&gt; Tensor:\n        log_true_prob = self.q_posterior(log_x_start=log_x_start, log_x_t=log_x_t, t=t)\n        log_model_prob = self.p_pred(model_out, log_x=log_x_t, t=t, out_dict=out_dict)\n\n        if detach_mean:\n            log_model_prob = log_model_prob.detach()\n\n        kl = self.multinomial_kl(log_true_prob, log_model_prob)\n        kl = sum_except_batch(kl)\n\n        decoder_nll = -log_categorical(log_x_start, log_model_prob)\n        decoder_nll = sum_except_batch(decoder_nll)\n\n        mask = (t == torch.zeros_like(t)).float()\n        return mask * decoder_nll + (1.0 - mask) * kl\n\n    def sample_time(self, b: int, device: torch.device, method: str = \"uniform\") -&gt; tuple[Tensor, Tensor]:\n        if method == \"importance\":\n            if not (self.Lt_count &gt; 10).all():\n                return self.sample_time(b, device, method=\"uniform\")\n\n            Lt_sqrt = torch.sqrt(self.Lt_history + 1e-10) + 0.0001\n            # ruff: noqa: N806\n            Lt_sqrt[0] = Lt_sqrt[1]  # Overwrite decoder term with L1.\n            pt_all = (Lt_sqrt / Lt_sqrt.sum()).to(device)\n\n            t = torch.multinomial(pt_all, num_samples=b, replacement=True).to(device)\n\n            pt = pt_all.gather(dim=0, index=t)\n\n            return t, pt\n\n        if method == \"uniform\":\n            t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n\n            pt = torch.ones_like(t).float() / self.num_timesteps\n            return t, pt\n        raise ValueError\n\n    def _multinomial_loss(\n        self,\n        model_out: Tensor,\n        log_x_start: Tensor,\n        log_x_t: Tensor,\n        t: Tensor,\n        pt: Tensor,\n        out_dict: dict[str, Tensor],\n    ) -&gt; Tensor:\n        if self.multinomial_loss_type == \"vb_stochastic\":\n            kl = self.compute_Lt(model_out, log_x_start, log_x_t, t, out_dict)\n            kl_prior = self.kl_prior(log_x_start)\n            # Upweigh loss term of the kl\n            return kl / pt + kl_prior\n\n        if self.multinomial_loss_type == \"vb_all\":\n            # Expensive, dont do it ;).\n            # DEPRECATED\n            # return -self.nll(log_x_start)\n            raise ValueError(\"multinomial_loss_type == 'vb_all' is deprecated.\")\n        raise ValueError\n\n    # Dead code\n    # def log_prob(self, x, out_dict):\n    #     b, device = x.size(0), x.device\n    #     if self.training:\n    #         return self._multinomial_loss(x, out_dict)\n\n    #     log_x_start = index_to_log_onehot(x, self.num_classes)\n\n    #     t, pt = self.sample_time(b, device, \"importance\")\n\n    #     kl = self.compute_Lt(log_x_start, self.q_sample(log_x_start=log_x_start, t=t), t, out_dict)\n\n    #     kl_prior = self.kl_prior(log_x_start)\n\n    #     # Upweigh loss term of the kl\n    #     loss = kl / pt + kl_prior\n\n    #     return -loss\n\n    def mixed_loss(self, x: Tensor, out_dict: dict[str, Tensor]) -&gt; tuple[Tensor, Tensor]:\n        b = x.shape[0]\n        device = x.device\n        t, pt = self.sample_time(b, device, \"uniform\")\n\n        x_num = x[:, : self.num_numerical_features]\n        x_cat = x[:, self.num_numerical_features :]\n\n        x_num_t = x_num\n        log_x_cat_t = x_cat\n        if x_num.shape[1] &gt; 0:\n            noise = torch.randn_like(x_num)\n            x_num_t = self.gaussian_q_sample(x_num, t, noise=noise)\n        if x_cat.shape[1] &gt; 0:\n            log_x_cat = index_to_log_onehot(x_cat.long(), torch.from_numpy(self.num_classes))\n            log_x_cat_t = self.q_sample(log_x_start=log_x_cat, t=t)\n\n        x_in = torch.cat([x_num_t, log_x_cat_t], dim=1)\n\n        model_out = self._denoise_fn(x_in, t, **out_dict)\n\n        model_out_num = model_out[:, : self.num_numerical_features]\n        model_out_cat = model_out[:, self.num_numerical_features :]\n\n        loss_multi = torch.zeros((1,)).float()\n        loss_gauss = torch.zeros((1,)).float()\n        if x_cat.shape[1] &gt; 0:\n            loss_multi = self._multinomial_loss(model_out_cat, log_x_cat, log_x_cat_t, t, pt, out_dict) / len(\n                self.num_classes\n            )\n\n        if x_num.shape[1] &gt; 0:\n            loss_gauss = self._gaussian_loss(model_out_num, x_num, x_num_t, t, noise)\n\n        # loss_multi = torch.where(out_dict['y'] == 1, loss_multi, 2 * loss_multi)\n        # loss_gauss = torch.where(out_dict['y'] == 1, loss_gauss, 2 * loss_gauss)\n\n        return loss_multi.mean(), loss_gauss.mean()\n\n    @torch.no_grad()\n    def mixed_elbo(self, x0, out_dict):\n        b = x0.size(0)\n        device = x0.device\n\n        x_num = x0[:, : self.num_numerical_features]\n        x_cat = x0[:, self.num_numerical_features :]\n        has_cat = x_cat.shape[1] &gt; 0\n        if has_cat:\n            log_x_cat = index_to_log_onehot(x_cat.long(), torch.from_numpy(self.num_classes)).to(device)\n\n        gaussian_loss = []\n        xstart_mse = []\n        mse = []\n        # mu_mse = []\n        out_mean = []\n        true_mean = []\n        multinomial_loss = []\n        for t in range(self.num_timesteps):\n            t_array = (torch.ones(b, device=device) * t).long()\n            noise = torch.randn_like(x_num)\n\n            x_num_t = self.gaussian_q_sample(x_start=x_num, t=t_array, noise=noise)\n            log_x_cat_t = self.q_sample(log_x_start=log_x_cat, t=t_array) if has_cat else x_cat\n\n            model_out = self._denoise_fn(torch.cat([x_num_t, log_x_cat_t], dim=1), t_array, **out_dict)\n\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n\n            kl = torch.tensor([0.0])\n            if has_cat:\n                kl = self.compute_Lt(\n                    model_out=model_out_cat,\n                    log_x_start=log_x_cat,\n                    log_x_t=log_x_cat_t,\n                    t=t_array,\n                    out_dict=out_dict,\n                )\n\n            out = self._vb_terms_bpd(\n                model_out_num,\n                x_start=x_num,\n                x_t=x_num_t,\n                t=t_array,\n                clip_denoised=False,\n            )\n\n            multinomial_loss.append(kl)\n            gaussian_loss.append(out[\"output\"])\n            xstart_mse.append(mean_flat((out[\"pred_xstart\"] - x_num) ** 2))\n            # mu_mse.append(mean_flat(out[\"mean_mse\"]))\n            out_mean.append(mean_flat(out[\"out_mean\"]))\n            true_mean.append(mean_flat(out[\"true_mean\"]))\n\n            eps = self._predict_eps_from_xstart(x_num_t, t_array, out[\"pred_xstart\"])\n            mse.append(mean_flat((eps - noise) ** 2))\n\n        gaussian_loss_tensor = torch.stack(gaussian_loss, dim=1)\n        multinomial_loss_tensor = torch.stack(multinomial_loss, dim=1)\n        xstart_mse_tensor = torch.stack(xstart_mse, dim=1)\n        mse_tensor = torch.stack(mse, dim=1)\n        # mu_mse = torch.stack(mu_mse, dim=1)\n        out_mean_tensor = torch.stack(out_mean, dim=1)\n        true_mean_tensor = torch.stack(true_mean, dim=1)\n\n        prior_gauss = self._prior_gaussian(x_num)\n\n        prior_multin = torch.tensor([0.0])\n        if has_cat:\n            prior_multin = self.kl_prior(log_x_cat)\n\n        total_gauss = gaussian_loss_tensor.sum(dim=1) + prior_gauss\n        total_multin = multinomial_loss_tensor.sum(dim=1) + prior_multin\n        return {\n            \"total_gaussian\": total_gauss,\n            \"total_multinomial\": total_multin,\n            \"losses_gaussian\": gaussian_loss_tensor,\n            \"losses_multinimial\": multinomial_loss_tensor,\n            \"xstart_mse\": xstart_mse_tensor,\n            \"mse\": mse_tensor,\n            # \"mu_mse\": mu_mse\n            \"out_mean\": out_mean_tensor,\n            \"true_mean\": true_mean_tensor,\n        }\n\n    @torch.no_grad()\n    def gaussian_ddim_step(\n        self,\n        model_out_num: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        denoised_fn: Callable | None = None,\n        eta: float = 0.0,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; Tensor:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        out = self.gaussian_p_mean_variance(\n            model_out_num,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=None,\n        )\n\n        if cond_fn is not None:\n            out = self.condition_score(cond_fn, out, x, t, model_kwargs=model_kwargs)\n\n        eps = self._predict_eps_from_xstart(x, t, out[\"pred_xstart\"])\n\n        alpha_bar = extract(self.alphas_cumprod, t, x.shape)\n        alpha_bar_prev = extract(self.alphas_cumprod_prev, t, x.shape)\n        sigma = eta * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar)) * torch.sqrt(1 - alpha_bar / alpha_bar_prev)\n\n        noise = torch.randn_like(x)\n        mean_pred = out[\"pred_xstart\"] * torch.sqrt(alpha_bar_prev) + torch.sqrt(1 - alpha_bar_prev - sigma**2) * eps\n        nonzero_mask = (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))  # no noise when t == 0\n        return mean_pred + nonzero_mask * sigma * noise\n\n    @torch.no_grad()\n    def gaussian_ddim_sample(self, noise, T, out_dict, eta=0.0, model_kwargs=None, cond_fn=None):\n        # ruff: noqa: D102, N803\n        x = noise\n        b = x.shape[0]\n        device = x.device\n        for t in reversed(range(T)):\n            print(f\"Sample timestep {t:4d}\", end=\"\\r\")\n            t_array = (torch.ones(b, device=device) * t).long()\n            out_num = self._denoise_fn(x, t_array, **out_dict)\n            x = self.gaussian_ddim_step(out_num, x, t_array, model_kwargs=model_kwargs, cond_fn=cond_fn)\n        print()\n        return x\n\n    @torch.no_grad()\n    def gaussian_ddim_reverse_step(\n        self,\n        model_out_num: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        eta: float = 0.0,\n    ) -&gt; Tensor:\n        # ruff: noqa: D102\n        assert eta == 0.0, \"Eta must be zero.\"\n        out = self.gaussian_p_mean_variance(\n            model_out_num,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=None,\n            model_kwargs=None,\n        )\n\n        eps = (extract(self.sqrt_recip_alphas_cumprod, t, x.shape) * x - out[\"pred_xstart\"]) / extract(\n            self.sqrt_recipm1_alphas_cumprod, t, x.shape\n        )\n        alpha_bar_next = extract(self.alphas_cumprod_next, t, x.shape)\n\n        return out[\"pred_xstart\"] * torch.sqrt(alpha_bar_next) + torch.sqrt(1 - alpha_bar_next) * eps\n\n    @torch.no_grad()\n    def gaussian_ddim_reverse_sample(\n        self,\n        x,\n        T,\n        # ruff: noqa: N803\n        out_dict,\n    ):\n        # ruff: noqa: D102\n        b = x.shape[0]\n        device = x.device\n        for t in range(T):\n            print(f\"Reverse timestep {t:4d}\", end=\"\\r\")\n            t_array = (torch.ones(b, device=device) * t).long()\n            out_num = self._denoise_fn(x, t_array, **out_dict)\n            x = self.gaussian_ddim_reverse_step(out_num, x, t_array, eta=0.0)\n        print()\n\n        return x\n\n    @torch.no_grad()\n    def multinomial_ddim_step(\n        self,\n        model_out_cat: Tensor,\n        log_x_t: Tensor,\n        t: Tensor,\n        out_dict: dict[str, Tensor],\n        eta: float = 0.0,\n    ) -&gt; Tensor:\n        # ruff: noqa: D102\n        # not ddim, essentially\n        log_x0 = self.predict_start(model_out_cat, log_x_t=log_x_t, t=t, out_dict=out_dict)\n\n        alpha_bar = extract(self.alphas_cumprod, t, log_x_t.shape)\n        alpha_bar_prev = extract(self.alphas_cumprod_prev, t, log_x_t.shape)\n        sigma = eta * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar)) * torch.sqrt(1 - alpha_bar / alpha_bar_prev)\n\n        coef1 = sigma\n        coef2 = alpha_bar_prev - sigma * alpha_bar\n        coef3 = 1 - coef1 - coef2\n\n        log_ps = torch.stack(\n            [\n                torch.log(coef1) + log_x_t,\n                torch.log(coef2) + log_x0,\n                torch.log(coef3) - torch.log(self.num_classes_expanded),\n            ],\n            dim=2,\n        )\n\n        log_prob = torch.logsumexp(log_ps, dim=2)\n\n        return self.log_sample_categorical(log_prob)\n\n    @torch.no_grad()\n    def sample_ddim(\n        self,\n        num_samples: int,\n        y_dist: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; tuple[Tensor, dict[str, Tensor]]:\n        # ruff: noqa: D102\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        b = num_samples\n        z_norm = torch.randn((b, self.num_numerical_features), device=self.device)\n\n        assert self.num_classes is not None\n        has_cat = self.num_classes[0] != 0\n        log_z = torch.zeros((b, 0), device=self.device).float()\n        if has_cat:\n            uniform_logits = torch.zeros((b, len(self.num_classes_expanded)), device=self.device)\n            log_z = self.log_sample_categorical(uniform_logits)\n\n        y = torch.multinomial(y_dist, num_samples=b, replacement=True)\n        out_dict = {\"y\": y.long().to(self.device)}\n        for i in reversed(range(0, self.num_timesteps)):\n            print(f\"Sample timestep {i:4d}\", end=\"\\r\")\n            t = torch.full((b,), i, device=self.device, dtype=torch.long)\n            model_out = self._denoise_fn(torch.cat([z_norm, log_z], dim=1).float(), t, **out_dict)\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n            z_norm = self.gaussian_ddim_step(\n                model_out_num,\n                z_norm,\n                t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n                cond_fn=cond_fn,\n            )\n            if has_cat:\n                log_z = self.multinomial_ddim_step(model_out_cat, log_z, t, out_dict)\n\n        print()\n        z_ohe = torch.exp(log_z).round()\n        z_cat = log_z\n        if has_cat:\n            z_cat = ohe_to_categories(z_ohe, torch.from_numpy(self.num_classes))\n        sample = torch.cat([z_norm, z_cat], dim=1).cpu()\n        return sample, out_dict\n\n    @torch.no_grad()\n    def conditional_sample(\n        self,\n        ys: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; tuple[Tensor, dict[str, Tensor]]:\n        # ruff: noqa: D102\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        b = len(ys)\n        z_norm = torch.randn((b, self.num_numerical_features), device=self.device)\n        assert self.num_classes is not None\n        has_cat = self.num_classes[0] != 0\n        log_z = torch.zeros((b, 0), device=self.device).float()\n\n        out_dict = {\"y\": ys.long().to(self.device)}\n        for i in reversed(range(0, self.num_timesteps)):\n            print(f\"Sample timestep {i:4d}\", end=\"\\r\")\n            t = torch.full((b,), i, device=self.device, dtype=torch.long)\n            model_out = self._denoise_fn(torch.cat([z_norm, log_z], dim=1).float(), t, **out_dict)\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n            z_norm = self.gaussian_p_sample(\n                model_out_num,\n                z_norm,\n                t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n                cond_fn=cond_fn,\n            )[\"sample\"]\n            if has_cat:\n                log_z = self.p_sample(model_out_cat, log_z, t, out_dict)\n\n        print()\n        z_ohe = torch.exp(log_z).round()\n        z_cat = log_z\n        if has_cat:\n            z_cat = ohe_to_categories(z_ohe, torch.from_numpy(self.num_classes))\n        sample = torch.cat([z_norm, z_cat], dim=1).cpu()\n        return sample, out_dict\n\n    @torch.no_grad()\n    def sample(\n        self,\n        num_samples: int,\n        y_dist: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; tuple[Tensor, dict[str, Tensor]]:\n        # ruff: noqa: D102\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        b = num_samples\n        z_norm = torch.randn((b, self.num_numerical_features), device=self.device)\n\n        assert self.num_classes is not None\n        has_cat = self.num_classes[0] != 0\n        log_z = torch.zeros((b, 0), device=self.device).float()\n        if has_cat:\n            uniform_logits = torch.zeros((b, len(self.num_classes_expanded)), device=self.device)\n            log_z = self.log_sample_categorical(uniform_logits)\n\n        y = torch.multinomial(y_dist, num_samples=b, replacement=True)\n        out_dict = {\"y\": y.long().to(self.device)}\n        for i in reversed(range(0, self.num_timesteps)):\n            print(f\"Sample timestep {i:4d}\", end=\"\\r\")\n            t = torch.full((b,), i, device=self.device, dtype=torch.long)\n            model_out = self._denoise_fn(torch.cat([z_norm, log_z], dim=1).float(), t, **out_dict)\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n            z_norm = self.gaussian_p_sample(\n                model_out_num,\n                z_norm,\n                t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n                cond_fn=cond_fn,\n            )[\"sample\"]\n            if has_cat:\n                log_z = self.p_sample(model_out_cat, log_z, t, out_dict)\n\n        print()\n        z_ohe = torch.exp(log_z).round()\n        z_cat = log_z\n        if has_cat:\n            z_cat = ohe_to_categories(z_ohe, torch.from_numpy(self.num_classes))\n        sample = torch.cat([z_norm, z_cat], dim=1).cpu()\n        return sample, out_dict\n\n    def sample_all(\n        self,\n        num_samples: int,\n        batch_size: int,\n        y_dist: Tensor,\n        ddim: bool = False,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; tuple[Tensor, Tensor]:\n        # ruff: noqa: D102\n        if ddim:\n            print(\"Sample using DDIM.\")\n            sample_fn = self.sample_ddim\n        else:\n            sample_fn = self.sample\n\n        b = batch_size\n\n        all_y = []\n        all_samples = []\n        num_generated = 0\n        while num_generated &lt; num_samples:\n            sample, out_dict = sample_fn(b, y_dist, model_kwargs=model_kwargs, cond_fn=cond_fn)\n            mask_nan = torch.any(sample.isnan(), dim=1)\n            sample = sample[~mask_nan]\n            out_dict[\"y\"] = out_dict[\"y\"][~mask_nan]\n\n            all_samples.append(sample)\n            all_y.append(out_dict[\"y\"].cpu())\n            if sample.shape[0] != b:\n                raise FoundNANsError\n            num_generated += sample.shape[0]\n\n        x_gen = torch.cat(all_samples, dim=0)[:num_samples]\n        y_gen = torch.cat(all_y, dim=0)[:num_samples]\n\n        return x_gen, y_gen\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.GaussianMultinomialDiffusion.condition_mean","title":"condition_mean","text":"<pre><code>condition_mean(\n    cond_fn, p_mean_var, x, t, model_kwargs=None\n)\n</code></pre> <p>Compute the mean for the previous step, given a function cond_fn that computes the gradient of a conditional log probability with respect to x. In particular, cond_fn computes grad(log(p(y|x))), and we want to condition on y.</p> <p>This uses the conditioning strategy from Sohl-Dickstein et al. (2015).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def condition_mean(\n    self,\n    cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n    p_mean_var: dict[str, Tensor],\n    x: Tensor,\n    t: Tensor,\n    model_kwargs: dict[str, Any] | None = None,\n) -&gt; Tensor:\n    \"\"\"\n    Compute the mean for the previous step, given a function cond_fn that\n    computes the gradient of a conditional log probability with respect to\n    x. In particular, cond_fn computes grad(log(p(y|x))), and we want to\n    condition on y.\n\n    This uses the conditioning strategy from Sohl-Dickstein et al. (2015).\n    \"\"\"\n    if model_kwargs is None:\n        model_kwargs = {}\n\n    gradient = cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n    return p_mean_var[\"mean\"].float() + p_mean_var[\"variance\"] * gradient.float()\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.GaussianMultinomialDiffusion.condition_score","title":"condition_score","text":"<pre><code>condition_score(\n    cond_fn, p_mean_var, x, t, model_kwargs=None\n)\n</code></pre> <p>Compute what the p_mean_variance output would have been, should the model's score function be conditioned by cond_fn.</p> <p>See condition_mean() for details on cond_fn.</p> <p>Unlike condition_mean(), this instead uses the conditioning strategy from Song et al (2020).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def condition_score(\n    self,\n    cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n    p_mean_var: dict[str, Tensor],\n    x: Tensor,\n    t: Tensor,\n    model_kwargs: dict[str, Any] | None = None,\n) -&gt; dict[str, Tensor]:\n    \"\"\"\n    Compute what the p_mean_variance output would have been, should the\n    model's score function be conditioned by cond_fn.\n\n    See condition_mean() for details on cond_fn.\n\n    Unlike condition_mean(), this instead uses the conditioning strategy\n    from Song et al (2020).\n    \"\"\"\n    if model_kwargs is None:\n        model_kwargs = {}\n\n    alpha_bar = extract(self.alphas_cumprod, t, x.shape)\n\n    eps = self._predict_eps_from_xstart(x, t, p_mean_var[\"pred_xstart\"])\n    eps = eps - (1 - alpha_bar).sqrt() * cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n\n    out = p_mean_var.copy()\n    out[\"pred_xstart\"] = self._predict_xstart_from_eps(x, t, eps)\n    out[\"mean\"], _, _ = self.gaussian_q_posterior_mean_variance(x_start=out[\"pred_xstart\"], x_t=x, t=t)\n    return out\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.get_named_beta_schedule","title":"get_named_beta_schedule","text":"<pre><code>get_named_beta_schedule(\n    schedule_name, num_diffusion_timesteps\n)\n</code></pre> <p>Get a pre-defined beta schedule for the given name. The beta schedule library consists of beta schedules which remain similar in the limit of num_diffusion_timesteps. Beta schedules may be added, but should not be removed or changed once they are committed to maintain backwards compatibility.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def get_named_beta_schedule(schedule_name: str, num_diffusion_timesteps: int) -&gt; np.ndarray:\n    \"\"\"\n    Get a pre-defined beta schedule for the given name.\n    The beta schedule library consists of beta schedules which remain similar\n    in the limit of num_diffusion_timesteps.\n    Beta schedules may be added, but should not be removed or changed once\n    they are committed to maintain backwards compatibility.\n    \"\"\"\n    if schedule_name == \"linear\":\n        # Linear schedule from Ho et al, extended to work for any number of\n        # diffusion steps.\n        scale = 1000 / num_diffusion_timesteps\n        beta_start = scale * 0.0001\n        beta_end = scale * 0.02\n        return np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64)\n    if schedule_name == \"cosine\":\n        return betas_for_alpha_bar(\n            num_diffusion_timesteps,\n            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n        )\n    raise NotImplementedError(f\"unknown beta schedule: {schedule_name}\")\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.betas_for_alpha_bar","title":"betas_for_alpha_bar","text":"<pre><code>betas_for_alpha_bar(\n    num_diffusion_timesteps, alpha_bar, max_beta=0.999\n)\n</code></pre> <p>Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of (1-beta) over time from t = [0,1]. :param num_diffusion_timesteps: the number of betas to produce. :param alpha_bar: a lambda that takes an argument t from 0 to 1 and                   produces the cumulative product of (1-beta) up to that                   part of the diffusion process. :param max_beta: the maximum beta to use; use values lower than 1 to                  prevent singularities.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def betas_for_alpha_bar(num_diffusion_timesteps: int, alpha_bar: Callable, max_beta: float = 0.999) -&gt; np.ndarray:\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function,\n    which defines the cumulative product of (1-beta) over time from t = [0,1].\n    :param num_diffusion_timesteps: the number of betas to produce.\n    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n                      produces the cumulative product of (1-beta) up to that\n                      part of the diffusion process.\n    :param max_beta: the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n    \"\"\"\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return np.array(betas)\n</code></pre>"},{"location":"api/#clavaddpm-model-module","title":"ClavaDDPM Model Module","text":""},{"location":"api/#midst_toolkit.models.clavaddpm.model","title":"midst_toolkit.models.clavaddpm.model","text":""},{"location":"api/#midst_toolkit.models.clavaddpm.model.FastTensorDataLoader","title":"FastTensorDataLoader","text":"<p>Defines a faster dataloader for PyTorch tensors.</p> <p>A DataLoader-like object for a set of tensors that can be much faster than TensorDataset + DataLoader because dataloader grabs individual indices of the dataset and calls cat (slow). Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class FastTensorDataLoader:\n    \"\"\"\n    Defines a faster dataloader for PyTorch tensors.\n\n    A DataLoader-like object for a set of tensors that can be much faster than\n    TensorDataset + DataLoader because dataloader grabs individual indices of\n    the dataset and calls cat (slow).\n    Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6\n    \"\"\"\n\n    def __init__(self, *tensors: Tensor, batch_size: int = 32, shuffle: bool = False):\n        \"\"\"\n        Initialize a FastTensorDataLoader.\n        :param *tensors: tensors to store. Must have the same length @ dim 0.\n        :param batch_size: batch size to load.\n        :param shuffle: if True, shuffle the data *in-place* whenever an\n            iterator is created out of this object.\n        :returns: A FastTensorDataLoader.\n        \"\"\"\n        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n        self.tensors = tensors\n\n        self.dataset_len = self.tensors[0].shape[0]\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n\n        # Calculate # batches\n        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n        if remainder &gt; 0:\n            n_batches += 1\n        self.n_batches = n_batches\n\n    def __iter__(self):\n        # ruff: noqa: D105\n        if self.shuffle:\n            r = torch.randperm(self.dataset_len)\n            self.tensors = [t[r] for t in self.tensors]  # type: ignore[assignment]\n        self.i = 0\n        return self\n\n    def __next__(self):\n        # ruff: noqa: D105\n        if self.i &gt;= self.dataset_len:\n            raise StopIteration\n        batch = tuple(t[self.i : self.i + self.batch_size] for t in self.tensors)\n        self.i += self.batch_size\n        return batch\n\n    def __len__(self):\n        # ruff: noqa: D105\n        return self.n_batches\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.FastTensorDataLoader.__init__","title":"__init__","text":"<pre><code>__init__(*tensors, batch_size=32, shuffle=False)\n</code></pre> <p>Initialize a FastTensorDataLoader. :param tensors: tensors to store. Must have the same length @ dim 0. :param batch_size: batch size to load. :param shuffle: if True, shuffle the data in-place* whenever an     iterator is created out of this object. :returns: A FastTensorDataLoader.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def __init__(self, *tensors: Tensor, batch_size: int = 32, shuffle: bool = False):\n    \"\"\"\n    Initialize a FastTensorDataLoader.\n    :param *tensors: tensors to store. Must have the same length @ dim 0.\n    :param batch_size: batch size to load.\n    :param shuffle: if True, shuffle the data *in-place* whenever an\n        iterator is created out of this object.\n    :returns: A FastTensorDataLoader.\n    \"\"\"\n    assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n    self.tensors = tensors\n\n    self.dataset_len = self.tensors[0].shape[0]\n    self.batch_size = batch_size\n    self.shuffle = shuffle\n\n    # Calculate # batches\n    n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n    if remainder &gt; 0:\n        n_batches += 1\n    self.n_batches = n_batches\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.MLP","title":"MLP","text":"<p>               Bases: <code>Module</code></p> <p>The MLP model used in [gorishniy2021revisiting].</p> <p>The following scheme describes the architecture:</p> <p>.. code-block:: text</p> <pre><code>  MLP: (in) -&gt; Block -&gt; ... -&gt; Block -&gt; Linear -&gt; (out)\nBlock: (in) -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; (out)\n</code></pre> <p>Examples:</p> <p>.. testcode::</p> <pre><code>x = torch.randn(4, 2)\nmodule = MLP.make_baseline(x.shape[1], [3, 5], 0.1, 1)\nassert module(x).shape == (len(x), 1)\n</code></pre> References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class MLP(nn.Module):\n    \"\"\"The MLP model used in [gorishniy2021revisiting].\n\n    The following scheme describes the architecture:\n\n    .. code-block:: text\n\n          MLP: (in) -&gt; Block -&gt; ... -&gt; Block -&gt; Linear -&gt; (out)\n        Block: (in) -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; (out)\n\n    Examples:\n        .. testcode::\n\n            x = torch.randn(4, 2)\n            module = MLP.make_baseline(x.shape[1], [3, 5], 0.1, 1)\n            assert module(x).shape == (len(x), 1)\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n\n    class Block(nn.Module):\n        \"\"\"The main building block of `MLP`.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            d_in: int,\n            d_out: int,\n            bias: bool,\n            activation: ModuleType,\n            dropout: float,\n        ) -&gt; None:\n            super().__init__()\n            self.linear = nn.Linear(d_in, d_out, bias)\n            self.activation = _make_nn_module(activation)\n            self.dropout = nn.Dropout(dropout)\n\n        def forward(self, x: Tensor) -&gt; Tensor:\n            return self.dropout(self.activation(self.linear(x)))\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        d_layers: list[int],\n        dropouts: float | list[float],\n        activation: str | Callable[[], nn.Module],\n        d_out: int,\n    ) -&gt; None:\n        \"\"\"\n        Note:\n            `make_baseline` is the recommended constructor.\n        \"\"\"\n        super().__init__()\n        if isinstance(dropouts, float):\n            dropouts = [dropouts] * len(d_layers)\n        assert len(d_layers) == len(dropouts)\n        assert activation not in [\"ReGLU\", \"GEGLU\"]\n\n        self.blocks = nn.ModuleList(\n            [\n                MLP.Block(\n                    d_in=d_layers[i - 1] if i else d_in,\n                    d_out=d,\n                    bias=True,\n                    activation=activation,\n                    dropout=dropout,\n                )\n                for i, (d, dropout) in enumerate(zip(d_layers, dropouts))\n            ]\n        )\n        self.head = nn.Linear(d_layers[-1] if d_layers else d_in, d_out)\n\n    @classmethod\n    def make_baseline(\n        cls,\n        d_in: int,\n        d_layers: list[int],\n        dropout: float,\n        d_out: int,\n    ) -&gt; Self:\n        \"\"\"Create a \"baseline\" `MLP`.\n\n        This variation of MLP was used in [gorishniy2021revisiting]. Features:\n\n        * all linear layers except for the first one and the last one are of the same dimension\n        * the dropout rate is the same for all dropout layers\n\n        Args:\n            d_in: the input size\n            d_layers: the dimensions of the linear layers. If there are more than two\n                layers, then all of them except for the first and the last ones must\n                have the same dimension.\n            dropout: the dropout rate for all hidden layers\n            d_out: the output size\n        Returns:\n            MLP\n\n        References:\n            * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n            Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n        \"\"\"\n        assert isinstance(dropout, float)\n        if len(d_layers) &gt; 2:\n            assert len(set(d_layers[1:-1])) == 1, (\n                \"if d_layers contains more than two elements, then\"\n                \" all elements except for the first and the last ones must be equal.\"\n            )\n        return cls(\n            d_in=d_in,\n            d_layers=d_layers,\n            dropouts=dropout,\n            activation=\"ReLU\",\n            d_out=d_out,\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = x.float()\n        for block in self.blocks:\n            x = block(x)\n        return self.head(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.MLP.Block","title":"Block","text":"<p>               Bases: <code>Module</code></p> <p>The main building block of <code>MLP</code>.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class Block(nn.Module):\n    \"\"\"The main building block of `MLP`.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        d_out: int,\n        bias: bool,\n        activation: ModuleType,\n        dropout: float,\n    ) -&gt; None:\n        super().__init__()\n        self.linear = nn.Linear(d_in, d_out, bias)\n        self.activation = _make_nn_module(activation)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        return self.dropout(self.activation(self.linear(x)))\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.MLP.__init__","title":"__init__","text":"<pre><code>__init__(*, d_in, d_layers, dropouts, activation, d_out)\n</code></pre> Note <p><code>make_baseline</code> is the recommended constructor.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def __init__(\n    self,\n    *,\n    d_in: int,\n    d_layers: list[int],\n    dropouts: float | list[float],\n    activation: str | Callable[[], nn.Module],\n    d_out: int,\n) -&gt; None:\n    \"\"\"\n    Note:\n        `make_baseline` is the recommended constructor.\n    \"\"\"\n    super().__init__()\n    if isinstance(dropouts, float):\n        dropouts = [dropouts] * len(d_layers)\n    assert len(d_layers) == len(dropouts)\n    assert activation not in [\"ReGLU\", \"GEGLU\"]\n\n    self.blocks = nn.ModuleList(\n        [\n            MLP.Block(\n                d_in=d_layers[i - 1] if i else d_in,\n                d_out=d,\n                bias=True,\n                activation=activation,\n                dropout=dropout,\n            )\n            for i, (d, dropout) in enumerate(zip(d_layers, dropouts))\n        ]\n    )\n    self.head = nn.Linear(d_layers[-1] if d_layers else d_in, d_out)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.MLP.make_baseline","title":"make_baseline  <code>classmethod</code>","text":"<pre><code>make_baseline(d_in, d_layers, dropout, d_out)\n</code></pre> <p>Create a \"baseline\" <code>MLP</code>.</p> <p>This variation of MLP was used in [gorishniy2021revisiting]. Features:</p> <ul> <li>all linear layers except for the first one and the last one are of the same dimension</li> <li>the dropout rate is the same for all dropout layers</li> </ul> <p>Parameters:</p> Name Type Description Default <code>d_in</code> <code>int</code> <p>the input size</p> required <code>d_layers</code> <code>list[int]</code> <p>the dimensions of the linear layers. If there are more than two layers, then all of them except for the first and the last ones must have the same dimension.</p> required <code>dropout</code> <code>float</code> <p>the dropout rate for all hidden layers</p> required <code>d_out</code> <code>int</code> <p>the output size</p> required <p>Returns:     MLP</p> References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>@classmethod\ndef make_baseline(\n    cls,\n    d_in: int,\n    d_layers: list[int],\n    dropout: float,\n    d_out: int,\n) -&gt; Self:\n    \"\"\"Create a \"baseline\" `MLP`.\n\n    This variation of MLP was used in [gorishniy2021revisiting]. Features:\n\n    * all linear layers except for the first one and the last one are of the same dimension\n    * the dropout rate is the same for all dropout layers\n\n    Args:\n        d_in: the input size\n        d_layers: the dimensions of the linear layers. If there are more than two\n            layers, then all of them except for the first and the last ones must\n            have the same dimension.\n        dropout: the dropout rate for all hidden layers\n        d_out: the output size\n    Returns:\n        MLP\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n    assert isinstance(dropout, float)\n    if len(d_layers) &gt; 2:\n        assert len(set(d_layers[1:-1])) == 1, (\n            \"if d_layers contains more than two elements, then\"\n            \" all elements except for the first and the last ones must be equal.\"\n        )\n    return cls(\n        d_in=d_in,\n        d_layers=d_layers,\n        dropouts=dropout,\n        activation=\"ReLU\",\n        d_out=d_out,\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ResNet","title":"ResNet","text":"<p>               Bases: <code>Module</code></p> <p>The ResNet model used in [gorishniy2021revisiting].</p> <p>The following scheme describes the architecture: .. code-block:: text     ResNet: (in) -&gt; Linear -&gt; Block -&gt; ... -&gt; Block -&gt; Head -&gt; (out)              |-&gt; Norm -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; Linear -&gt; Dropout -&gt;|              |                                                                  |      Block: (in) ------------------------------------------------------------&gt; Add -&gt; (out)       Head: (in) -&gt; Norm -&gt; Activation -&gt; Linear -&gt; (out)</p> <p>Examples:</p> <p>.. testcode::     x = torch.randn(4, 2)     module = ResNet.make_baseline(         d_in=x.shape[1],         n_blocks=2,         d_main=3,         d_hidden=4,         dropout_first=0.25,         dropout_second=0.0,         d_out=1     )     assert module(x).shape == (len(x), 1)</p> References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class ResNet(nn.Module):\n    \"\"\"\n    The ResNet model used in [gorishniy2021revisiting].\n\n    The following scheme describes the architecture:\n    .. code-block:: text\n        ResNet: (in) -&gt; Linear -&gt; Block -&gt; ... -&gt; Block -&gt; Head -&gt; (out)\n                 |-&gt; Norm -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; Linear -&gt; Dropout -&gt;|\n                 |                                                                  |\n         Block: (in) ------------------------------------------------------------&gt; Add -&gt; (out)\n          Head: (in) -&gt; Norm -&gt; Activation -&gt; Linear -&gt; (out)\n\n    Examples:\n        .. testcode::\n            x = torch.randn(4, 2)\n            module = ResNet.make_baseline(\n                d_in=x.shape[1],\n                n_blocks=2,\n                d_main=3,\n                d_hidden=4,\n                dropout_first=0.25,\n                dropout_second=0.0,\n                d_out=1\n            )\n            assert module(x).shape == (len(x), 1)\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n\n    class Block(nn.Module):\n        \"\"\"The main building block of `ResNet`.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            d_main: int,\n            d_hidden: int,\n            bias_first: bool,\n            bias_second: bool,\n            dropout_first: float,\n            dropout_second: float,\n            normalization: ModuleType,\n            activation: ModuleType,\n            skip_connection: bool,\n        ) -&gt; None:\n            super().__init__()\n            self.normalization = _make_nn_module(normalization, d_main)\n            self.linear_first = nn.Linear(d_main, d_hidden, bias_first)\n            self.activation = _make_nn_module(activation)\n            self.dropout_first = nn.Dropout(dropout_first)\n            self.linear_second = nn.Linear(d_hidden, d_main, bias_second)\n            self.dropout_second = nn.Dropout(dropout_second)\n            self.skip_connection = skip_connection\n\n        def forward(self, x: Tensor) -&gt; Tensor:\n            x_input = x\n            x = self.normalization(x)\n            x = self.linear_first(x)\n            x = self.activation(x)\n            x = self.dropout_first(x)\n            x = self.linear_second(x)\n            x = self.dropout_second(x)\n            if self.skip_connection:\n                x = x_input + x\n            return x\n\n    class Head(nn.Module):\n        \"\"\"The final module of `ResNet`.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            d_in: int,\n            d_out: int,\n            bias: bool,\n            normalization: ModuleType,\n            activation: ModuleType,\n        ) -&gt; None:\n            super().__init__()\n            self.normalization = _make_nn_module(normalization, d_in)\n            self.activation = _make_nn_module(activation)\n            self.linear = nn.Linear(d_in, d_out, bias)\n\n        def forward(self, x: Tensor) -&gt; Tensor:\n            if self.normalization is not None:\n                x = self.normalization(x)\n            x = self.activation(x)\n            return self.linear(x)\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        n_blocks: int,\n        d_main: int,\n        d_hidden: int,\n        dropout_first: float,\n        dropout_second: float,\n        normalization: ModuleType,\n        activation: ModuleType,\n        d_out: int,\n    ) -&gt; None:\n        \"\"\"\n        Note:\n            `make_baseline` is the recommended constructor.\n        \"\"\"\n        super().__init__()\n\n        self.first_layer = nn.Linear(d_in, d_main)\n        if d_main is None:\n            d_main = d_in\n        self.blocks = nn.Sequential(\n            *[\n                ResNet.Block(\n                    d_main=d_main,\n                    d_hidden=d_hidden,\n                    bias_first=True,\n                    bias_second=True,\n                    dropout_first=dropout_first,\n                    dropout_second=dropout_second,\n                    normalization=normalization,\n                    activation=activation,\n                    skip_connection=True,\n                )\n                for _ in range(n_blocks)\n            ]\n        )\n        self.head = ResNet.Head(\n            d_in=d_main,\n            d_out=d_out,\n            bias=True,\n            normalization=normalization,\n            activation=activation,\n        )\n\n    @classmethod\n    def make_baseline(\n        cls,\n        *,\n        d_in: int,\n        n_blocks: int,\n        d_main: int,\n        d_hidden: int,\n        dropout_first: float,\n        dropout_second: float,\n        d_out: int,\n    ) -&gt; Self:\n        \"\"\"\n        Create a \"baseline\" `ResNet`. This variation of ResNet was used in [gorishniy2021revisiting].\n\n        Args:\n            d_in: the input size\n            n_blocks: the number of Blocks\n            d_main: the input size (or, equivalently, the output size) of each Block\n            d_hidden: the output size of the first linear layer in each Block\n            dropout_first: the dropout rate of the first dropout layer in each Block.\n            dropout_second: the dropout rate of the second dropout layer in each Block.\n            d_out: Output dimension.\n\n        References:\n            * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n            Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n        \"\"\"\n        return cls(\n            d_in=d_in,\n            n_blocks=n_blocks,\n            d_main=d_main,\n            d_hidden=d_hidden,\n            dropout_first=dropout_first,\n            dropout_second=dropout_second,\n            normalization=\"BatchNorm1d\",\n            activation=\"ReLU\",\n            d_out=d_out,\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = x.float()\n        x = self.first_layer(x)\n        x = self.blocks(x)\n        return self.head(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ResNet.Block","title":"Block","text":"<p>               Bases: <code>Module</code></p> <p>The main building block of <code>ResNet</code>.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class Block(nn.Module):\n    \"\"\"The main building block of `ResNet`.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        d_main: int,\n        d_hidden: int,\n        bias_first: bool,\n        bias_second: bool,\n        dropout_first: float,\n        dropout_second: float,\n        normalization: ModuleType,\n        activation: ModuleType,\n        skip_connection: bool,\n    ) -&gt; None:\n        super().__init__()\n        self.normalization = _make_nn_module(normalization, d_main)\n        self.linear_first = nn.Linear(d_main, d_hidden, bias_first)\n        self.activation = _make_nn_module(activation)\n        self.dropout_first = nn.Dropout(dropout_first)\n        self.linear_second = nn.Linear(d_hidden, d_main, bias_second)\n        self.dropout_second = nn.Dropout(dropout_second)\n        self.skip_connection = skip_connection\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x_input = x\n        x = self.normalization(x)\n        x = self.linear_first(x)\n        x = self.activation(x)\n        x = self.dropout_first(x)\n        x = self.linear_second(x)\n        x = self.dropout_second(x)\n        if self.skip_connection:\n            x = x_input + x\n        return x\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ResNet.Head","title":"Head","text":"<p>               Bases: <code>Module</code></p> <p>The final module of <code>ResNet</code>.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class Head(nn.Module):\n    \"\"\"The final module of `ResNet`.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        d_out: int,\n        bias: bool,\n        normalization: ModuleType,\n        activation: ModuleType,\n    ) -&gt; None:\n        super().__init__()\n        self.normalization = _make_nn_module(normalization, d_in)\n        self.activation = _make_nn_module(activation)\n        self.linear = nn.Linear(d_in, d_out, bias)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        if self.normalization is not None:\n            x = self.normalization(x)\n        x = self.activation(x)\n        return self.linear(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ResNet.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    d_in,\n    n_blocks,\n    d_main,\n    d_hidden,\n    dropout_first,\n    dropout_second,\n    normalization,\n    activation,\n    d_out,\n)\n</code></pre> Note <p><code>make_baseline</code> is the recommended constructor.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def __init__(\n    self,\n    *,\n    d_in: int,\n    n_blocks: int,\n    d_main: int,\n    d_hidden: int,\n    dropout_first: float,\n    dropout_second: float,\n    normalization: ModuleType,\n    activation: ModuleType,\n    d_out: int,\n) -&gt; None:\n    \"\"\"\n    Note:\n        `make_baseline` is the recommended constructor.\n    \"\"\"\n    super().__init__()\n\n    self.first_layer = nn.Linear(d_in, d_main)\n    if d_main is None:\n        d_main = d_in\n    self.blocks = nn.Sequential(\n        *[\n            ResNet.Block(\n                d_main=d_main,\n                d_hidden=d_hidden,\n                bias_first=True,\n                bias_second=True,\n                dropout_first=dropout_first,\n                dropout_second=dropout_second,\n                normalization=normalization,\n                activation=activation,\n                skip_connection=True,\n            )\n            for _ in range(n_blocks)\n        ]\n    )\n    self.head = ResNet.Head(\n        d_in=d_main,\n        d_out=d_out,\n        bias=True,\n        normalization=normalization,\n        activation=activation,\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ResNet.make_baseline","title":"make_baseline  <code>classmethod</code>","text":"<pre><code>make_baseline(\n    *,\n    d_in,\n    n_blocks,\n    d_main,\n    d_hidden,\n    dropout_first,\n    dropout_second,\n    d_out,\n)\n</code></pre> <p>Create a \"baseline\" <code>ResNet</code>. This variation of ResNet was used in [gorishniy2021revisiting].</p> <p>Parameters:</p> Name Type Description Default <code>d_in</code> <code>int</code> <p>the input size</p> required <code>n_blocks</code> <code>int</code> <p>the number of Blocks</p> required <code>d_main</code> <code>int</code> <p>the input size (or, equivalently, the output size) of each Block</p> required <code>d_hidden</code> <code>int</code> <p>the output size of the first linear layer in each Block</p> required <code>dropout_first</code> <code>float</code> <p>the dropout rate of the first dropout layer in each Block.</p> required <code>dropout_second</code> <code>float</code> <p>the dropout rate of the second dropout layer in each Block.</p> required <code>d_out</code> <code>int</code> <p>Output dimension.</p> required References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>@classmethod\ndef make_baseline(\n    cls,\n    *,\n    d_in: int,\n    n_blocks: int,\n    d_main: int,\n    d_hidden: int,\n    dropout_first: float,\n    dropout_second: float,\n    d_out: int,\n) -&gt; Self:\n    \"\"\"\n    Create a \"baseline\" `ResNet`. This variation of ResNet was used in [gorishniy2021revisiting].\n\n    Args:\n        d_in: the input size\n        n_blocks: the number of Blocks\n        d_main: the input size (or, equivalently, the output size) of each Block\n        d_hidden: the output size of the first linear layer in each Block\n        dropout_first: the dropout rate of the first dropout layer in each Block.\n        dropout_second: the dropout rate of the second dropout layer in each Block.\n        d_out: Output dimension.\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n    return cls(\n        d_in=d_in,\n        n_blocks=n_blocks,\n        d_main=d_main,\n        d_hidden=d_hidden,\n        dropout_first=dropout_first,\n        dropout_second=dropout_second,\n        normalization=\"BatchNorm1d\",\n        activation=\"ReLU\",\n        d_out=d_out,\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ReGLU","title":"ReGLU","text":"<p>               Bases: <code>Module</code></p> <p>The ReGLU activation function from [shazeer2020glu].</p> <p>Examples:</p> <p>.. testcode::</p> <pre><code>module = ReGLU()\nx = torch.randn(3, 4)\nassert module(x).shape == (3, 2)\n</code></pre> References <ul> <li>[shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class ReGLU(nn.Module):\n    \"\"\"The ReGLU activation function from [shazeer2020glu].\n\n    Examples:\n        .. testcode::\n\n            module = ReGLU()\n            x = torch.randn(3, 4)\n            assert module(x).shape == (3, 2)\n\n    References:\n        * [shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        # ruff: noqa: D102\n        return reglu(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.GEGLU","title":"GEGLU","text":"<p>               Bases: <code>Module</code></p> <p>The GEGLU activation function from [shazeer2020glu].</p> <p>Examples:</p> <p>.. testcode::</p> <pre><code>module = GEGLU()\nx = torch.randn(3, 4)\nassert module(x).shape == (3, 2)\n</code></pre> References <ul> <li>[shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class GEGLU(nn.Module):\n    \"\"\"The GEGLU activation function from [shazeer2020glu].\n\n    Examples:\n        .. testcode::\n\n            module = GEGLU()\n            x = torch.randn(3, 4)\n            assert module(x).shape == (3, 2)\n\n    References:\n        * [shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        # ruff: noqa: D102\n        return geglu(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.make_dataset_from_df","title":"make_dataset_from_df","text":"<pre><code>make_dataset_from_df(\n    df, T, is_y_cond, df_info, ratios=None, std=0\n)\n</code></pre> <p>The order of the generated dataset: (y, X_num, X_cat).</p> is_y_cond <p>concat: y is concatenated to X, the model learn a joint distribution of (y, X) embedding: y is not concatenated to X. During computations, y is embedded     and added to the latent vector of X none: y column is completely ignored</p> <p>How does is_y_cond affect the generation of y? is_y_cond:     concat: the model synthesizes (y, X) directly, so y is just the first column     embedding: y is first sampled using empirical distribution of y. The model only         synthesizes X. When returning the generated data, we return the generated X         and the sampled y. (y is sampled from empirical distribution, instead of being         generated by the model)         Note that in this way, y is still not independent of X, because the model has been         adding the embedding of y to the latent vector of X during computations.     none:         y is synthesized using y's empirical distribution. X is generated by the model.         In this case, y is completely independent of X.</p> <p>Note: For now, n_classes has to be set to 0. This is because our matrix is the concatenation of (X_num, X_cat). In this case, if we have is_y_cond == 'concat', we can guarantee that y is the first column of the matrix. However, if we have n_classes &gt; 0, then y is not the first column of the matrix.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def make_dataset_from_df(\n    # ruff: noqa: PLR0915, PLR0912\n    df: pd.DataFrame,\n    T: Transformations,\n    is_y_cond: str,\n    df_info: pd.DataFrame,\n    ratios: list[float] | None = None,\n    std: float = 0,\n) -&gt; tuple[Dataset, dict[int, LabelEncoder], list[int]]:\n    \"\"\"\n    The order of the generated dataset: (y, X_num, X_cat).\n\n    is_y_cond:\n        concat: y is concatenated to X, the model learn a joint distribution of (y, X)\n        embedding: y is not concatenated to X. During computations, y is embedded\n            and added to the latent vector of X\n        none: y column is completely ignored\n\n    How does is_y_cond affect the generation of y?\n    is_y_cond:\n        concat: the model synthesizes (y, X) directly, so y is just the first column\n        embedding: y is first sampled using empirical distribution of y. The model only\n            synthesizes X. When returning the generated data, we return the generated X\n            and the sampled y. (y is sampled from empirical distribution, instead of being\n            generated by the model)\n            Note that in this way, y is still not independent of X, because the model has been\n            adding the embedding of y to the latent vector of X during computations.\n        none:\n            y is synthesized using y's empirical distribution. X is generated by the model.\n            In this case, y is completely independent of X.\n\n    Note: For now, n_classes has to be set to 0. This is because our matrix is the concatenation\n    of (X_num, X_cat). In this case, if we have is_y_cond == 'concat', we can guarantee that y\n    is the first column of the matrix.\n    However, if we have n_classes &gt; 0, then y is not the first column of the matrix.\n    \"\"\"\n    if ratios is None:\n        ratios = [0.7, 0.2, 0.1]\n\n    train_val_df, test_df = train_test_split(df, test_size=ratios[2], random_state=42)\n    train_df, val_df = train_test_split(train_val_df, test_size=ratios[1] / (ratios[0] + ratios[1]), random_state=42)\n\n    cat_column_orders = []\n    num_column_orders = []\n    index_to_column = list(df.columns)\n    column_to_index = {col: i for i, col in enumerate(index_to_column)}\n\n    if df_info[\"n_classes\"] &gt; 0:\n        X_cat: dict[str, np.ndarray] | None = {} if df_info[\"cat_cols\"] is not None or is_y_cond == \"concat\" else None\n        X_num: dict[str, np.ndarray] | None = {} if df_info[\"num_cols\"] is not None else None\n        y = {}\n\n        cat_cols_with_y = []\n        if df_info[\"cat_cols\"] is not None:\n            cat_cols_with_y += df_info[\"cat_cols\"]\n        if is_y_cond == \"concat\":\n            cat_cols_with_y = [df_info[\"y_col\"]] + cat_cols_with_y\n\n        if len(cat_cols_with_y) &gt; 0:\n            X_cat[\"train\"] = train_df[cat_cols_with_y].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"val\"] = val_df[cat_cols_with_y].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"test\"] = test_df[cat_cols_with_y].to_numpy(dtype=np.str_)  # type: ignore[index]\n\n        y[\"train\"] = train_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"val\"] = val_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"test\"] = test_df[df_info[\"y_col\"]].values.astype(np.float32)\n\n        if df_info[\"num_cols\"] is not None:\n            X_num[\"train\"] = train_df[df_info[\"num_cols\"]].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"val\"] = val_df[df_info[\"num_cols\"]].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"test\"] = test_df[df_info[\"num_cols\"]].values.astype(np.float32)  # type: ignore[index]\n\n        cat_column_orders = [column_to_index[col] for col in cat_cols_with_y]\n        num_column_orders = [column_to_index[col] for col in df_info[\"num_cols\"]]\n\n    else:\n        X_cat = {} if df_info[\"cat_cols\"] is not None else None\n        X_num = {} if df_info[\"num_cols\"] is not None or is_y_cond == \"concat\" else None\n        y = {}\n\n        num_cols_with_y = []\n        if df_info[\"num_cols\"] is not None:\n            num_cols_with_y += df_info[\"num_cols\"]\n        if is_y_cond == \"concat\":\n            num_cols_with_y = [df_info[\"y_col\"]] + num_cols_with_y\n\n        if len(num_cols_with_y) &gt; 0:\n            X_num[\"train\"] = train_df[num_cols_with_y].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"val\"] = val_df[num_cols_with_y].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"test\"] = test_df[num_cols_with_y].values.astype(np.float32)  # type: ignore[index]\n\n        y[\"train\"] = train_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"val\"] = val_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"test\"] = test_df[df_info[\"y_col\"]].values.astype(np.float32)\n\n        if df_info[\"cat_cols\"] is not None:\n            X_cat[\"train\"] = train_df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"val\"] = val_df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"test\"] = test_df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)  # type: ignore[index]\n\n        cat_column_orders = [column_to_index[col] for col in df_info[\"cat_cols\"]]\n        num_column_orders = [column_to_index[col] for col in num_cols_with_y]\n\n    column_orders = num_column_orders + cat_column_orders\n    column_orders = [index_to_column[index] for index in column_orders]\n\n    label_encoders = {}\n    if X_cat is not None and len(df_info[\"cat_cols\"]) &gt; 0:\n        X_cat_all = np.vstack((X_cat[\"train\"], X_cat[\"val\"], X_cat[\"test\"]))\n        X_cat_converted = []\n        for col_index in range(X_cat_all.shape[1]):\n            label_encoder = LabelEncoder()\n            X_cat_converted.append(label_encoder.fit_transform(X_cat_all[:, col_index]).astype(float))\n            if std &gt; 0:\n                # add noise\n                X_cat_converted[-1] += np.random.normal(0, std, X_cat_converted[-1].shape)\n            label_encoders[col_index] = label_encoder\n\n        X_cat_converted = np.vstack(X_cat_converted).T  # type: ignore[assignment]\n\n        train_num = X_cat[\"train\"].shape[0]\n        val_num = X_cat[\"val\"].shape[0]\n        # test_num = X_cat[\"test\"].shape[0]\n\n        X_cat[\"train\"] = X_cat_converted[:train_num, :]  # type: ignore[call-overload]\n        X_cat[\"val\"] = X_cat_converted[train_num : train_num + val_num, :]  # type: ignore[call-overload]\n        X_cat[\"test\"] = X_cat_converted[train_num + val_num :, :]  # type: ignore[call-overload]\n\n        if X_num and len(X_num) &gt; 0:\n            X_num[\"train\"] = np.concatenate((X_num[\"train\"], X_cat[\"train\"]), axis=1)\n            X_num[\"val\"] = np.concatenate((X_num[\"val\"], X_cat[\"val\"]), axis=1)\n            X_num[\"test\"] = np.concatenate((X_num[\"test\"], X_cat[\"test\"]), axis=1)\n        else:\n            X_num = X_cat\n            X_cat = None\n\n    D = Dataset(\n        # ruff: noqa: N806\n        X_num,\n        None,\n        y,\n        y_info={},\n        task_type=TaskType(df_info[\"task_type\"]),\n        n_classes=df_info[\"n_classes\"],\n    )\n\n    return transform_dataset(D, T, None), label_encoders, column_orders\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.timestep_embedding","title":"timestep_embedding","text":"<pre><code>timestep_embedding(timesteps, dim, max_period=10000)\n</code></pre> <p>Create sinusoidal timestep embeddings.</p> <p>:param timesteps: a 1-D Tensor of N indices, one per batch element.                   These may be fractional. :param dim: the dimension of the output. :param max_period: controls the minimum frequency of the embeddings. :return: an [N x dim] Tensor of positional embeddings.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def timestep_embedding(timesteps: Tensor, dim: int, max_period: int = 10000) -&gt; Tensor:\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2\n    freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(\n        device=timesteps.device\n    )\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.reglu","title":"reglu","text":"<pre><code>reglu(x)\n</code></pre> <p>The ReGLU activation function from [1].</p> References <p>[1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def reglu(x: Tensor) -&gt; Tensor:\n    \"\"\"The ReGLU activation function from [1].\n\n    References:\n        [1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n    assert x.shape[-1] % 2 == 0\n    a, b = x.chunk(2, dim=-1)\n    return a * F.relu(b)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.geglu","title":"geglu","text":"<pre><code>geglu(x)\n</code></pre> <p>The GEGLU activation function from [1].</p> References <p>[1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def geglu(x: Tensor) -&gt; Tensor:\n    \"\"\"The GEGLU activation function from [1].\n\n    References:\n        [1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n    assert x.shape[-1] % 2 == 0\n    a, b = x.chunk(2, dim=-1)\n    return a * F.gelu(b)\n</code></pre>"},{"location":"user_guide/","title":"User Guide","text":""},{"location":"user_guide/#pyprojecttoml-file-and-dependency-management","title":"pyproject.toml file and dependency management","text":"<p>If your project doesn't have a pyproject.toml file, simply copy the one from the template and update file according to your project.</p> <p>For managing dependencies, this template makes use of uv, which according to some benchmarks is faster than alternative like Poetry (which our original AI Engineering Template makes use of).</p> <p>Hence, be sure to install uv in order to to setup the development virtual environment. Instructions for installing uv can be found here. Note that uv supports optional dependency groups which helps to manage dependencies for different parts of development such as <code>documentation</code>, <code>testing</code>, etc. The core dependencies are installed using the command:</p> <pre><code>uv sync\n</code></pre> <p>Additional dependency groups can be installed using the <code>--group</code> flag followed by the group name. For example:</p> <pre><code>uv sync --all-extras --group docs --group test\n</code></pre> <p>mypy configuration options</p> <p>By default, the <code>mypy</code> configuration in the <code>pyproject.toml</code> disallows subclassing the <code>Any</code> type - <code>allow_subclassing_any = false</code>. In cases where the type checker is not able to determine the types of objects in some external library (e.g. <code>PyTorch</code>), it will treat them as <code>Any</code> and raise errors. If your codebase has many of such cases, you can set <code>allow_subclassing_any = true</code> in the <code>mypy</code> configuration or remove it entirely to use the default value (which is <code>true</code>). For example, in a <code>PyTorch</code> project, subclassing <code>nn.Module</code> will raise errors if <code>allow_subclassing_any</code> is set to <code>false</code>.</p>"},{"location":"user_guide/#pre-commit","title":"pre-commit","text":"<p>You can use pre-commit to run pre-commit hooks (code checks, liniting, etc.) when you run <code>git commit</code> and commit your code. Simply copy the <code>.pre-commit-config.yaml</code> file to the root of the repository and install the test dependencies which installs pre-commit. Then run:</p> <pre><code>pre-commit install\n</code></pre> <p>If you prefer to not enforce using pre-commit every time you run <code>git commit</code>, you will have to run <code>pre-commit run --all-files</code> from the command line before you commit your code.</p> <p>hook configuration</p> <p>Some of the pre-commit hooks use supported hooks from the web.</p> <p>For some others, they are locally installed and hence use the python virtual environment locally. If <code>language</code> is set to <code>python</code>, each time the hook is installed, a separate python virtual environment is created and you can specify dependencies needed using <code>additional_dependencies</code>.</p> <p>If <code>language</code> is set to <code>system</code>, the activated python virtual environment is used and and hence you have to ensure that the required dependencies and their versions are correctly installed.</p> <pre><code>  - repo: local\n    hooks:\n    - id: pytest\n      name: pytest\n      entry: python3 -m pytest -m \"not integration_test\"\n      language: python/system # set according to your project needs\n</code></pre> <p>typos</p> <p>The typos pre-commit hook is used to check for common spelling mistakes in the codebase. While useful, it may require some configuration to ignore certain words or phrases that are not typos. You can configure the typos hook in the <code>pyproject.toml</code> file. In a large codebase, it may be useful to disable the typos hook and only run it occasionally on the entire codebase.</p>"},{"location":"user_guide/#pre-commit-ci","title":"pre-commit ci","text":"<p>Instead of fixing pre-commit errors manually, a CI to fix them as well as update pre-commit hooks periodically can be enabled for your repository. Please check pre-commit.ci and add your repository. The configuration for <code>pre-commit.ci</code> can be added to the <code>.pre-commit-config.yaml</code> file.</p>"},{"location":"user_guide/#documentation","title":"documentation","text":"<p>If your project doesn't have documentation, copy the directory named <code>docs</code> to the root directory of your repository. This template uses MkDocs with the Material for MkDocs theme.</p> <p>In order to build the documentation, install the documentation dependencies as mentioned in the previous section, then run the command:</p> <pre><code>mkdocs build\n</code></pre> <p>If you're making changes to the docs, and want to serve them locally on your machine, then you can use this command instead:</p> <pre><code>mkdocs serve\n</code></pre> <p>The above will launch the docs locally on <code>http://127.0.0.1:8000</code>, which you can enter into your browser of choice. Conveniently, this process also watches for any changes you make to the docs and will update them as they occur.</p> <p>You can configure the documentation by updating the <code>mkdocs.yml</code> file at the root of your repository. The markdown files in the <code>docs</code> directory can be updated to reflect the project's documentation.</p>"},{"location":"user_guide/#github-actions","title":"github actions","text":"<p>The template consists of some github action continuous integration workflows that you can add to your repository.</p> <p>The available workflows are:</p> <ul> <li>code checks: Static code analysis, code formatting and unit tests</li> <li>documentation: Project documentation including example API reference</li> <li>integration tests: Integration tests</li> <li>publish: Publishing python package to PyPI. Create a <code>PYPI_API_TOKEN</code> and add it to the repository's actions secret variables in order to publish PyPI packages when new software releases are created on Github.</li> </ul>"}]}