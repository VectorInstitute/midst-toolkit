{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MIDST Toolkit Repository","text":""},{"location":"#midst-toolkit","title":"MIDST Toolkit","text":"<p>A toolkit for facilitating MIA resiliency testing on diffusion-model-based synthetic tabular data. Many of the attacks included in this toolkit are based on the most success ones used in the 2025 SaTML MIDST Competition.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#top-level-module","title":"Top Level Module","text":""},{"location":"api/#midst_toolkit","title":"midst_toolkit","text":"<p>PLACEHOLDER.</p>"},{"location":"api/#midst_toolkit.common","title":"common","text":""},{"location":"api/#midst_toolkit.common.enumerations","title":"enumerations","text":""},{"location":"api/#midst_toolkit.common.enumerations.TaskType","title":"TaskType","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>src/midst_toolkit/common/enumerations.py</code> <pre><code>class TaskType(Enum):\n    BINCLASS = \"binclass\"\n    MULTICLASS = \"multiclass\"\n    REGRESSION = \"regression\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string value of the enum.\"\"\"\n        return self.value\n</code></pre>"},{"location":"api/#midst_toolkit.common.enumerations.TaskType.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Return the string value of the enum.</p> Source code in <code>src/midst_toolkit/common/enumerations.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string value of the enum.\"\"\"\n    return self.value\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger","title":"logger","text":"<p>MIDST Toolkit Logger. Borrowed heavily from the Flower Labs logger.</p>"},{"location":"api/#midst_toolkit.common.logger.ConsoleHandler","title":"ConsoleHandler","text":"<p>               Bases: <code>StreamHandler</code></p> Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>class ConsoleHandler(StreamHandler):\n    def __init__(\n        self,\n        timestamps: bool = False,\n        json: bool = False,\n        colored: bool = True,\n        stream: TextIO | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Console handler that allows configurable formatting.\n\n        Args:\n            timestamps: Whether or not to include timestamps. Defaults to False.\n            json: Whether or not to accept json. Defaults to False.\n            colored: Whether or not to apply color to the logs. Defaults to True.\n            stream: To initialize the underlying StreamHandler. Defaults to None.\n        \"\"\"\n        super().__init__(stream)\n        self.timestamps = timestamps\n        self.json = json\n        self.colored = colored\n\n    def emit(self, record: LogRecord) -&gt; None:\n        \"\"\"\n        Console handler that emits the provided record.\n\n        Args:\n            record: Record to emit\n        \"\"\"\n        if self.json:\n            record.message = record.getMessage().replace(\"\\t\", \"\").strip()\n\n            # Check if the message is empty\n            if not record.message:\n                return\n\n        super().emit(record)\n\n    def format(self, record: LogRecord) -&gt; str:\n        \"\"\"\n        Format function that adds colors to log level.\n\n        Args:\n            record: Record to have color added\n\n        Returns:\n            String with color formatting corresponding to the log.\n        \"\"\"\n        seperator = \" \" * (8 - len(record.levelname))\n        if self.json:\n            log_fmt = \"{lvl='%(levelname)s', time='%(asctime)s', msg='%(message)s'}\"\n        else:\n            log_fmt = (\n                f\"{LOG_COLORS[record.levelname] if self.colored else ''}\"\n                f\"%(levelname)s {'%(asctime)s' if self.timestamps else ''}\"\n                f\"{LOG_COLORS['RESET'] if self.colored else ''}\"\n                f\": {seperator} %(message)s\"\n            )\n        formatter = logging.Formatter(log_fmt)\n        return formatter.format(record)\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.ConsoleHandler.__init__","title":"__init__","text":"<pre><code>__init__(\n    timestamps=False, json=False, colored=True, stream=None\n)\n</code></pre> <p>Console handler that allows configurable formatting.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>bool</code> <p>Whether or not to include timestamps. Defaults to False.</p> <code>False</code> <code>json</code> <code>bool</code> <p>Whether or not to accept json. Defaults to False.</p> <code>False</code> <code>colored</code> <code>bool</code> <p>Whether or not to apply color to the logs. Defaults to True.</p> <code>True</code> <code>stream</code> <code>TextIO | None</code> <p>To initialize the underlying StreamHandler. Defaults to None.</p> <code>None</code> Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def __init__(\n    self,\n    timestamps: bool = False,\n    json: bool = False,\n    colored: bool = True,\n    stream: TextIO | None = None,\n) -&gt; None:\n    \"\"\"\n    Console handler that allows configurable formatting.\n\n    Args:\n        timestamps: Whether or not to include timestamps. Defaults to False.\n        json: Whether or not to accept json. Defaults to False.\n        colored: Whether or not to apply color to the logs. Defaults to True.\n        stream: To initialize the underlying StreamHandler. Defaults to None.\n    \"\"\"\n    super().__init__(stream)\n    self.timestamps = timestamps\n    self.json = json\n    self.colored = colored\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.ConsoleHandler.emit","title":"emit","text":"<pre><code>emit(record)\n</code></pre> <p>Console handler that emits the provided record.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>LogRecord</code> <p>Record to emit</p> required Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def emit(self, record: LogRecord) -&gt; None:\n    \"\"\"\n    Console handler that emits the provided record.\n\n    Args:\n        record: Record to emit\n    \"\"\"\n    if self.json:\n        record.message = record.getMessage().replace(\"\\t\", \"\").strip()\n\n        # Check if the message is empty\n        if not record.message:\n            return\n\n    super().emit(record)\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.ConsoleHandler.format","title":"format","text":"<pre><code>format(record)\n</code></pre> <p>Format function that adds colors to log level.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>LogRecord</code> <p>Record to have color added</p> required <p>Returns:</p> Type Description <code>str</code> <p>String with color formatting corresponding to the log.</p> Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def format(self, record: LogRecord) -&gt; str:\n    \"\"\"\n    Format function that adds colors to log level.\n\n    Args:\n        record: Record to have color added\n\n    Returns:\n        String with color formatting corresponding to the log.\n    \"\"\"\n    seperator = \" \" * (8 - len(record.levelname))\n    if self.json:\n        log_fmt = \"{lvl='%(levelname)s', time='%(asctime)s', msg='%(message)s'}\"\n    else:\n        log_fmt = (\n            f\"{LOG_COLORS[record.levelname] if self.colored else ''}\"\n            f\"%(levelname)s {'%(asctime)s' if self.timestamps else ''}\"\n            f\"{LOG_COLORS['RESET'] if self.colored else ''}\"\n            f\": {seperator} %(message)s\"\n        )\n    formatter = logging.Formatter(log_fmt)\n    return formatter.format(record)\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.update_console_handler","title":"update_console_handler","text":"<pre><code>update_console_handler(\n    level=None, timestamps=None, colored=None\n)\n</code></pre> <p>Helper function for setting the proper logging.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int | str | None</code> <p>Level of the logger. Defaults to None.</p> <code>None</code> <code>timestamps</code> <code>bool | None</code> <p>Whether to include timestamps. Defaults to None.</p> <code>None</code> <code>colored</code> <code>bool | None</code> <p>Whether to apply color formatting. Defaults to None.</p> <code>None</code> Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def update_console_handler(\n    level: int | str | None = None,\n    timestamps: bool | None = None,\n    colored: bool | None = None,\n) -&gt; None:\n    \"\"\"\n    Helper function for setting the proper logging.\n\n    Args:\n        level: Level of the logger. Defaults to None.\n        timestamps: Whether to include timestamps. Defaults to None.\n        colored: Whether to apply color formatting. Defaults to None.\n    \"\"\"\n    for handler in logging.getLogger(LOGGER_NAME).handlers:\n        if isinstance(handler, ConsoleHandler):\n            if level is not None:\n                handler.setLevel(level)\n            if timestamps is not None:\n                handler.timestamps = timestamps\n            if colored is not None:\n                handler.colored = colored\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.configure","title":"configure","text":"<pre><code>configure(identifier, filename=None)\n</code></pre> <p>Configure logging to file.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>Identifier to front the logged string</p> required <code>filename</code> <code>str | None</code> <p>Name of the file producing the log, if desired. Defaults to None.</p> <code>None</code> Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def configure(identifier: str, filename: str | None = None) -&gt; None:\n    \"\"\"\n    Configure logging to file.\n\n    Args:\n        identifier: Identifier to front the logged string\n        filename: Name of the file producing the log, if desired. Defaults to None.\n    \"\"\"\n    # Create formatter\n    string_to_input = f\"{identifier} | %(levelname)s %(name)s %(asctime)s \"\n    string_to_input += \"| %(filename)s:%(lineno)d | %(message)s\"\n    formatter = logging.Formatter(string_to_input)\n\n    file_path = Path(filename) if filename else None\n\n    if file_path:\n        assert file_path.parent.exists(), \"Folder into which the logging file is to be inserted does not exist.\"\n        # Create file handler and log to disk\n        file_handler = logging.FileHandler(file_path)\n        file_handler.setLevel(logging.DEBUG)\n        file_handler.setFormatter(formatter)\n        TOOLKIT_LOGGER.addHandler(file_handler)\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.set_logger_propagation","title":"set_logger_propagation","text":"<pre><code>set_logger_propagation(child_logger, value=True)\n</code></pre> <p>Set the logger propagation attribute.</p> <p>Parameters:</p> Name Type Description Default <code>child_logger</code> <code>Logger</code> <p>Child logger object</p> required <code>value</code> <code>bool</code> <p>Boolean setting for propagation. If True, both parent and child logger display messages. Otherwise, only the child logger displays a message. This False setting prevents duplicate logs in Colab notebooks. Reference: https://stackoverflow.com/a/19561320. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Logger</code> <p>Child logger object with updated propagation setting</p> Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def set_logger_propagation(child_logger: logging.Logger, value: bool = True) -&gt; logging.Logger:\n    \"\"\"\n    Set the logger propagation attribute.\n\n    Args:\n        child_logger: Child logger object\n        value: Boolean setting for propagation. If True, both parent and child logger display messages. Otherwise,\n            only the child logger displays a message. This False setting prevents duplicate logs in Colab notebooks.\n            Reference: https://stackoverflow.com/a/19561320. Defaults to True.\n\n    Returns:\n        Child logger object with updated propagation setting\n    \"\"\"\n    child_logger.propagate = value\n    if not child_logger.propagate:\n        child_logger.log(logging.DEBUG, \"Logger propagate set to False\")\n    return child_logger\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.redirect_output","title":"redirect_output","text":"<pre><code>redirect_output(output_buffer)\n</code></pre> <p>Redirect stdout and stderr to text I/O buffer.</p> <p>Parameters:</p> Name Type Description Default <code>output_buffer</code> <code>StringIO</code> <p>output buffer to be directed to the I/O buffer</p> required Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def redirect_output(output_buffer: StringIO) -&gt; None:\n    \"\"\"\n    Redirect stdout and stderr to text I/O buffer.\n\n    Args:\n        output_buffer: output buffer to be directed to the I/O buffer\n    \"\"\"\n    sys.stdout = output_buffer\n    sys.stderr = output_buffer\n    console_handler.stream = sys.stdout\n</code></pre>"},{"location":"api/#midst_toolkit.core","title":"core","text":"<p>PLACEHOLDER.</p>"},{"location":"api/#midst_toolkit.core.logger","title":"logger","text":"<p>Logger copied from OpenAI baselines to avoid extra RL-based dependencies.</p> <p>https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/logger.py</p>"},{"location":"api/#midst_toolkit.core.logger.TensorBoardOutputFormat","title":"TensorBoardOutputFormat","text":"<p>               Bases: <code>KVWriter</code></p> <p>Dumps key/value pairs into TensorBoard's numeric format.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>class TensorBoardOutputFormat(KVWriter):\n    \"\"\"Dumps key/value pairs into TensorBoard's numeric format.\"\"\"\n\n    def __init__(self, dir: str):\n        os.makedirs(dir, exist_ok=True)\n        self.dir = dir\n        self.step = 1\n        prefix = \"events\"\n        path = osp.join(osp.abspath(dir), prefix)\n        import tensorflow as tf\n        from tensorflow.core.util import event_pb2\n        from tensorflow.python import pywrap_tensorflow\n        from tensorflow.python.util import compat\n\n        self.tf = tf\n        self.event_pb2 = event_pb2\n        self.pywrap_tensorflow = pywrap_tensorflow\n        self.writer = pywrap_tensorflow.EventsWriter(compat.as_bytes(path))\n\n    def writekvs(self, kvs: dict[str, Any]) -&gt; None:\n        def summary_val(k: str, v: Any) -&gt; Any:\n            kwargs = {\"tag\": k, \"simple_value\": float(v)}\n            return self.tf.Summary.Value(**kwargs)\n\n        summary = self.tf.Summary(value=[summary_val(k, v) for k, v in kvs.items()])\n        event = self.event_pb2.Event(wall_time=time.time(), summary=summary)\n        event.step = self.step  # is there any reason why you'd want to specify the step?\n        self.writer.WriteEvent(event)\n        self.writer.Flush()\n        self.step += 1\n\n    def close(self) -&gt; None:\n        if self.writer:\n            self.writer.Close()\n            self.writer = None\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.logkv","title":"logkv","text":"<pre><code>logkv(key, val)\n</code></pre> <p>Log a value of some diagnostic.</p> <p>Call this once for each diagnostic quantity, each iteration If called many times, last value will be used.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def logkv(key: str, val: Any) -&gt; None:\n    \"\"\"\n    Log a value of some diagnostic.\n\n    Call this once for each diagnostic quantity, each iteration\n    If called many times, last value will be used.\n    \"\"\"\n    get_current().logkv(key, val)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.logkv_mean","title":"logkv_mean","text":"<pre><code>logkv_mean(key, val)\n</code></pre> <p>The same as logkv(), but if called many times, values averaged.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def logkv_mean(key: str, val: Any) -&gt; None:\n    \"\"\"The same as logkv(), but if called many times, values averaged.\"\"\"\n    get_current().logkv_mean(key, val)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.logkvs","title":"logkvs","text":"<pre><code>logkvs(d)\n</code></pre> <p>Log a dictionary of key-value pairs.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def logkvs(d: dict[str, Any]) -&gt; None:\n    \"\"\"Log a dictionary of key-value pairs.\"\"\"\n    for k, v in d.items():\n        logkv(k, v)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.dumpkvs","title":"dumpkvs","text":"<pre><code>dumpkvs()\n</code></pre> <p>Write all of the diagnostics from the current iteration.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def dumpkvs() -&gt; dict[str, Any]:\n    \"\"\"Write all of the diagnostics from the current iteration.\"\"\"\n    return get_current().dumpkvs()\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.log","title":"log","text":"<pre><code>log(*args, level=INFO)\n</code></pre> <p>Logs the args in the desired level.</p> <p>Write the sequence of args, with no separators, to the console and output files (if you've configured an output file).</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def log(*args: Iterable[Any], level: int = INFO) -&gt; None:\n    \"\"\"\n    Logs the args in the desired level.\n\n    Write the sequence of args, with no separators, to the console and output\n    files (if you've configured an output file).\n    \"\"\"\n    get_current().log(*args, level=level)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.set_level","title":"set_level","text":"<pre><code>set_level(level)\n</code></pre> <p>Set logging threshold on current logger.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def set_level(level: int) -&gt; None:\n    \"\"\"Set logging threshold on current logger.\"\"\"\n    get_current().set_level(level)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.get_dir","title":"get_dir","text":"<pre><code>get_dir()\n</code></pre> <p>Get directory that log files are being written to.</p> <p>will be None if there is no output directory (i.e., if you didn't call start)</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def get_dir() -&gt; str:\n    \"\"\"\n    Get directory that log files are being written to.\n\n    will be None if there is no output directory (i.e., if you didn't call start)\n    \"\"\"\n    return get_current().get_dir()\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.profile","title":"profile","text":"<pre><code>profile(n)\n</code></pre> <p>Usage.</p> <p>@profile(\"my_func\") def my_func(): code</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def profile(n):\n    \"\"\"\n    Usage.\n\n    @profile(\"my_func\")\n    def my_func(): code\n    \"\"\"\n\n    def decorator_with_name(func):\n        def func_wrapper(*args, **kwargs):\n            with profile_kv(n):\n                return func(*args, **kwargs)\n\n        return func_wrapper\n\n    return decorator_with_name\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.mpi_weighted_mean","title":"mpi_weighted_mean","text":"<pre><code>mpi_weighted_mean(comm, local_name2valcount)\n</code></pre> <p>Copied from below.</p> <p>https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/common/mpi_util.py#L110 Perform a weighted average over dicts that are each on a different node Input: local_name2valcount: dict mapping key -&gt; (value, count) Returns: key -&gt; mean</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def mpi_weighted_mean(comm: Any, local_name2valcount: dict[str, tuple[float, float]]) -&gt; dict[str, float]:\n    \"\"\"\n    Copied from below.\n\n    https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/common/mpi_util.py#L110\n    Perform a weighted average over dicts that are each on a different node\n    Input: local_name2valcount: dict mapping key -&gt; (value, count)\n    Returns: key -&gt; mean\n    \"\"\"\n    all_name2valcount = comm.gather(local_name2valcount)\n    if comm.rank == 0:\n        name2sum: defaultdict[str, float] = defaultdict(float)\n        name2count: defaultdict[str, float] = defaultdict(float)\n        for n2vc in all_name2valcount:\n            for name, (val, count) in n2vc.items():\n                try:\n                    val = float(val)\n                except ValueError:\n                    if comm.rank == 0:\n                        warnings.warn(\"WARNING: tried to compute mean on non-float {}={}\".format(name, val))\n                        # ruff: noqa: B028\n                else:\n                    name2sum[name] += val * count\n                    name2count[name] += count\n        return {name: name2sum[name] / name2count[name] for name in name2sum}\n    return {}\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.configure","title":"configure","text":"<pre><code>configure(\n    dir=None, format_strs=None, comm=None, log_suffix=\"\"\n)\n</code></pre> <p>If comm is provided, average all numerical stats across that comm.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def configure(\n    dir: str | None = None,\n    format_strs: list[str] | None = None,\n    comm: Any | None = None,\n    log_suffix: str = \"\",\n) -&gt; None:\n    \"\"\"If comm is provided, average all numerical stats across that comm.\"\"\"\n    if dir is None:\n        dir = os.getenv(\"OPENAI_LOGDIR\")\n    if dir is None:\n        dir = osp.join(\n            tempfile.gettempdir(),\n            datetime.datetime.now().strftime(\"openai-%Y-%m-%d-%H-%M-%S-%f\"),\n        )\n    assert isinstance(dir, str)\n    dir = os.path.expanduser(dir)\n    os.makedirs(os.path.expanduser(dir), exist_ok=True)\n\n    rank = get_rank_without_mpi_import()\n    if rank &gt; 0:\n        log_suffix = log_suffix + \"-rank%03i\" % rank\n\n    if format_strs is None:\n        if rank == 0:\n            format_strs = os.getenv(\"OPENAI_LOG_FORMAT\", \"stdout,log,csv\").split(\",\")\n        else:\n            format_strs = os.getenv(\"OPENAI_LOG_FORMAT_MPI\", \"log\").split(\",\")\n    format_strs_filter = filter(None, format_strs)\n    output_formats = [make_output_format(f, dir, log_suffix) for f in format_strs_filter]\n\n    Logger.CURRENT = Logger(dir=dir, output_formats=output_formats, comm=comm)  # type: ignore[assignment]\n    if output_formats:\n        log(\"Logging to %s\" % dir)\n</code></pre>"},{"location":"api/#midst_toolkit.data_processing","title":"data_processing","text":""},{"location":"api/#midst_toolkit.data_processing.midst_data_processing","title":"midst_data_processing","text":""},{"location":"api/#midst_toolkit.data_processing.midst_data_processing.process_midst_data_for_quality_evaluation","title":"process_midst_data_for_quality_evaluation","text":"<pre><code>process_midst_data_for_quality_evaluation(\n    numerical_real_data,\n    categorical_real_data,\n    numerical_synthetic_data,\n    categorical_synthetic_data,\n    dataset_name,\n    model,\n)\n</code></pre> <p>This function handles data preprocessing customized to some of the models and datasets used in the MIDST competition. The processing is drawn from https://github.com/VectorInstitute/MIDSTModels/blob/main/midst_models/single_table_TabDDPM/eval/eval_quality.py.</p> <p>It has special considerations for how the provided dataframes are processed into numpy arrays depending on the dataname and model provided in the arguments.</p> <p>Parameters:</p> Name Type Description Default <code>numerical_real_data</code> <code>DataFrame</code> <p>Real data with numerical values</p> required <code>categorical_real_data</code> <code>DataFrame</code> <p>Real data with categorical values</p> required <code>numerical_synthetic_data</code> <code>DataFrame</code> <p>Synthetically generated data with numerical values</p> required <code>categorical_synthetic_data</code> <code>DataFrame</code> <p>Synthetically generated data with numerical values</p> required <code>dataset_name</code> <code>str</code> <p>Name of the dataset to which the real data belongs. The way that the data is processed will depend on whether special treatment is required for the specified name.</p> required <code>model</code> <code>str</code> <p>Model that was used to generate the synthetic data. Specific model names require special postprocessing in order for quality evaluation</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A tuple of four Numpy arrays, one for each of the numerical and categorical collections of real and synthetic</p> <code>ndarray</code> <p>data after processing. The order is numerical and categorical data for the real data, followed by the same</p> <code>ndarray</code> <p>for the synthetic data.</p> Source code in <code>src/midst_toolkit/data_processing/midst_data_processing.py</code> <pre><code>def process_midst_data_for_quality_evaluation(\n    numerical_real_data: pd.DataFrame,\n    categorical_real_data: pd.DataFrame,\n    numerical_synthetic_data: pd.DataFrame,\n    categorical_synthetic_data: pd.DataFrame,\n    dataset_name: str,\n    model: str,\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    This function handles data preprocessing customized to some of the models and datasets used in the MIDST\n    competition. The processing is drawn from\n    https://github.com/VectorInstitute/MIDSTModels/blob/main/midst_models/single_table_TabDDPM/eval/eval_quality.py.\n\n    It has special considerations for how the provided dataframes are processed into numpy arrays depending on the\n    dataname and model provided in the arguments.\n\n    Args:\n        numerical_real_data: Real data with numerical values\n        categorical_real_data: Real data with categorical values\n        numerical_synthetic_data: Synthetically generated data with numerical values\n        categorical_synthetic_data: Synthetically generated data with numerical values\n        dataset_name: Name of the dataset to which the real data belongs. The way that the data is processed will\n            depend on whether special treatment is required for the specified name.\n        model: Model that was used to generate the synthetic data. Specific model names require special postprocessing\n            in order for quality evaluation\n\n    Returns:\n        A tuple of four Numpy arrays, one for each of the numerical and categorical collections of real and synthetic\n        data after processing. The order is numerical and categorical data for the real data, followed by the same\n        for the synthetic data.\n    \"\"\"\n    categorical_synthetic_numpy = categorical_synthetic_data.to_numpy().astype(\"str\")\n\n    # Perform some special data post-processing for specific datasets and models as specified in the script\n    # arguments\n\n    if dataset_name in CONVERSION_DATASETS and model.startswith(CONVERSION_MODEL_PREFIX):\n        # If using the default or news dataset and a model postfixed with \"codi,\" need to perform an int cast\n        categorical_synthetic_numpy = categorical_synthetic_data.astype(\"int\").to_numpy().astype(\"str\")\n    elif model.startswith(CLIPPING_MODEL_PREFIX):\n        if dataset_name in MAX_CLIPPING_DATASETS:\n            # Column reassignment\n            categorical_synthetic_numpy[:, 1] = categorical_synthetic_data[11].astype(\"int\").to_numpy().astype(\"str\")\n            categorical_synthetic_numpy[:, 2] = categorical_synthetic_data[12].astype(\"int\").to_numpy().astype(\"str\")\n            categorical_synthetic_numpy[:, 3] = categorical_synthetic_data[13].astype(\"int\").to_numpy().astype(\"str\")\n\n            # Clip the maximum value to reflect that of the real data\n            max_data = categorical_real_data[14].max()\n            categorical_synthetic_data.loc[categorical_synthetic_data[14] &gt; max_data, 14] = max_data\n\n            # Perform column reassignment\n            categorical_synthetic_numpy[:, 4] = categorical_synthetic_data[14].astype(\"int\").to_numpy().astype(\"str\")\n            categorical_synthetic_numpy[:, 4] = categorical_synthetic_data[14].astype(\"int\").to_numpy().astype(\"str\")\n\n        elif dataset_name in MIN_MAX_CLIPPING_DATASETS:\n            # Note that columns here are not contiguous, so we enumerate\n            columns = categorical_real_data.columns\n            for i, col in enumerate(columns):\n                if categorical_real_data[col].dtype == \"int\":\n                    max_data = categorical_real_data[col].max()\n                    min_data = categorical_real_data[col].min()\n\n                    # Perform clipping based on the real data on both sides (min and max)\n                    categorical_synthetic_data.loc[categorical_synthetic_data[col] &gt; max_data, col] = max_data\n                    categorical_synthetic_data.loc[categorical_synthetic_data[col] &lt; min_data, col] = min_data\n\n                    categorical_synthetic_numpy[:, i] = (\n                        categorical_synthetic_data[col].astype(\"int\").to_numpy().astype(\"str\")\n                    )\n    return (\n        numerical_real_data.to_numpy(),\n        categorical_real_data.to_numpy().astype(\"str\"),\n        numerical_synthetic_data.to_numpy(),\n        categorical_synthetic_numpy,\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.data_processing.midst_data_processing.load_midst_data","title":"load_midst_data","text":"<pre><code>load_midst_data(\n    real_data_path, synthetic_data_path, meta_info_path\n)\n</code></pre> <p>Helper function for loading data at the specified paths. These paths are constructed either by the user or with a particular set of defaults that were used in the original MIDST competition (see, for example, https://github.com/VectorInstitute/MIDSTModels/blob/main/midst_models/single_table_TabDDPM/eval/eval_quality.py).</p> <p>Parameters:</p> Name Type Description Default <code>real_data_path</code> <code>Path</code> <p>Path from which to load the real data to which the synthetic data will be compared. This should be a CSV file.</p> required <code>synthetic_data_path</code> <code>Path</code> <p>Path from which to load the synthetic data to which the real data will be compared. This should be a CSV file.</p> required <code>meta_info_path</code> <code>Path</code> <p>This should be a JSON file containing meta information about the data generation process. Specifically, it should contain information about which columns of the real and synthetic data should actually be compared. It must contain keys: 'num_col_idx', 'cat_col_idx', 'target_col_idx', and 'task_type'.</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame, dict[str, Any]]</code> <p>The loaded real data, synthetic data, and meta information json for further processing.</p> Source code in <code>src/midst_toolkit/data_processing/midst_data_processing.py</code> <pre><code>def load_midst_data(\n    real_data_path: Path, synthetic_data_path: Path, meta_info_path: Path\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, dict[str, Any]]:\n    \"\"\"\n    Helper function for loading data at the specified paths. These paths are constructed either by the user or with a\n    particular set of defaults that were used in the original MIDST competition (see, for example,\n    https://github.com/VectorInstitute/MIDSTModels/blob/main/midst_models/single_table_TabDDPM/eval/eval_quality.py).\n\n    Args:\n        real_data_path: Path from which to load the real data to which the synthetic data will be compared. This\n            should be a CSV file.\n        synthetic_data_path: Path from which to load the synthetic data to which the real data will be compared. This\n            should be a CSV file.\n        meta_info_path: This should be a JSON file containing meta information about the data generation process.\n            Specifically, it should contain information about which columns of the real and synthetic data should\n            actually be compared. It must contain keys: 'num_col_idx', 'cat_col_idx', 'target_col_idx', and\n            'task_type'.\n\n    Returns:\n        The loaded real data, synthetic data, and meta information json for further processing.\n    \"\"\"\n    real_data = pd.read_csv(real_data_path)\n    synthetic_data = pd.read_csv(synthetic_data_path)\n\n    with open(meta_info_path, \"r\") as f:\n        meta_info = json.load(f)\n\n    return real_data, synthetic_data, meta_info\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation","title":"evaluation","text":""},{"location":"api/#midst_toolkit.evaluation.generation_quality","title":"generation_quality","text":""},{"location":"api/#midst_toolkit.evaluation.generation_quality.alpha_precision","title":"alpha_precision","text":""},{"location":"api/#midst_toolkit.evaluation.generation_quality.alpha_precision.synthcity_alpha_precision_metrics","title":"synthcity_alpha_precision_metrics","text":"<pre><code>synthcity_alpha_precision_metrics(\n    real_data, synthetic_data, naive_only=True\n)\n</code></pre> <p>Computes a number of quality metrics comparing the synthetic data to ground truth data using the Synthcity library. This function uses the AlphaPrecision class in that library, which computes the alpha-precision, beta-recall, and authenticity scores between the two datasets. If the <code>naive_only</code> boolean is True, then only the \"naive\" metrics are reported, i.e. metrics with \"naive\" in their name.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>DataFrame</code> <p>Real data that the synthetic data is meant to mimic/replace.</p> required <code>synthetic_data</code> <code>DataFrame</code> <p>Synthetic data to be compared against the provided real data.</p> required <code>naive_only</code> <code>bool</code> <p>If True, then only the \"naive\" metrics are reported. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the computed scores using the AlphaPrecision class in the Synthcity library.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/alpha_precision.py</code> <pre><code>def synthcity_alpha_precision_metrics(\n    real_data: pd.DataFrame, synthetic_data: pd.DataFrame, naive_only: bool = True\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Computes a number of quality metrics comparing the synthetic data to ground truth data using the Synthcity library.\n    This function uses the AlphaPrecision class in that library, which computes the alpha-precision, beta-recall, and\n    authenticity scores between the two datasets. If the ``naive_only`` boolean is True, then only the \"naive\" metrics\n    are reported, i.e. metrics with \"naive\" in their name.\n\n    Args:\n        real_data: Real data that the synthetic data is meant to mimic/replace.\n        synthetic_data: Synthetic data to be compared against the provided real data.\n        naive_only: If True, then only the \"naive\" metrics are reported. Defaults to True.\n\n    Returns:\n        A dictionary containing the computed scores using the AlphaPrecision class in the Synthcity library.\n    \"\"\"\n    # Wrap the dataframes in a Synthcity compatible dataloader\n    real_data_loader = GenericDataLoader(real_data)\n    synthetic_data_loader = GenericDataLoader(synthetic_data)\n\n    quality_evaluator = AlphaPrecision()\n    quality_results = quality_evaluator.evaluate(real_data_loader, synthetic_data_loader)\n\n    # Log results and filter to naive keys if requested\n    for metric_key, metric_value in quality_results.items():\n        log(INFO, f\"{metric_key}: {metric_value}\")\n        if naive_only and (NAIVE_METRIC_SUFFIX not in metric_key):\n            del quality_results[metric_key]\n\n    return quality_results\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.generation_quality.synthcity","title":"synthcity","text":"<p>This Module reproduces the code necessary to run the Synthcity alpha precision evaluation computations. The files are pulled and modified from this repository https://github.com/vanderschaarlab/synthcity.</p> <p>There are two reasons for this port.</p> <p>1: Synthcity, in its larger form, is incompatible with newer versions of PyTorch, beyond 2.4, which is limiting.</p> <p>2: On Mac OS, every time its imported, some external python process is kicked off. This manifests as a launcher icon appearing temporarily in the dock and then disappearing. If you're using VS code, it periodically re-imports the library in the background (for whatever reason), resulting in the launcher icon appearing repeatedly and endlessly.</p> <p>We don't need much of the Synthcity library for our evals. So having the computations reproduced here locally alleviates both issues. We can re-evaluate this port with newer versions of Synthcity, if they come.</p>"},{"location":"api/#midst_toolkit.evaluation.generation_quality.synthcity.dataloader","title":"dataloader","text":"DataLoader \u00b6 Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>class DataLoader(metaclass=ABCMeta):\n    def __init__(\n        self,\n        data_type: str,\n        data: Any,\n        static_features: list[str] | None = None,\n        temporal_features: list[str] | None = None,\n        sensitive_features: list[str] | None = None,\n        important_features: list[str] | None = None,\n        outcome_features: list[str] | None = None,\n        train_size: float = 0.8,\n        random_state: int = 0,\n    ) -&gt; None:\n        \"\"\"\n        Base class for all data loaders.\n\n        Args:\n            data_type: The type of DataLoader, currently supports \"generic\"\n            data: The object that contains the data\n            static_features: List of feature names that are static features (as opposed to temporal features).\n                Defaults to None.\n            temporal_features: List of feature names that are temporal features, i.e. observed over time.\n                Defaults to None.\n            sensitive_features: Name of sensitive features. Defaults to None.\n            important_features: Only relevant for SurvivalGAN method. Defaults to None.\n            outcome_features: The feature name that provides labels for downstream tasks. Defaults to None.\n            train_size: Proportion of data to be used for training, versus evaluation. Defaults to 0.8.\n            random_state: Random state for sampling from the dataloaders. Defaults to 0.\n        \"\"\"\n        self.static_features = static_features if static_features else []\n        self.temporal_features = temporal_features if temporal_features else []\n        self.sensitive_features = sensitive_features if sensitive_features else []\n        self.important_features = important_features if important_features else []\n        self.outcome_features = outcome_features if outcome_features else []\n        self.random_state = random_state\n\n        self.data = data\n        self.data_type = data_type\n        self.train_size = train_size\n\n    def raw(self) -&gt; Any:\n        \"\"\"Just return the data in the dataloader.\"\"\"\n        return self.data\n\n    @abstractmethod\n    def unpack(self, as_numpy: bool = False, pad: bool = False) -&gt; Any:\n        \"\"\"A method that unpacks the columns and returns features and labels (X, y).\"\"\"\n        ...\n\n    @abstractmethod\n    def decorate(self, data: Any) -&gt; DataLoader:\n        \"\"\"\n        A method that creates a new instance of DataLoader by decorating the input data with the same DataLoader\n        properties (e.g. sensitive features, target column, etc.).\n        \"\"\"\n        ...\n\n    def type(self) -&gt; str:\n        \"\"\"Return data type.\"\"\"\n        return self.data_type\n\n    @property\n    @abstractmethod\n    def shape(self) -&gt; tuple:\n        \"\"\"Return shape of the data.\"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def columns(self) -&gt; list:\n        \"\"\"Return list of data columns.\"\"\"\n        ...\n\n    @abstractmethod\n    def dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"A method that returns the pandas dataframe that contains all features and samples.\"\"\"\n        ...\n\n    @abstractmethod\n    def numpy(self) -&gt; np.ndarray:\n        \"\"\"A method that returns the numpy array that contains all features and samples.\"\"\"\n        ...\n\n    @property\n    def values(self) -&gt; np.ndarray:\n        \"\"\"Pass through to the numpy method.\"\"\"\n        return self.numpy()\n\n    @abstractmethod\n    def info(self) -&gt; dict:\n        \"\"\"A method that returns a dictionary of DataLoader information.\"\"\"\n        ...\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"A method that returns the number of samples in the DataLoader.\"\"\"\n        ...\n\n    @staticmethod\n    @abstractmethod\n    def from_info(data: pd.DataFrame, info: dict) -&gt; DataLoader:\n        \"\"\"A static method that creates a DataLoader from the data and the information dictionary.\"\"\"\n        ...\n\n    @abstractmethod\n    def sample(self, count: int, random_state: int = 0) -&gt; DataLoader:\n        \"\"\"Returns a new DataLoader that contains a random subset of N samples.\"\"\"\n        ...\n\n    @abstractmethod\n    def drop(self, columns: list | None) -&gt; DataLoader:\n        \"\"\"Returns a new DataLoader with a list of columns dropped.\"\"\"\n        ...\n\n    @abstractmethod\n    def __getitem__(self, feature: str | list) -&gt; Any:\n        \"\"\"Get an item as specified in the feature argument.\"\"\"\n        ...\n\n    @abstractmethod\n    def __setitem__(self, feature: str, val: Any) -&gt; None:\n        \"\"\"Set an item in the dataloader to the provided value.\"\"\"\n        ...\n\n    @abstractmethod\n    def train(self) -&gt; DataLoader:\n        \"\"\"Returns a DataLoader containing the training set.\"\"\"\n        ...\n\n    @abstractmethod\n    def test(self) -&gt; DataLoader:\n        \"\"\"Returns a DataLoader containing the test set.\"\"\"\n        ...\n\n    def __repr__(self, *args: Any, **kwargs: Any) -&gt; str:\n        \"\"\"Return a string representation.\"\"\"\n        return self.dataframe().__repr__(*args, **kwargs)\n\n    def _repr_html_(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Return a string representation in html format.\"\"\"\n        return self.dataframe()._repr_html_(*args, **kwargs)\n\n    @abstractmethod\n    def fillna(self, value: Any) -&gt; DataLoader:\n        \"\"\"Returns a DataLoader with NaN filled by the provided number(s).\"\"\"\n        ...\n\n    @abstractmethod\n    def compression_protected_features(self) -&gt; list:\n        \"\"\"No idea.\"\"\"\n        ...\n\n    def domain(self) -&gt; str | None:\n        \"\"\"Domain of the data.\"\"\"\n        return None\n\n    @abstractmethod\n    def is_tabular(self) -&gt; bool:\n        \"\"\"Specifies whether the dataloader represents tabular data.\"\"\"\n        ...\n\n    @abstractmethod\n    def get_fairness_column(self) -&gt; str | Any:\n        \"\"\"Get the name of the column associated with Fairness.\"\"\"\n        ...\n</code></pre> <code></code> shape <code>abstractmethod</code> <code>property</code> \u00b6 <pre><code>shape\n</code></pre> <p>Return shape of the data.</p> <code></code> columns <code>abstractmethod</code> <code>property</code> \u00b6 <pre><code>columns\n</code></pre> <p>Return list of data columns.</p> <code></code> values <code>property</code> \u00b6 <pre><code>values\n</code></pre> <p>Pass through to the numpy method.</p> <code></code> __init__ \u00b6 <pre><code>__init__(\n    data_type,\n    data,\n    static_features=None,\n    temporal_features=None,\n    sensitive_features=None,\n    important_features=None,\n    outcome_features=None,\n    train_size=0.8,\n    random_state=0,\n)\n</code></pre> <p>Base class for all data loaders.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>str</code> <p>The type of DataLoader, currently supports \"generic\"</p> required <code>data</code> <code>Any</code> <p>The object that contains the data</p> required <code>static_features</code> <code>list[str] | None</code> <p>List of feature names that are static features (as opposed to temporal features). Defaults to None.</p> <code>None</code> <code>temporal_features</code> <code>list[str] | None</code> <p>List of feature names that are temporal features, i.e. observed over time. Defaults to None.</p> <code>None</code> <code>sensitive_features</code> <code>list[str] | None</code> <p>Name of sensitive features. Defaults to None.</p> <code>None</code> <code>important_features</code> <code>list[str] | None</code> <p>Only relevant for SurvivalGAN method. Defaults to None.</p> <code>None</code> <code>outcome_features</code> <code>list[str] | None</code> <p>The feature name that provides labels for downstream tasks. Defaults to None.</p> <code>None</code> <code>train_size</code> <code>float</code> <p>Proportion of data to be used for training, versus evaluation. Defaults to 0.8.</p> <code>0.8</code> <code>random_state</code> <code>int</code> <p>Random state for sampling from the dataloaders. Defaults to 0.</p> <code>0</code> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def __init__(\n    self,\n    data_type: str,\n    data: Any,\n    static_features: list[str] | None = None,\n    temporal_features: list[str] | None = None,\n    sensitive_features: list[str] | None = None,\n    important_features: list[str] | None = None,\n    outcome_features: list[str] | None = None,\n    train_size: float = 0.8,\n    random_state: int = 0,\n) -&gt; None:\n    \"\"\"\n    Base class for all data loaders.\n\n    Args:\n        data_type: The type of DataLoader, currently supports \"generic\"\n        data: The object that contains the data\n        static_features: List of feature names that are static features (as opposed to temporal features).\n            Defaults to None.\n        temporal_features: List of feature names that are temporal features, i.e. observed over time.\n            Defaults to None.\n        sensitive_features: Name of sensitive features. Defaults to None.\n        important_features: Only relevant for SurvivalGAN method. Defaults to None.\n        outcome_features: The feature name that provides labels for downstream tasks. Defaults to None.\n        train_size: Proportion of data to be used for training, versus evaluation. Defaults to 0.8.\n        random_state: Random state for sampling from the dataloaders. Defaults to 0.\n    \"\"\"\n    self.static_features = static_features if static_features else []\n    self.temporal_features = temporal_features if temporal_features else []\n    self.sensitive_features = sensitive_features if sensitive_features else []\n    self.important_features = important_features if important_features else []\n    self.outcome_features = outcome_features if outcome_features else []\n    self.random_state = random_state\n\n    self.data = data\n    self.data_type = data_type\n    self.train_size = train_size\n</code></pre> <code></code> raw \u00b6 <pre><code>raw()\n</code></pre> <p>Just return the data in the dataloader.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def raw(self) -&gt; Any:\n    \"\"\"Just return the data in the dataloader.\"\"\"\n    return self.data\n</code></pre> <code></code> unpack <code>abstractmethod</code> \u00b6 <pre><code>unpack(as_numpy=False, pad=False)\n</code></pre> <p>A method that unpacks the columns and returns features and labels (X, y).</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef unpack(self, as_numpy: bool = False, pad: bool = False) -&gt; Any:\n    \"\"\"A method that unpacks the columns and returns features and labels (X, y).\"\"\"\n    ...\n</code></pre> <code></code> decorate <code>abstractmethod</code> \u00b6 <pre><code>decorate(data)\n</code></pre> <p>A method that creates a new instance of DataLoader by decorating the input data with the same DataLoader properties (e.g. sensitive features, target column, etc.).</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef decorate(self, data: Any) -&gt; DataLoader:\n    \"\"\"\n    A method that creates a new instance of DataLoader by decorating the input data with the same DataLoader\n    properties (e.g. sensitive features, target column, etc.).\n    \"\"\"\n    ...\n</code></pre> <code></code> type \u00b6 <pre><code>type()\n</code></pre> <p>Return data type.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def type(self) -&gt; str:\n    \"\"\"Return data type.\"\"\"\n    return self.data_type\n</code></pre> <code></code> dataframe <code>abstractmethod</code> \u00b6 <pre><code>dataframe()\n</code></pre> <p>A method that returns the pandas dataframe that contains all features and samples.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"A method that returns the pandas dataframe that contains all features and samples.\"\"\"\n    ...\n</code></pre> <code></code> numpy <code>abstractmethod</code> \u00b6 <pre><code>numpy()\n</code></pre> <p>A method that returns the numpy array that contains all features and samples.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef numpy(self) -&gt; np.ndarray:\n    \"\"\"A method that returns the numpy array that contains all features and samples.\"\"\"\n    ...\n</code></pre> <code></code> info <code>abstractmethod</code> \u00b6 <pre><code>info()\n</code></pre> <p>A method that returns a dictionary of DataLoader information.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef info(self) -&gt; dict:\n    \"\"\"A method that returns a dictionary of DataLoader information.\"\"\"\n    ...\n</code></pre> <code></code> __len__ <code>abstractmethod</code> \u00b6 <pre><code>__len__()\n</code></pre> <p>A method that returns the number of samples in the DataLoader.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"A method that returns the number of samples in the DataLoader.\"\"\"\n    ...\n</code></pre> <code></code> from_info <code>abstractmethod</code> <code>staticmethod</code> \u00b6 <pre><code>from_info(data, info)\n</code></pre> <p>A static method that creates a DataLoader from the data and the information dictionary.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef from_info(data: pd.DataFrame, info: dict) -&gt; DataLoader:\n    \"\"\"A static method that creates a DataLoader from the data and the information dictionary.\"\"\"\n    ...\n</code></pre> <code></code> sample <code>abstractmethod</code> \u00b6 <pre><code>sample(count, random_state=0)\n</code></pre> <p>Returns a new DataLoader that contains a random subset of N samples.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef sample(self, count: int, random_state: int = 0) -&gt; DataLoader:\n    \"\"\"Returns a new DataLoader that contains a random subset of N samples.\"\"\"\n    ...\n</code></pre> <code></code> drop <code>abstractmethod</code> \u00b6 <pre><code>drop(columns)\n</code></pre> <p>Returns a new DataLoader with a list of columns dropped.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef drop(self, columns: list | None) -&gt; DataLoader:\n    \"\"\"Returns a new DataLoader with a list of columns dropped.\"\"\"\n    ...\n</code></pre> <code></code> __getitem__ <code>abstractmethod</code> \u00b6 <pre><code>__getitem__(feature)\n</code></pre> <p>Get an item as specified in the feature argument.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, feature: str | list) -&gt; Any:\n    \"\"\"Get an item as specified in the feature argument.\"\"\"\n    ...\n</code></pre> <code></code> __setitem__ <code>abstractmethod</code> \u00b6 <pre><code>__setitem__(feature, val)\n</code></pre> <p>Set an item in the dataloader to the provided value.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef __setitem__(self, feature: str, val: Any) -&gt; None:\n    \"\"\"Set an item in the dataloader to the provided value.\"\"\"\n    ...\n</code></pre> <code></code> train <code>abstractmethod</code> \u00b6 <pre><code>train()\n</code></pre> <p>Returns a DataLoader containing the training set.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef train(self) -&gt; DataLoader:\n    \"\"\"Returns a DataLoader containing the training set.\"\"\"\n    ...\n</code></pre> <code></code> test <code>abstractmethod</code> \u00b6 <pre><code>test()\n</code></pre> <p>Returns a DataLoader containing the test set.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef test(self) -&gt; DataLoader:\n    \"\"\"Returns a DataLoader containing the test set.\"\"\"\n    ...\n</code></pre> <code></code> __repr__ \u00b6 <pre><code>__repr__(*args, **kwargs)\n</code></pre> <p>Return a string representation.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def __repr__(self, *args: Any, **kwargs: Any) -&gt; str:\n    \"\"\"Return a string representation.\"\"\"\n    return self.dataframe().__repr__(*args, **kwargs)\n</code></pre> <code></code> fillna <code>abstractmethod</code> \u00b6 <pre><code>fillna(value)\n</code></pre> <p>Returns a DataLoader with NaN filled by the provided number(s).</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef fillna(self, value: Any) -&gt; DataLoader:\n    \"\"\"Returns a DataLoader with NaN filled by the provided number(s).\"\"\"\n    ...\n</code></pre> <code></code> compression_protected_features <code>abstractmethod</code> \u00b6 <pre><code>compression_protected_features()\n</code></pre> <p>No idea.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef compression_protected_features(self) -&gt; list:\n    \"\"\"No idea.\"\"\"\n    ...\n</code></pre> <code></code> domain \u00b6 <pre><code>domain()\n</code></pre> <p>Domain of the data.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def domain(self) -&gt; str | None:\n    \"\"\"Domain of the data.\"\"\"\n    return None\n</code></pre> <code></code> is_tabular <code>abstractmethod</code> \u00b6 <pre><code>is_tabular()\n</code></pre> <p>Specifies whether the dataloader represents tabular data.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef is_tabular(self) -&gt; bool:\n    \"\"\"Specifies whether the dataloader represents tabular data.\"\"\"\n    ...\n</code></pre> <code></code> get_fairness_column <code>abstractmethod</code> \u00b6 <pre><code>get_fairness_column()\n</code></pre> <p>Get the name of the column associated with Fairness.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>@abstractmethod\ndef get_fairness_column(self) -&gt; str | Any:\n    \"\"\"Get the name of the column associated with Fairness.\"\"\"\n    ...\n</code></pre> <code></code> GenericDataLoader \u00b6 <p>               Bases: <code>DataLoader</code></p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>class GenericDataLoader(DataLoader):\n    def __init__(\n        self,\n        data: pd.DataFrame | list | np.ndarray,\n        sensitive_features: list[str] | None = None,\n        important_features: list[str] | None = None,\n        target_column: str | None = None,\n        fairness_column: str | None = None,\n        domain_column: str | None = None,\n        random_state: int = 0,\n        train_size: float = 0.8,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Data loader for generic tabular data.\n\n        Args:\n            data: The dataset. Either a Pandas DataFrame, list, or a Numpy Array.\n            sensitive_features:  Name of sensitive features.. Defaults to None.\n            important_features: Only relevant for SurvivalGAN method. Defaults to None.\n            target_column: The feature name that provides labels for downstream tasks. Defaults to None.\n            fairness_column:  Optional fairness column label, used for fairness benchmarking. Defaults to None.\n            domain_column: Optional domain label, used for domain adaptation algorithms. Defaults to None.\n            random_state: Random state for sampling from the dataloaders. Defaults to 0.\n            train_size: Proportion of data to be used for training, versus evaluation. Defaults to 0.8.\n            kwargs: Other settings to be processed.\n        \"\"\"\n        if not isinstance(data, pd.DataFrame):\n            data = pd.DataFrame(data)\n\n        data.columns = data.columns.astype(str)\n        if target_column is not None:\n            self.target_column = target_column\n        elif len(data.columns) &gt; 0:\n            self.target_column = data.columns[-1]\n        else:\n            self.target_column = \"---\"\n\n        self.fairness_column = fairness_column\n        self.domain_column = domain_column\n\n        super().__init__(\n            data_type=\"generic\",\n            data=data,\n            static_features=list(data.columns),\n            sensitive_features=sensitive_features,\n            important_features=important_features,\n            outcome_features=[self.target_column],\n            random_state=random_state,\n            train_size=train_size,\n            **kwargs,\n        )\n\n    @property\n    def shape(self) -&gt; tuple:\n        \"\"\"Return the shape of the data.\"\"\"\n        return self.data.shape\n\n    def domain(self) -&gt; str | None:\n        \"\"\"Return the domain column if it exists.\"\"\"\n        return self.domain_column\n\n    def get_fairness_column(self) -&gt; str | Any:\n        \"\"\"Return the fairness column if it exists.\"\"\"\n        return self.fairness_column\n\n    @property\n    def columns(self) -&gt; list:\n        \"\"\"Return a list of the data columns.\"\"\"\n        return list(self.data.columns)\n\n    def compression_protected_features(self) -&gt; list:\n        \"\"\"No idea.\"\"\"\n        out = [self.target_column]\n        domain = self.domain()\n\n        if domain is not None:\n            out.append(domain)\n\n        return out\n\n    def unpack(self, as_numpy: bool = False, pad: bool = False) -&gt; Any:\n        \"\"\"A method that unpacks the columns and returns features and labels (X, y).\"\"\"\n        x = self.data.drop(columns=[self.target_column])\n        y = self.data[self.target_column]\n\n        if as_numpy:\n            return np.asarray(x), np.asarray(y)\n        return x, y\n\n    def dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"A method that returns the pandas dataframe that contains all features and samples.\"\"\"\n        return self.data\n\n    def numpy(self) -&gt; np.ndarray:\n        \"\"\"A method that returns the numpy array that contains all features and samples.\"\"\"\n        return self.dataframe().values\n\n    def info(self) -&gt; dict:\n        \"\"\"A method that returns a dictionary of DataLoader information.\"\"\"\n        return {\n            \"data_type\": self.data_type,\n            \"len\": len(self),\n            \"static_features\": self.static_features,\n            \"sensitive_features\": self.sensitive_features,\n            \"important_features\": self.important_features,\n            \"outcome_features\": self.outcome_features,\n            \"target_column\": self.target_column,\n            \"fairness_column\": self.fairness_column,\n            \"domain_column\": self.domain_column,\n            \"train_size\": self.train_size,\n        }\n\n    def __len__(self) -&gt; int:\n        \"\"\"A method that returns the number of samples in the DataLoader.\"\"\"\n        return len(self.data)\n\n    def decorate(self, data: Any) -&gt; DataLoader:\n        \"\"\"\n        A method that creates a new instance of DataLoader by decorating the input data with the same DataLoader\n        properties (e.g. sensitive features, target column, etc.).\n        \"\"\"\n        return GenericDataLoader(\n            data,\n            sensitive_features=self.sensitive_features,\n            important_features=self.important_features,\n            target_column=self.target_column,\n            random_state=self.random_state,\n            train_size=self.train_size,\n            fairness_column=self.fairness_column,\n            domain_column=self.domain_column,\n        )\n\n    def sample(self, count: int, random_state: int = 0) -&gt; DataLoader:\n        \"\"\"Returns a new DataLoader that contains a random subset of N samples.\"\"\"\n        return self.decorate(self.data.sample(count, random_state=random_state))\n\n    def drop(self, columns: list | None = None) -&gt; DataLoader:\n        \"\"\"Returns a new DataLoader with a list of columns dropped.\"\"\"\n        return self.decorate(self.data.drop(columns=(columns if columns else [])))\n\n    @staticmethod\n    def from_info(data: pd.DataFrame, info: dict) -&gt; GenericDataLoader:\n        \"\"\"A static method that creates a DataLoader from the data and the information dictionary.\"\"\"\n        if not isinstance(data, pd.DataFrame):\n            raise ValueError(f\"Invalid data type {type(data)}\")\n\n        return GenericDataLoader(\n            data,\n            sensitive_features=info[\"sensitive_features\"],\n            important_features=info[\"important_features\"],\n            target_column=info[\"target_column\"],\n            fairness_column=info[\"fairness_column\"],\n            domain_column=info[\"domain_column\"],\n            train_size=info[\"train_size\"],\n        )\n\n    def __getitem__(self, feature: str | list | int) -&gt; Any:\n        \"\"\"Get an item from the dataloader.\"\"\"\n        return self.data[feature]\n\n    def __setitem__(self, feature: str, val: Any) -&gt; None:\n        \"\"\"Replace an item in the dataloader with the specified value.\"\"\"\n        self.data[feature] = val\n\n    def _train_test_split(self) -&gt; tuple:\n        \"\"\"Split the dataset into train and test sets according to a specified ratio.\"\"\"\n        stratify = None\n        if self.target_column in self.data:\n            target = self.data[self.target_column]\n            if target.value_counts().min() &gt; 1:\n                stratify = target\n\n        return train_test_split(\n            self.data,\n            train_size=self.train_size,\n            random_state=self.random_state,\n            stratify=stratify,\n        )\n\n    def train(self) -&gt; DataLoader:\n        \"\"\"Returns a DataLoader containing the training set.\"\"\"\n        train_data, _ = self._train_test_split()\n        return self.decorate(train_data.reset_index(drop=True))\n\n    def test(self) -&gt; DataLoader:\n        \"\"\"Returns a DataLoader containing the training set.\"\"\"\n        _, test_data = self._train_test_split()\n        return self.decorate(test_data.reset_index(drop=True))\n\n    def fillna(self, value: Any) -&gt; DataLoader:\n        \"\"\"Returns a DataLoader with NaN filled by the provided number(s).\"\"\"\n        self.data = self.data.fillna(value)\n        return self\n\n    def is_tabular(self) -&gt; bool:\n        \"\"\"Always represents a tabular dataset.\"\"\"\n        return True\n</code></pre> <code></code> shape <code>property</code> \u00b6 <pre><code>shape\n</code></pre> <p>Return the shape of the data.</p> <code></code> columns <code>property</code> \u00b6 <pre><code>columns\n</code></pre> <p>Return a list of the data columns.</p> <code></code> __init__ \u00b6 <pre><code>__init__(\n    data,\n    sensitive_features=None,\n    important_features=None,\n    target_column=None,\n    fairness_column=None,\n    domain_column=None,\n    random_state=0,\n    train_size=0.8,\n    **kwargs,\n)\n</code></pre> <p>Data loader for generic tabular data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame | list | ndarray</code> <p>The dataset. Either a Pandas DataFrame, list, or a Numpy Array.</p> required <code>sensitive_features</code> <code>list[str] | None</code> <p>Name of sensitive features.. Defaults to None.</p> <code>None</code> <code>important_features</code> <code>list[str] | None</code> <p>Only relevant for SurvivalGAN method. Defaults to None.</p> <code>None</code> <code>target_column</code> <code>str | None</code> <p>The feature name that provides labels for downstream tasks. Defaults to None.</p> <code>None</code> <code>fairness_column</code> <code>str | None</code> <p>Optional fairness column label, used for fairness benchmarking. Defaults to None.</p> <code>None</code> <code>domain_column</code> <code>str | None</code> <p>Optional domain label, used for domain adaptation algorithms. Defaults to None.</p> <code>None</code> <code>random_state</code> <code>int</code> <p>Random state for sampling from the dataloaders. Defaults to 0.</p> <code>0</code> <code>train_size</code> <code>float</code> <p>Proportion of data to be used for training, versus evaluation. Defaults to 0.8.</p> <code>0.8</code> <code>kwargs</code> <code>Any</code> <p>Other settings to be processed.</p> <code>{}</code> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame | list | np.ndarray,\n    sensitive_features: list[str] | None = None,\n    important_features: list[str] | None = None,\n    target_column: str | None = None,\n    fairness_column: str | None = None,\n    domain_column: str | None = None,\n    random_state: int = 0,\n    train_size: float = 0.8,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Data loader for generic tabular data.\n\n    Args:\n        data: The dataset. Either a Pandas DataFrame, list, or a Numpy Array.\n        sensitive_features:  Name of sensitive features.. Defaults to None.\n        important_features: Only relevant for SurvivalGAN method. Defaults to None.\n        target_column: The feature name that provides labels for downstream tasks. Defaults to None.\n        fairness_column:  Optional fairness column label, used for fairness benchmarking. Defaults to None.\n        domain_column: Optional domain label, used for domain adaptation algorithms. Defaults to None.\n        random_state: Random state for sampling from the dataloaders. Defaults to 0.\n        train_size: Proportion of data to be used for training, versus evaluation. Defaults to 0.8.\n        kwargs: Other settings to be processed.\n    \"\"\"\n    if not isinstance(data, pd.DataFrame):\n        data = pd.DataFrame(data)\n\n    data.columns = data.columns.astype(str)\n    if target_column is not None:\n        self.target_column = target_column\n    elif len(data.columns) &gt; 0:\n        self.target_column = data.columns[-1]\n    else:\n        self.target_column = \"---\"\n\n    self.fairness_column = fairness_column\n    self.domain_column = domain_column\n\n    super().__init__(\n        data_type=\"generic\",\n        data=data,\n        static_features=list(data.columns),\n        sensitive_features=sensitive_features,\n        important_features=important_features,\n        outcome_features=[self.target_column],\n        random_state=random_state,\n        train_size=train_size,\n        **kwargs,\n    )\n</code></pre> <code></code> domain \u00b6 <pre><code>domain()\n</code></pre> <p>Return the domain column if it exists.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def domain(self) -&gt; str | None:\n    \"\"\"Return the domain column if it exists.\"\"\"\n    return self.domain_column\n</code></pre> <code></code> get_fairness_column \u00b6 <pre><code>get_fairness_column()\n</code></pre> <p>Return the fairness column if it exists.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def get_fairness_column(self) -&gt; str | Any:\n    \"\"\"Return the fairness column if it exists.\"\"\"\n    return self.fairness_column\n</code></pre> <code></code> compression_protected_features \u00b6 <pre><code>compression_protected_features()\n</code></pre> <p>No idea.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def compression_protected_features(self) -&gt; list:\n    \"\"\"No idea.\"\"\"\n    out = [self.target_column]\n    domain = self.domain()\n\n    if domain is not None:\n        out.append(domain)\n\n    return out\n</code></pre> <code></code> unpack \u00b6 <pre><code>unpack(as_numpy=False, pad=False)\n</code></pre> <p>A method that unpacks the columns and returns features and labels (X, y).</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def unpack(self, as_numpy: bool = False, pad: bool = False) -&gt; Any:\n    \"\"\"A method that unpacks the columns and returns features and labels (X, y).\"\"\"\n    x = self.data.drop(columns=[self.target_column])\n    y = self.data[self.target_column]\n\n    if as_numpy:\n        return np.asarray(x), np.asarray(y)\n    return x, y\n</code></pre> <code></code> dataframe \u00b6 <pre><code>dataframe()\n</code></pre> <p>A method that returns the pandas dataframe that contains all features and samples.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"A method that returns the pandas dataframe that contains all features and samples.\"\"\"\n    return self.data\n</code></pre> <code></code> numpy \u00b6 <pre><code>numpy()\n</code></pre> <p>A method that returns the numpy array that contains all features and samples.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def numpy(self) -&gt; np.ndarray:\n    \"\"\"A method that returns the numpy array that contains all features and samples.\"\"\"\n    return self.dataframe().values\n</code></pre> <code></code> info \u00b6 <pre><code>info()\n</code></pre> <p>A method that returns a dictionary of DataLoader information.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def info(self) -&gt; dict:\n    \"\"\"A method that returns a dictionary of DataLoader information.\"\"\"\n    return {\n        \"data_type\": self.data_type,\n        \"len\": len(self),\n        \"static_features\": self.static_features,\n        \"sensitive_features\": self.sensitive_features,\n        \"important_features\": self.important_features,\n        \"outcome_features\": self.outcome_features,\n        \"target_column\": self.target_column,\n        \"fairness_column\": self.fairness_column,\n        \"domain_column\": self.domain_column,\n        \"train_size\": self.train_size,\n    }\n</code></pre> <code></code> __len__ \u00b6 <pre><code>__len__()\n</code></pre> <p>A method that returns the number of samples in the DataLoader.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"A method that returns the number of samples in the DataLoader.\"\"\"\n    return len(self.data)\n</code></pre> <code></code> decorate \u00b6 <pre><code>decorate(data)\n</code></pre> <p>A method that creates a new instance of DataLoader by decorating the input data with the same DataLoader properties (e.g. sensitive features, target column, etc.).</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def decorate(self, data: Any) -&gt; DataLoader:\n    \"\"\"\n    A method that creates a new instance of DataLoader by decorating the input data with the same DataLoader\n    properties (e.g. sensitive features, target column, etc.).\n    \"\"\"\n    return GenericDataLoader(\n        data,\n        sensitive_features=self.sensitive_features,\n        important_features=self.important_features,\n        target_column=self.target_column,\n        random_state=self.random_state,\n        train_size=self.train_size,\n        fairness_column=self.fairness_column,\n        domain_column=self.domain_column,\n    )\n</code></pre> <code></code> sample \u00b6 <pre><code>sample(count, random_state=0)\n</code></pre> <p>Returns a new DataLoader that contains a random subset of N samples.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def sample(self, count: int, random_state: int = 0) -&gt; DataLoader:\n    \"\"\"Returns a new DataLoader that contains a random subset of N samples.\"\"\"\n    return self.decorate(self.data.sample(count, random_state=random_state))\n</code></pre> <code></code> drop \u00b6 <pre><code>drop(columns=None)\n</code></pre> <p>Returns a new DataLoader with a list of columns dropped.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def drop(self, columns: list | None = None) -&gt; DataLoader:\n    \"\"\"Returns a new DataLoader with a list of columns dropped.\"\"\"\n    return self.decorate(self.data.drop(columns=(columns if columns else [])))\n</code></pre> <code></code> from_info <code>staticmethod</code> \u00b6 <pre><code>from_info(data, info)\n</code></pre> <p>A static method that creates a DataLoader from the data and the information dictionary.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>@staticmethod\ndef from_info(data: pd.DataFrame, info: dict) -&gt; GenericDataLoader:\n    \"\"\"A static method that creates a DataLoader from the data and the information dictionary.\"\"\"\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(f\"Invalid data type {type(data)}\")\n\n    return GenericDataLoader(\n        data,\n        sensitive_features=info[\"sensitive_features\"],\n        important_features=info[\"important_features\"],\n        target_column=info[\"target_column\"],\n        fairness_column=info[\"fairness_column\"],\n        domain_column=info[\"domain_column\"],\n        train_size=info[\"train_size\"],\n    )\n</code></pre> <code></code> __getitem__ \u00b6 <pre><code>__getitem__(feature)\n</code></pre> <p>Get an item from the dataloader.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def __getitem__(self, feature: str | list | int) -&gt; Any:\n    \"\"\"Get an item from the dataloader.\"\"\"\n    return self.data[feature]\n</code></pre> <code></code> __setitem__ \u00b6 <pre><code>__setitem__(feature, val)\n</code></pre> <p>Replace an item in the dataloader with the specified value.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def __setitem__(self, feature: str, val: Any) -&gt; None:\n    \"\"\"Replace an item in the dataloader with the specified value.\"\"\"\n    self.data[feature] = val\n</code></pre> <code></code> train \u00b6 <pre><code>train()\n</code></pre> <p>Returns a DataLoader containing the training set.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def train(self) -&gt; DataLoader:\n    \"\"\"Returns a DataLoader containing the training set.\"\"\"\n    train_data, _ = self._train_test_split()\n    return self.decorate(train_data.reset_index(drop=True))\n</code></pre> <code></code> test \u00b6 <pre><code>test()\n</code></pre> <p>Returns a DataLoader containing the training set.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def test(self) -&gt; DataLoader:\n    \"\"\"Returns a DataLoader containing the training set.\"\"\"\n    _, test_data = self._train_test_split()\n    return self.decorate(test_data.reset_index(drop=True))\n</code></pre> <code></code> fillna \u00b6 <pre><code>fillna(value)\n</code></pre> <p>Returns a DataLoader with NaN filled by the provided number(s).</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def fillna(self, value: Any) -&gt; DataLoader:\n    \"\"\"Returns a DataLoader with NaN filled by the provided number(s).\"\"\"\n    self.data = self.data.fillna(value)\n    return self\n</code></pre> <code></code> is_tabular \u00b6 <pre><code>is_tabular()\n</code></pre> <p>Always represents a tabular dataset.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def is_tabular(self) -&gt; bool:\n    \"\"\"Always represents a tabular dataset.\"\"\"\n    return True\n</code></pre> <code></code> create_from_info \u00b6 <pre><code>create_from_info(data, info)\n</code></pre> <p>Helper for creating a DataLoader from existing information.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/dataloader.py</code> <pre><code>def create_from_info(data: pd.DataFrame | torch.utils.data.Dataset, info: dict) -&gt; DataLoader:\n    \"\"\"Helper for creating a DataLoader from existing information.\"\"\"\n    if info[\"data_type\"] == \"generic\":\n        return GenericDataLoader.from_info(data, info)\n    raise RuntimeError(f\"invalid datatype {info}\")\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.generation_quality.synthcity.feature_encoder","title":"feature_encoder","text":"FeatureEncoder \u00b6 <p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/feature_encoder.py</code> <pre><code>class FeatureEncoder(TransformerMixin, BaseEstimator):  # type: ignore\n    def __init__(self, n_dim_in: int = 1, n_dim_out: int = 2) -&gt; None:\n        \"\"\"\n        Base feature encoder with sklearn-style API.\n\n        Args:\n            n_dim_in: Size of the input to the feature encoder. Defaults to 1.\n            n_dim_out: Size of the output from the feature encoder. Defaults to 2.\n        \"\"\"\n        super().__init__()\n        self.n_dim_in = n_dim_in\n        self.n_dim_out = n_dim_out\n        self.n_features_out: int\n        self.feature_name_in: str\n        self.feature_names_out: list[str]\n        self.feature_types_out: list[str]\n        self.categorical: bool = False\n\n    def fit(self, x: pd.Series, y: Any = None, **kwargs: Any) -&gt; FeatureEncoder:\n        \"\"\"\n        Fit the feature encoder using the input Pandas series x and possibly the layers in the form of y.\n\n        Args:\n            x: Input for fitting the encoder\n            y: Optional labels that might be used in fitting the encoder. Defaults to None.\n            kwargs: Other settings to be processed\n\n        Returns:\n            The fitted FeatureEncoder object.\n        \"\"\"\n        self.feature_name_in = x.name\n        self.feature_type_in = self._get_feature_type(x)\n        input = validate_shape(x.values, self.n_dim_in)\n        output = self._fit(input, **kwargs)._transform(input)\n        self._out_shape = (-1, *output.shape[1:])  # for inverse_transform\n        output = validate_shape(output, self.n_dim_out)\n        if self.n_dim_out == 1:\n            self.n_features_out = 1\n        else:\n            self.n_features_out = output.shape[1]\n        self.feature_names_out = self.get_feature_names_out()\n        self.feature_types_out = self.get_feature_types_out(output)\n        return self\n\n    def _fit(self, x: np.ndarray, **kwargs: Any) -&gt; FeatureEncoder:\n        return self\n\n    def transform(self, x: pd.Series) -&gt; pd.DataFrame | pd.Series:\n        \"\"\"\n        Take in the Series and use the encoder to transform the series after fitting.\n\n        Args:\n            x: The input to be transformed.\n\n        Returns:\n            The transformed input.\n        \"\"\"\n        data = validate_shape(x.values, self.n_dim_in)\n        out = self._transform(data)\n        out = validate_shape(out, self.n_dim_out)\n        if self.n_dim_out == 1:\n            return pd.Series(out, name=self.feature_name_in)\n        return pd.DataFrame(out, columns=self.feature_names_out)\n\n    def _transform(self, x: np.ndarray) -&gt; np.ndarray:\n        return x\n\n    def get_feature_names_out(self) -&gt; list[str]:\n        \"\"\"A list of the names of the features being encoded.\"\"\"\n        n = self.n_features_out\n        if n == 1:\n            return [self.feature_name_in]\n        return [f\"{self.feature_name_in}_{i}\" for i in range(n)]\n\n    def get_feature_types_out(self, output: np.ndarray) -&gt; list[str]:\n        \"\"\"A list of the name of the features produced by the encoder.\"\"\"\n        t = self._get_feature_type(output)\n        return [t] * self.n_features_out\n\n    def _get_feature_type(self, x: Any) -&gt; str:\n        \"\"\"A string indicating the feature type associated with the input.\"\"\"\n        if self.categorical:\n            return \"discrete\"\n        if np.issubdtype(x.dtype, np.floating):\n            return \"continuous\"\n        if np.issubdtype(x.dtype, np.datetime64):\n            return \"datetime\"\n        return \"discrete\"\n\n    def inverse_transform(self, df: pd.DataFrame | pd.Series) -&gt; pd.Series:\n        \"\"\"Reverse the encoder mapping.\"\"\"\n        y = df.values.reshape(self._out_shape)\n        x = self._inverse_transform(y)\n        x = validate_shape(x, 1)\n        return pd.Series(x, name=self.feature_name_in)\n\n    def _inverse_transform(self, data: np.ndarray) -&gt; np.ndarray:\n        return data\n\n    @classmethod\n    def wraps(cls: type, encoder_class: TransformerMixin, **params: Any) -&gt; type[FeatureEncoder]:\n        \"\"\"Wraps sklearn transformer to FeatureEncoder.\"\"\"\n\n        class WrappedEncoder(FeatureEncoder):\n            def __init__(self, n_dim_in: int = 2, *args: Any, **kwargs: Any) -&gt; None:\n                self.encoder = encoder_class(n_dim_in, *args, **kwargs)\n\n            def _fit(self, x: np.ndarray, **kwargs: Any) -&gt; FeatureEncoder:\n                self.encoder.fit(x, **kwargs)\n                return self\n\n            def _transform(self, x: np.ndarray) -&gt; np.ndarray:\n                return self.encoder.transform(x)\n\n            def _inverse_transform(self, data: np.ndarray) -&gt; np.ndarray:\n                return self.encoder.inverse_transform(data)\n\n            def get_feature_names_out(self) -&gt; list[str]:\n                return list(self.encoder.get_feature_names_out([self.feature_name_in]))\n\n        for attr in (\"__name__\", \"__qualname__\", \"__doc__\"):\n            setattr(WrappedEncoder, attr, getattr(encoder_class, attr))\n        for attr, val in params.items():\n            setattr(WrappedEncoder, attr, val)\n\n        return WrappedEncoder\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(n_dim_in=1, n_dim_out=2)\n</code></pre> <p>Base feature encoder with sklearn-style API.</p> <p>Parameters:</p> Name Type Description Default <code>n_dim_in</code> <code>int</code> <p>Size of the input to the feature encoder. Defaults to 1.</p> <code>1</code> <code>n_dim_out</code> <code>int</code> <p>Size of the output from the feature encoder. Defaults to 2.</p> <code>2</code> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/feature_encoder.py</code> <pre><code>def __init__(self, n_dim_in: int = 1, n_dim_out: int = 2) -&gt; None:\n    \"\"\"\n    Base feature encoder with sklearn-style API.\n\n    Args:\n        n_dim_in: Size of the input to the feature encoder. Defaults to 1.\n        n_dim_out: Size of the output from the feature encoder. Defaults to 2.\n    \"\"\"\n    super().__init__()\n    self.n_dim_in = n_dim_in\n    self.n_dim_out = n_dim_out\n    self.n_features_out: int\n    self.feature_name_in: str\n    self.feature_names_out: list[str]\n    self.feature_types_out: list[str]\n    self.categorical: bool = False\n</code></pre> <code></code> fit \u00b6 <pre><code>fit(x, y=None, **kwargs)\n</code></pre> <p>Fit the feature encoder using the input Pandas series x and possibly the layers in the form of y.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>Input for fitting the encoder</p> required <code>y</code> <code>Any</code> <p>Optional labels that might be used in fitting the encoder. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Other settings to be processed</p> <code>{}</code> <p>Returns:</p> Type Description <code>FeatureEncoder</code> <p>The fitted FeatureEncoder object.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/feature_encoder.py</code> <pre><code>def fit(self, x: pd.Series, y: Any = None, **kwargs: Any) -&gt; FeatureEncoder:\n    \"\"\"\n    Fit the feature encoder using the input Pandas series x and possibly the layers in the form of y.\n\n    Args:\n        x: Input for fitting the encoder\n        y: Optional labels that might be used in fitting the encoder. Defaults to None.\n        kwargs: Other settings to be processed\n\n    Returns:\n        The fitted FeatureEncoder object.\n    \"\"\"\n    self.feature_name_in = x.name\n    self.feature_type_in = self._get_feature_type(x)\n    input = validate_shape(x.values, self.n_dim_in)\n    output = self._fit(input, **kwargs)._transform(input)\n    self._out_shape = (-1, *output.shape[1:])  # for inverse_transform\n    output = validate_shape(output, self.n_dim_out)\n    if self.n_dim_out == 1:\n        self.n_features_out = 1\n    else:\n        self.n_features_out = output.shape[1]\n    self.feature_names_out = self.get_feature_names_out()\n    self.feature_types_out = self.get_feature_types_out(output)\n    return self\n</code></pre> <code></code> transform \u00b6 <pre><code>transform(x)\n</code></pre> <p>Take in the Series and use the encoder to transform the series after fitting.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>The input to be transformed.</p> required <p>Returns:</p> Type Description <code>DataFrame | Series</code> <p>The transformed input.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/feature_encoder.py</code> <pre><code>def transform(self, x: pd.Series) -&gt; pd.DataFrame | pd.Series:\n    \"\"\"\n    Take in the Series and use the encoder to transform the series after fitting.\n\n    Args:\n        x: The input to be transformed.\n\n    Returns:\n        The transformed input.\n    \"\"\"\n    data = validate_shape(x.values, self.n_dim_in)\n    out = self._transform(data)\n    out = validate_shape(out, self.n_dim_out)\n    if self.n_dim_out == 1:\n        return pd.Series(out, name=self.feature_name_in)\n    return pd.DataFrame(out, columns=self.feature_names_out)\n</code></pre> <code></code> get_feature_names_out \u00b6 <pre><code>get_feature_names_out()\n</code></pre> <p>A list of the names of the features being encoded.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/feature_encoder.py</code> <pre><code>def get_feature_names_out(self) -&gt; list[str]:\n    \"\"\"A list of the names of the features being encoded.\"\"\"\n    n = self.n_features_out\n    if n == 1:\n        return [self.feature_name_in]\n    return [f\"{self.feature_name_in}_{i}\" for i in range(n)]\n</code></pre> <code></code> get_feature_types_out \u00b6 <pre><code>get_feature_types_out(output)\n</code></pre> <p>A list of the name of the features produced by the encoder.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/feature_encoder.py</code> <pre><code>def get_feature_types_out(self, output: np.ndarray) -&gt; list[str]:\n    \"\"\"A list of the name of the features produced by the encoder.\"\"\"\n    t = self._get_feature_type(output)\n    return [t] * self.n_features_out\n</code></pre> <code></code> inverse_transform \u00b6 <pre><code>inverse_transform(df)\n</code></pre> <p>Reverse the encoder mapping.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/feature_encoder.py</code> <pre><code>def inverse_transform(self, df: pd.DataFrame | pd.Series) -&gt; pd.Series:\n    \"\"\"Reverse the encoder mapping.\"\"\"\n    y = df.values.reshape(self._out_shape)\n    x = self._inverse_transform(y)\n    x = validate_shape(x, 1)\n    return pd.Series(x, name=self.feature_name_in)\n</code></pre> <code></code> wraps <code>classmethod</code> \u00b6 <pre><code>wraps(encoder_class, **params)\n</code></pre> <p>Wraps sklearn transformer to FeatureEncoder.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/feature_encoder.py</code> <pre><code>@classmethod\ndef wraps(cls: type, encoder_class: TransformerMixin, **params: Any) -&gt; type[FeatureEncoder]:\n    \"\"\"Wraps sklearn transformer to FeatureEncoder.\"\"\"\n\n    class WrappedEncoder(FeatureEncoder):\n        def __init__(self, n_dim_in: int = 2, *args: Any, **kwargs: Any) -&gt; None:\n            self.encoder = encoder_class(n_dim_in, *args, **kwargs)\n\n        def _fit(self, x: np.ndarray, **kwargs: Any) -&gt; FeatureEncoder:\n            self.encoder.fit(x, **kwargs)\n            return self\n\n        def _transform(self, x: np.ndarray) -&gt; np.ndarray:\n            return self.encoder.transform(x)\n\n        def _inverse_transform(self, data: np.ndarray) -&gt; np.ndarray:\n            return self.encoder.inverse_transform(data)\n\n        def get_feature_names_out(self) -&gt; list[str]:\n            return list(self.encoder.get_feature_names_out([self.feature_name_in]))\n\n    for attr in (\"__name__\", \"__qualname__\", \"__doc__\"):\n        setattr(WrappedEncoder, attr, getattr(encoder_class, attr))\n    for attr, val in params.items():\n        setattr(WrappedEncoder, attr, val)\n\n    return WrappedEncoder\n</code></pre> <code></code> DatetimeEncoder \u00b6 <p>               Bases: <code>FeatureEncoder</code></p> <p>Datetime variables encoder.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/feature_encoder.py</code> <pre><code>class DatetimeEncoder(FeatureEncoder):\n    \"\"\"Datetime variables encoder.\"\"\"\n\n    def __init__(self, n_dim_in: int = 1, n_dim_out: int = 1):\n        \"\"\"\n        Datetime variables encoder.\n\n        Args:\n            n_dim_in: Size of the input to the feature encoder. Defaults to 1.\n            n_dim_out: Size of the output from the feature encoder. Defaults to 1.\n        \"\"\"\n        super().__init__(n_dim_in, n_dim_out)\n\n    def _transform(self, x: np.ndarray) -&gt; np.ndarray:\n        return pd.to_numeric(x).astype(float)\n\n    def _inverse_transform(self, data: np.ndarray) -&gt; np.ndarray:\n        return pd.to_datetime(data)\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(n_dim_in=1, n_dim_out=1)\n</code></pre> <p>Datetime variables encoder.</p> <p>Parameters:</p> Name Type Description Default <code>n_dim_in</code> <code>int</code> <p>Size of the input to the feature encoder. Defaults to 1.</p> <code>1</code> <code>n_dim_out</code> <code>int</code> <p>Size of the output from the feature encoder. Defaults to 1.</p> <code>1</code> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/feature_encoder.py</code> <pre><code>def __init__(self, n_dim_in: int = 1, n_dim_out: int = 1):\n    \"\"\"\n    Datetime variables encoder.\n\n    Args:\n        n_dim_in: Size of the input to the feature encoder. Defaults to 1.\n        n_dim_out: Size of the output from the feature encoder. Defaults to 1.\n    \"\"\"\n    super().__init__(n_dim_in, n_dim_out)\n</code></pre> <code></code> validate_shape \u00b6 <pre><code>validate_shape(x, n_dim)\n</code></pre> <p>Perform validation of the shape of x against the specified <code>n_dim</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Numpy array to be validated</p> required <code>n_dim</code> <code>int</code> <p>value that determines the validation</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Thrown when there is a dissonance between the <code>n_dim</code> and data shape of x)</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Returns the data, as long as it has the right shape.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/feature_encoder.py</code> <pre><code>def validate_shape(x: np.ndarray, n_dim: int) -&gt; np.ndarray:\n    \"\"\"\n    Perform validation of the shape of x against the specified ``n_dim``.\n\n    Args:\n        x: Numpy array to be validated\n        n_dim: value that determines the validation\n\n    Raises:\n        ValueError: Thrown when there is a dissonance between the ``n_dim`` and data shape of x)\n\n    Returns:\n        Returns the data, as long as it has the right shape.\n    \"\"\"\n    if n_dim == 1:\n        if x.ndim == 2:\n            x = np.squeeze(x, axis=1)\n        if x.ndim != 1:\n            raise ValueError(\"array must be 1D\")\n        return x\n    if n_dim == 2:\n        if x.ndim == 1:\n            x = x.reshape(-1, 1)\n        if x.ndim != 2:\n            raise ValueError(\"array must be 2D\")\n        return x\n    raise ValueError(\"n_dim must be 1 or 2\")\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.generation_quality.synthcity.metric","title":"metric","text":"MetricEvaluator \u00b6 Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/metric.py</code> <pre><code>class MetricEvaluator(metaclass=ABCMeta):\n    def __init__(\n        self,\n        reduction: str = \"mean\",\n        n_histogram_bins: int = 10,\n        n_folds: int = 3,\n        task_type: str = \"classification\",\n        random_state: int = 0,\n        workspace: Path = Path(\"workspace\"),\n        use_cache: bool = True,\n        default_metric: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Base class for all metrics.\n\n        If any method implementation is missing, the class constructor will fail.\n\n        Args:\n            reduction: The way to aggregate metrics across folds. Defaults to \"mean\".\n            n_histogram_bins: The number of bins used in histogram calculation. Defaults to 10.\n            n_folds: The number of folds in cross validation. Defaults to 3.\n            task_type: The type of downstream task.. Defaults to \"classification\".\n            random_state: Random state seed. Defaults to 0.\n            workspace: The directory to save intermediate models or results.. Defaults to Path(\"workspace\").\n            use_cache:  Whether to use cache. If True, it will try to load saved results in workspace directory\n                where possible. Defaults to True.\n            default_metric: Type of metric to be used if one not specified. Defaults to None.\n        \"\"\"\n        self._reduction = reduction\n        self._n_histogram_bins = n_histogram_bins\n        self._n_folds = n_folds\n\n        self._task_type = task_type\n        self._random_state = random_state\n        self._workspace = workspace\n        self._use_cache = use_cache\n        if default_metric is None:\n            default_metric = reduction\n        self._default_metric = default_metric\n\n        workspace.mkdir(parents=True, exist_ok=True)\n\n    @abstractmethod\n    def evaluate(self, x_gt: DataLoader, x_syn: DataLoader) -&gt; dict:\n        \"\"\"Compare two datasets and return a dictionary of metrics.\"\"\"\n        ...\n\n    @abstractmethod\n    def evaluate_default(self, x_gt: DataLoader, x_syn: DataLoader) -&gt; float:\n        \"\"\"Default evaluation.\"\"\"\n        ...\n\n    @staticmethod\n    @abstractmethod\n    def direction() -&gt; str:\n        \"\"\"Direction of metric (bigger better or smaller better).\"\"\"\n        ...\n\n    @staticmethod\n    @abstractmethod\n    def type() -&gt; str:\n        \"\"\"Type of metric.\"\"\"\n        ...\n\n    @staticmethod\n    @abstractmethod\n    def name() -&gt; str:\n        \"\"\"Name of the metric.\"\"\"\n        ...\n\n    @classmethod\n    def fqdn(cls) -&gt; str:\n        \"\"\"No idea.\"\"\"\n        return f\"{cls.type()}.{cls.name()}\"\n\n    def reduction(self) -&gt; Callable:\n        \"\"\"The way in which the input should be reduced if necessary.\"\"\"\n        if self._reduction == \"mean\":\n            return np.mean\n        if self._reduction == \"max\":\n            return np.max\n        if self._reduction == \"min\":\n            return np.min\n        raise ValueError(f\"Unknown reduction {self._reduction}\")\n\n    def _get_oneclass_model(self, x_gt: np.ndarray) -&gt; OneClassLayer:\n        model = OneClassLayer(\n            input_dim=x_gt.shape[1],\n            rep_dim=x_gt.shape[1],\n            center=torch.ones(x_gt.shape[1]) * 10,\n        )\n        model.fit(torch.from_numpy(x_gt))\n\n        return model.to(DEVICE)\n\n    def _oneclass_predict(self, model: OneClassLayer, X: np.ndarray) -&gt; np.ndarray:\n        with torch.no_grad():\n            return model(torch.from_numpy(X).float().to(DEVICE)).cpu().detach().numpy()\n\n    def use_cache(self, path: Path) -&gt; bool:\n        \"\"\"Whether to save information to the provided path.\"\"\"\n        return path.exists() and self._use_cache\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(\n    reduction=\"mean\",\n    n_histogram_bins=10,\n    n_folds=3,\n    task_type=\"classification\",\n    random_state=0,\n    workspace=Path(\"workspace\"),\n    use_cache=True,\n    default_metric=None,\n)\n</code></pre> <p>Base class for all metrics.</p> <p>If any method implementation is missing, the class constructor will fail.</p> <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>str</code> <p>The way to aggregate metrics across folds. Defaults to \"mean\".</p> <code>'mean'</code> <code>n_histogram_bins</code> <code>int</code> <p>The number of bins used in histogram calculation. Defaults to 10.</p> <code>10</code> <code>n_folds</code> <code>int</code> <p>The number of folds in cross validation. Defaults to 3.</p> <code>3</code> <code>task_type</code> <code>str</code> <p>The type of downstream task.. Defaults to \"classification\".</p> <code>'classification'</code> <code>random_state</code> <code>int</code> <p>Random state seed. Defaults to 0.</p> <code>0</code> <code>workspace</code> <code>Path</code> <p>The directory to save intermediate models or results.. Defaults to Path(\"workspace\").</p> <code>Path('workspace')</code> <code>use_cache</code> <code>bool</code> <p>Whether to use cache. If True, it will try to load saved results in workspace directory where possible. Defaults to True.</p> <code>True</code> <code>default_metric</code> <code>str | None</code> <p>Type of metric to be used if one not specified. Defaults to None.</p> <code>None</code> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/metric.py</code> <pre><code>def __init__(\n    self,\n    reduction: str = \"mean\",\n    n_histogram_bins: int = 10,\n    n_folds: int = 3,\n    task_type: str = \"classification\",\n    random_state: int = 0,\n    workspace: Path = Path(\"workspace\"),\n    use_cache: bool = True,\n    default_metric: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Base class for all metrics.\n\n    If any method implementation is missing, the class constructor will fail.\n\n    Args:\n        reduction: The way to aggregate metrics across folds. Defaults to \"mean\".\n        n_histogram_bins: The number of bins used in histogram calculation. Defaults to 10.\n        n_folds: The number of folds in cross validation. Defaults to 3.\n        task_type: The type of downstream task.. Defaults to \"classification\".\n        random_state: Random state seed. Defaults to 0.\n        workspace: The directory to save intermediate models or results.. Defaults to Path(\"workspace\").\n        use_cache:  Whether to use cache. If True, it will try to load saved results in workspace directory\n            where possible. Defaults to True.\n        default_metric: Type of metric to be used if one not specified. Defaults to None.\n    \"\"\"\n    self._reduction = reduction\n    self._n_histogram_bins = n_histogram_bins\n    self._n_folds = n_folds\n\n    self._task_type = task_type\n    self._random_state = random_state\n    self._workspace = workspace\n    self._use_cache = use_cache\n    if default_metric is None:\n        default_metric = reduction\n    self._default_metric = default_metric\n\n    workspace.mkdir(parents=True, exist_ok=True)\n</code></pre> <code></code> evaluate <code>abstractmethod</code> \u00b6 <pre><code>evaluate(x_gt, x_syn)\n</code></pre> <p>Compare two datasets and return a dictionary of metrics.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/metric.py</code> <pre><code>@abstractmethod\ndef evaluate(self, x_gt: DataLoader, x_syn: DataLoader) -&gt; dict:\n    \"\"\"Compare two datasets and return a dictionary of metrics.\"\"\"\n    ...\n</code></pre> <code></code> evaluate_default <code>abstractmethod</code> \u00b6 <pre><code>evaluate_default(x_gt, x_syn)\n</code></pre> <p>Default evaluation.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/metric.py</code> <pre><code>@abstractmethod\ndef evaluate_default(self, x_gt: DataLoader, x_syn: DataLoader) -&gt; float:\n    \"\"\"Default evaluation.\"\"\"\n    ...\n</code></pre> <code></code> direction <code>abstractmethod</code> <code>staticmethod</code> \u00b6 <pre><code>direction()\n</code></pre> <p>Direction of metric (bigger better or smaller better).</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/metric.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef direction() -&gt; str:\n    \"\"\"Direction of metric (bigger better or smaller better).\"\"\"\n    ...\n</code></pre> <code></code> type <code>abstractmethod</code> <code>staticmethod</code> \u00b6 <pre><code>type()\n</code></pre> <p>Type of metric.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/metric.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef type() -&gt; str:\n    \"\"\"Type of metric.\"\"\"\n    ...\n</code></pre> <code></code> name <code>abstractmethod</code> <code>staticmethod</code> \u00b6 <pre><code>name()\n</code></pre> <p>Name of the metric.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/metric.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef name() -&gt; str:\n    \"\"\"Name of the metric.\"\"\"\n    ...\n</code></pre> <code></code> fqdn <code>classmethod</code> \u00b6 <pre><code>fqdn()\n</code></pre> <p>No idea.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/metric.py</code> <pre><code>@classmethod\ndef fqdn(cls) -&gt; str:\n    \"\"\"No idea.\"\"\"\n    return f\"{cls.type()}.{cls.name()}\"\n</code></pre> <code></code> reduction \u00b6 <pre><code>reduction()\n</code></pre> <p>The way in which the input should be reduced if necessary.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/metric.py</code> <pre><code>def reduction(self) -&gt; Callable:\n    \"\"\"The way in which the input should be reduced if necessary.\"\"\"\n    if self._reduction == \"mean\":\n        return np.mean\n    if self._reduction == \"max\":\n        return np.max\n    if self._reduction == \"min\":\n        return np.min\n    raise ValueError(f\"Unknown reduction {self._reduction}\")\n</code></pre> <code></code> use_cache \u00b6 <pre><code>use_cache(path)\n</code></pre> <p>Whether to save information to the provided path.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/metric.py</code> <pre><code>def use_cache(self, path: Path) -&gt; bool:\n    \"\"\"Whether to save information to the provided path.\"\"\"\n    return path.exists() and self._use_cache\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.generation_quality.synthcity.networks","title":"networks","text":"build_network \u00b6 <pre><code>build_network(network_name, params)\n</code></pre> <p>Placeholder for now. Would be a factory type method if there where more than one option.</p> <p>Parameters:</p> Name Type Description Default <code>network_name</code> <code>str</code> <p>Name of the network to be generated.</p> required <code>params</code> <code>dict</code> <p>Parameters/configuration used to create the network in a custom way.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>A torch module to be trained.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/networks.py</code> <pre><code>def build_network(network_name: str, params: dict) -&gt; nn.Module:\n    \"\"\"\n    Placeholder for now. Would be a factory type method if there where more than one option.\n\n    Args:\n        network_name: Name of the network to be generated.\n        params: Parameters/configuration used to create the network in a custom way.\n\n    Returns:\n        A torch module to be trained.\n    \"\"\"\n    if network_name == \"feedforward\":\n        return feedforward_network(params)\n    raise ValueError(\"Network name not recognized.\")\n</code></pre> <code></code> feedforward_network \u00b6 <pre><code>feedforward_network(params)\n</code></pre> <p>Architecture for a Feedforward Neural Network.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Has keys [\"input_dim\", \"rep_dim\", \"num_hidden\", \"activation\", \"num_layers\", \"dropout_prob\", \"dropout_active\", \"LossFn\". These determine the architecture structure</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The constructed network.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/networks.py</code> <pre><code>def feedforward_network(params: dict) -&gt; nn.Module:\n    \"\"\"\n    Architecture for a Feedforward Neural Network.\n\n    Args:\n        params: Has keys [\"input_dim\", \"rep_dim\", \"num_hidden\", \"activation\", \"num_layers\", \"dropout_prob\",\n            \"dropout_active\", \"LossFn\". These determine the architecture structure\n\n    Returns:\n        The constructed network.\n    \"\"\"\n    modules: list[nn.Module] = []\n\n    if params[\"dropout_active\"]:\n        modules.append(torch.nn.Dropout(p=params[\"dropout_prob\"]))\n\n    # Input layer\n\n    modules.append(torch.nn.Linear(params[\"input_dim\"], params[\"num_hidden\"], bias=False))\n    modules.append(ACTIVATION_DICT[params[\"activation\"]])\n\n    # Intermediate layers\n\n    for _ in range(params[\"num_layers\"] - 1):\n        if params[\"dropout_active\"]:\n            modules.append(torch.nn.Dropout(p=params[\"dropout_prob\"]))\n\n        modules.append(torch.nn.Linear(params[\"num_hidden\"], params[\"num_hidden\"], bias=False))\n        modules.append(ACTIVATION_DICT[params[\"activation\"]])\n\n    # Output layer\n\n    modules.append(torch.nn.Linear(params[\"num_hidden\"], params[\"rep_dim\"], bias=False))\n\n    return nn.Sequential(*modules)\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.generation_quality.synthcity.one_class","title":"one_class","text":"BaseNet \u00b6 <p>               Bases: <code>Module</code></p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/one_class.py</code> <pre><code>class BaseNet(nn.Module):\n    def __init__(self) -&gt; None:\n        \"\"\"Base class for all neural networks.\"\"\"\n        super().__init__()\n\n    def forward(self, X: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Abstract forward pass through the network.\n\n        Args:\n            X: input to the network\n\n        Raises:\n            NotImplementedError: Must be implemented by the inheriting network\n\n        Returns:\n            Output of the network\n        \"\"\"\n        raise NotImplementedError\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__()\n</code></pre> <p>Base class for all neural networks.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/one_class.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Base class for all neural networks.\"\"\"\n    super().__init__()\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(X)\n</code></pre> <p>Abstract forward pass through the network.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>input to the network</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented by the inheriting network</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output of the network</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/one_class.py</code> <pre><code>def forward(self, X: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Abstract forward pass through the network.\n\n    Args:\n        X: input to the network\n\n    Raises:\n        NotImplementedError: Must be implemented by the inheriting network\n\n    Returns:\n        Output of the network\n    \"\"\"\n    raise NotImplementedError\n</code></pre> <code></code> OneClassLayer \u00b6 <p>               Bases: <code>BaseNet</code></p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/one_class.py</code> <pre><code>class OneClassLayer(BaseNet):\n    def __init__(\n        self,\n        input_dim: int,\n        rep_dim: int,\n        center: torch.Tensor,\n        num_layers: int = 4,\n        num_hidden: int = 32,\n        activation: str = \"ReLU\",\n        dropout_prob: float = 0.2,\n        dropout_active: bool = False,\n        lr: float = 2e-3,\n        epochs: int = 1000,\n        warm_up_epochs: int = 20,\n        train_prop: float = 1.0,\n        weight_decay: float = 2e-3,\n        radius: float = 1,\n        nu: float = 1e-2,\n    ):\n        \"\"\"Neural network.\"\"\"\n        super().__init__()\n\n        self.rep_dim = rep_dim\n        self.input_dim = input_dim\n        self.num_layers = num_layers\n        self.num_hidden = num_hidden\n        self.activation = activation\n        self.dropout_prob = dropout_prob\n        self.dropout_active = dropout_active\n        self.train_prop = train_prop\n        self.learningRate = lr\n        self.epochs = epochs\n        self.warm_up_epochs = warm_up_epochs\n        self.weight_decay = weight_decay\n        if torch.cuda.is_available():\n            self.device = torch.device(\"cuda\")  # Make this an option\n        else:\n            self.device = torch.device(\"cpu\")\n        # set up the network\n\n        self.model = build_network(\n            network_name=\"feedforward\",\n            params={\n                \"input_dim\": input_dim,\n                \"rep_dim\": rep_dim,\n                \"num_hidden\": num_hidden,\n                \"activation\": activation,\n                \"num_layers\": num_layers,\n                \"dropout_prob\": dropout_prob,\n                \"dropout_active\": dropout_active,\n                \"LossFn\": \"SoftBoundary\",\n            },\n        ).to(self.device)\n\n        # create the loss function\n\n        self.c = center.to(self.device)\n        self.r = radius\n        self.nu = nu\n\n        self.loss_fn = soft_boundary_loss\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Pass the input through the network for a forward pass.\n\n        Args:\n            x: Input tensor\n\n        Returns:\n            Output tensors.\n        \"\"\"\n        return self.model(x)\n\n    def fit(self, x_train: torch.Tensor) -&gt; None:\n        \"\"\"\n        Perform a training step using the ``x_train`` tensor.\n\n        Args:\n            x_train: Batch of training data.\n        \"\"\"\n        self.optimizer = torch.optim.AdamW(\n            self.model.parameters(),\n            lr=self.learningRate,\n            weight_decay=self.weight_decay,\n        )\n        self.X = torch.tensor(x_train.reshape((-1, self.input_dim))).float()\n\n        if self.train_prop != 1:\n            x_train, x_val = (\n                x_train[: int(self.train_prop * len(x_train))],\n                x_train[int(self.train_prop * len(x_train)) :],\n            )\n            inputs_val = Variable(x_val.to(self.device)).float()\n\n        self.losses = []\n        self.loss_vals = []\n\n        for epoch in range(self.epochs):\n            # Converting inputs and labels to Variable\n            inputs = Variable(x_train).to(self.device).float()\n\n            self.model.zero_grad()\n\n            self.optimizer.zero_grad()\n\n            # get output from the model, given the inputs\n            outputs = self.model(inputs)\n\n            # get loss for the predicted output\n            self.loss = self.loss_fn(outputs=outputs, r=torch.Tensor([self.r]), c=self.c, nu=torch.Tensor([self.nu]))\n\n            # get gradients w.r.t to parameters\n            self.loss.backward(retain_graph=True)\n            self.losses.append(self.loss.detach().cpu().numpy())\n\n            # update parameters\n            self.optimizer.step()\n\n            if self.train_prop != 1.0:\n                with torch.no_grad():\n                    # get output from the model, given the inputs\n                    outputs = self.model(inputs_val)\n\n                    # get loss for the predicted output\n                    loss_val = self.loss_fn(\n                        outputs=outputs, r=torch.Tensor([self.r]), c=self.c, nu=torch.Tensor([self.nu])\n                    )\n\n                    self.loss_vals.append(loss_val)\n\n            if self.train_prop == 1:\n                log(DEBUG, \"epoch {}, loss {}\".format(epoch, self.loss.item()))\n            else:\n                log(DEBUG, \"epoch {:4}, train loss {:.4e}, val loss {:.4e}\".format(epoch, self.loss.item(), loss_val))\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(\n    input_dim,\n    rep_dim,\n    center,\n    num_layers=4,\n    num_hidden=32,\n    activation=\"ReLU\",\n    dropout_prob=0.2,\n    dropout_active=False,\n    lr=0.002,\n    epochs=1000,\n    warm_up_epochs=20,\n    train_prop=1.0,\n    weight_decay=0.002,\n    radius=1,\n    nu=0.01,\n)\n</code></pre> <p>Neural network.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/one_class.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    rep_dim: int,\n    center: torch.Tensor,\n    num_layers: int = 4,\n    num_hidden: int = 32,\n    activation: str = \"ReLU\",\n    dropout_prob: float = 0.2,\n    dropout_active: bool = False,\n    lr: float = 2e-3,\n    epochs: int = 1000,\n    warm_up_epochs: int = 20,\n    train_prop: float = 1.0,\n    weight_decay: float = 2e-3,\n    radius: float = 1,\n    nu: float = 1e-2,\n):\n    \"\"\"Neural network.\"\"\"\n    super().__init__()\n\n    self.rep_dim = rep_dim\n    self.input_dim = input_dim\n    self.num_layers = num_layers\n    self.num_hidden = num_hidden\n    self.activation = activation\n    self.dropout_prob = dropout_prob\n    self.dropout_active = dropout_active\n    self.train_prop = train_prop\n    self.learningRate = lr\n    self.epochs = epochs\n    self.warm_up_epochs = warm_up_epochs\n    self.weight_decay = weight_decay\n    if torch.cuda.is_available():\n        self.device = torch.device(\"cuda\")  # Make this an option\n    else:\n        self.device = torch.device(\"cpu\")\n    # set up the network\n\n    self.model = build_network(\n        network_name=\"feedforward\",\n        params={\n            \"input_dim\": input_dim,\n            \"rep_dim\": rep_dim,\n            \"num_hidden\": num_hidden,\n            \"activation\": activation,\n            \"num_layers\": num_layers,\n            \"dropout_prob\": dropout_prob,\n            \"dropout_active\": dropout_active,\n            \"LossFn\": \"SoftBoundary\",\n        },\n    ).to(self.device)\n\n    # create the loss function\n\n    self.c = center.to(self.device)\n    self.r = radius\n    self.nu = nu\n\n    self.loss_fn = soft_boundary_loss\n</code></pre> <code></code> forward \u00b6 <pre><code>forward(x)\n</code></pre> <p>Pass the input through the network for a forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensors.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/one_class.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Pass the input through the network for a forward pass.\n\n    Args:\n        x: Input tensor\n\n    Returns:\n        Output tensors.\n    \"\"\"\n    return self.model(x)\n</code></pre> <code></code> fit \u00b6 <pre><code>fit(x_train)\n</code></pre> <p>Perform a training step using the <code>x_train</code> tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>Tensor</code> <p>Batch of training data.</p> required Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/one_class.py</code> <pre><code>def fit(self, x_train: torch.Tensor) -&gt; None:\n    \"\"\"\n    Perform a training step using the ``x_train`` tensor.\n\n    Args:\n        x_train: Batch of training data.\n    \"\"\"\n    self.optimizer = torch.optim.AdamW(\n        self.model.parameters(),\n        lr=self.learningRate,\n        weight_decay=self.weight_decay,\n    )\n    self.X = torch.tensor(x_train.reshape((-1, self.input_dim))).float()\n\n    if self.train_prop != 1:\n        x_train, x_val = (\n            x_train[: int(self.train_prop * len(x_train))],\n            x_train[int(self.train_prop * len(x_train)) :],\n        )\n        inputs_val = Variable(x_val.to(self.device)).float()\n\n    self.losses = []\n    self.loss_vals = []\n\n    for epoch in range(self.epochs):\n        # Converting inputs and labels to Variable\n        inputs = Variable(x_train).to(self.device).float()\n\n        self.model.zero_grad()\n\n        self.optimizer.zero_grad()\n\n        # get output from the model, given the inputs\n        outputs = self.model(inputs)\n\n        # get loss for the predicted output\n        self.loss = self.loss_fn(outputs=outputs, r=torch.Tensor([self.r]), c=self.c, nu=torch.Tensor([self.nu]))\n\n        # get gradients w.r.t to parameters\n        self.loss.backward(retain_graph=True)\n        self.losses.append(self.loss.detach().cpu().numpy())\n\n        # update parameters\n        self.optimizer.step()\n\n        if self.train_prop != 1.0:\n            with torch.no_grad():\n                # get output from the model, given the inputs\n                outputs = self.model(inputs_val)\n\n                # get loss for the predicted output\n                loss_val = self.loss_fn(\n                    outputs=outputs, r=torch.Tensor([self.r]), c=self.c, nu=torch.Tensor([self.nu])\n                )\n\n                self.loss_vals.append(loss_val)\n\n        if self.train_prop == 1:\n            log(DEBUG, \"epoch {}, loss {}\".format(epoch, self.loss.item()))\n        else:\n            log(DEBUG, \"epoch {:4}, train loss {:.4e}, val loss {:.4e}\".format(epoch, self.loss.item(), loss_val))\n</code></pre> <code></code> one_class_loss \u00b6 <pre><code>one_class_loss(outputs, c)\n</code></pre> <p>Computes the sum of the Euclidean distances from the center tensor (c) and then the mean over the sum.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>Tensor</code> <p>Output from the neural network for the batch.</p> required <code>c</code> <code>Tensor</code> <p>center point, from which we're measuring the Euclidean distances.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Mean distances.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/one_class.py</code> <pre><code>def one_class_loss(outputs: torch.Tensor, c: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the sum of the Euclidean distances from the center tensor (c) and then the mean over the sum.\n\n    Args:\n        outputs: Output from the neural network for the batch.\n        c: center point, from which we're measuring the Euclidean distances.\n\n    Returns:\n        Mean distances.\n    \"\"\"\n    dist = torch.sum((outputs - c) ** 2, dim=1)\n    return torch.mean(dist)\n</code></pre> <code></code> soft_boundary_loss \u00b6 <pre><code>soft_boundary_loss(outputs, r, c, nu)\n</code></pre> <p>A similar loss function to the one class loss but with some small modifications.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/one_class.py</code> <pre><code>def soft_boundary_loss(outputs: torch.Tensor, r: torch.Tensor, c: torch.Tensor, nu: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"A similar loss function to the one class loss but with some small modifications.\"\"\"\n    dist = torch.sum((outputs - c) ** 2, dim=1)\n    scores = dist\n    return (1 / nu) * torch.mean(torch.max(torch.zeros_like(scores), scores))\n</code></pre> <code></code> get_radius \u00b6 <pre><code>get_radius(dist, nu)\n</code></pre> <p>Optimally solve for radius R via the (1-nu)-quantile of distances.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <code>Tensor</code> <p>Distances tensor</p> required <code>nu</code> <code>float</code> <p>hyper-parameter for the quantile</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Radii</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/one_class.py</code> <pre><code>def get_radius(dist: torch.Tensor, nu: float) -&gt; np.ndarray:\n    \"\"\"\n    Optimally solve for radius R via the (1-nu)-quantile of distances.\n\n    Args:\n        dist: Distances tensor\n        nu: hyper-parameter for the quantile\n\n    Returns:\n        Radii\n    \"\"\"\n    return np.quantile(np.sqrt(dist.clone().data.float().cpu().numpy()), 1 - nu)\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.generation_quality.synthcity.statistical_eval","title":"statistical_eval","text":"StatisticalEvaluator \u00b6 <p>               Bases: <code>MetricEvaluator</code></p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/statistical_eval.py</code> <pre><code>class StatisticalEvaluator(MetricEvaluator):\n    def __init__(self, **kwargs: Any) -&gt; None:\n        \"\"\"Base class for statistical evaluators to inherit from.\"\"\"\n        super().__init__(**kwargs)\n\n    @staticmethod\n    def type() -&gt; str:\n        \"\"\"Type of evaluator.\"\"\"\n        return \"stats\"\n\n    @abstractmethod\n    def _evaluate(self, X_gt: DataLoader, X_syn: DataLoader) -&gt; dict: ...\n\n    def evaluate(self, X_gt: DataLoader, X_syn: DataLoader) -&gt; dict:\n        \"\"\"\n        Performs evaluation using the ground truth and synthetic datasets as dataloaders by calling the internal\n        ``_evaluate`` function of inheriting classes.\n\n        Args:\n            X_gt: Dataloader with ground truth (real) data.\n            X_syn: Dataloader with synthetically generated data.\n\n        Returns:\n            A dictionary of results from the evaluation\n        \"\"\"\n        return self._evaluate(X_gt, X_syn)\n\n    def evaluate_default(\n        self,\n        x_gt: DataLoader,\n        x_syn: DataLoader,\n    ) -&gt; float:\n        \"\"\"Perform a default evaluation if one is not specified.\"\"\"\n        return self.evaluate(x_gt, x_syn)[self._default_metric]\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(**kwargs)\n</code></pre> <p>Base class for statistical evaluators to inherit from.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/statistical_eval.py</code> <pre><code>def __init__(self, **kwargs: Any) -&gt; None:\n    \"\"\"Base class for statistical evaluators to inherit from.\"\"\"\n    super().__init__(**kwargs)\n</code></pre> <code></code> type <code>staticmethod</code> \u00b6 <pre><code>type()\n</code></pre> <p>Type of evaluator.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/statistical_eval.py</code> <pre><code>@staticmethod\ndef type() -&gt; str:\n    \"\"\"Type of evaluator.\"\"\"\n    return \"stats\"\n</code></pre> <code></code> evaluate \u00b6 <pre><code>evaluate(X_gt, X_syn)\n</code></pre> <p>Performs evaluation using the ground truth and synthetic datasets as dataloaders by calling the internal <code>_evaluate</code> function of inheriting classes.</p> <p>Parameters:</p> Name Type Description Default <code>X_gt</code> <code>DataLoader</code> <p>Dataloader with ground truth (real) data.</p> required <code>X_syn</code> <code>DataLoader</code> <p>Dataloader with synthetically generated data.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary of results from the evaluation</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/statistical_eval.py</code> <pre><code>def evaluate(self, X_gt: DataLoader, X_syn: DataLoader) -&gt; dict:\n    \"\"\"\n    Performs evaluation using the ground truth and synthetic datasets as dataloaders by calling the internal\n    ``_evaluate`` function of inheriting classes.\n\n    Args:\n        X_gt: Dataloader with ground truth (real) data.\n        X_syn: Dataloader with synthetically generated data.\n\n    Returns:\n        A dictionary of results from the evaluation\n    \"\"\"\n    return self._evaluate(X_gt, X_syn)\n</code></pre> <code></code> evaluate_default \u00b6 <pre><code>evaluate_default(x_gt, x_syn)\n</code></pre> <p>Perform a default evaluation if one is not specified.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/statistical_eval.py</code> <pre><code>def evaluate_default(\n    self,\n    x_gt: DataLoader,\n    x_syn: DataLoader,\n) -&gt; float:\n    \"\"\"Perform a default evaluation if one is not specified.\"\"\"\n    return self.evaluate(x_gt, x_syn)[self._default_metric]\n</code></pre> <code></code> AlphaPrecision \u00b6 <p>               Bases: <code>StatisticalEvaluator</code></p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/statistical_eval.py</code> <pre><code>class AlphaPrecision(StatisticalEvaluator):\n    def __init__(self, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Evaluates the alpha-precision, beta-recall, and authenticity scores.\n\n        The class evaluates the synthetic data using a tuple of three metrics:\n\n            alpha-precision, beta-recall, and authenticity.\n\n        Note that these metrics can be evaluated for each synthetic data point (which are useful for auditing and\n        post-processing). Here we average the scores to reflect the overall quality of the data.\n        The formal definitions can be found in the reference below:\n\n        Alaa, Ahmed, Boris Van Breugel, Evgeny S. Saveliev, and Mihaela van der Schaar. \"How faithful is your synthetic\n        data? sample-level metrics for evaluating and auditing generative models.\"\n        In International Conference on Machine Learning, pp. 290-306. PMLR, 2022.\n        \"\"\"\n        super().__init__(default_metric=\"authenticity_OC\", **kwargs)\n\n    @staticmethod\n    def name() -&gt; str:\n        \"\"\"Return name.\"\"\"\n        return \"alpha_precision\"\n\n    @staticmethod\n    def direction() -&gt; str:\n        \"\"\"Return optimization direction.\"\"\"\n        return \"maximize\"\n\n    def metrics(\n        self,\n        x: np.ndarray,\n        x_syn: np.ndarray,\n        emb_center: np.ndarray | None = None,\n    ) -&gt; tuple[list[float], list[float], list[float], float, float, float]:\n        \"\"\"\n        Compute the alpha-precision, beta-recall, and authenticity scores provided real data (x) and synthetic\n        data (x_syn). If ``emb_center`` is provided this are \"non-naive\" metrics. If it is none, these constitute\n        \"naive\" scores.\n\n        Args:\n            x: Real data\n            x_syn: Synthetically generated data.\n            emb_center: Center for the embeddings of the data. If None, we just use the mean of the features of x).\n                Defaults to None.\n\n        Raises:\n            RuntimeError: Raised if the datasets are not the same sizes.\n            RuntimeError: Raised if there is an invalid score for delta_precision_alpha.\n            RuntimeError: Raised if there is an invalid score for delta_coverage_beta.\n\n        Returns:\n            alphas, alpha_precision_curve, beta_coverage_curve, delta_precision_alpha, delta_coverage_beta,\n            authenticity.\n        \"\"\"\n        if len(x) != len(x_syn):\n            raise RuntimeError(\"The real and synthetic data must have the same length\")\n\n        if emb_center is None:\n            emb_center = np.mean(x, axis=0)\n\n        n_steps = 30\n        alphas = np.linspace(0, 1, n_steps)\n\n        radii = np.quantile(np.sqrt(np.sum((x - emb_center) ** 2, axis=1)), alphas)\n\n        synth_center = np.mean(x_syn, axis=0)\n\n        alpha_precision_curve: list[float] = []\n        beta_coverage_curve: list[float] = []\n\n        synth_to_center = np.sqrt(np.sum((x_syn - emb_center) ** 2, axis=1))\n\n        nbrs_real = NearestNeighbors(n_neighbors=2, n_jobs=-1, p=2).fit(x)\n        k_neighbors_real = nbrs_real.kneighbors(x)\n        assert isinstance(k_neighbors_real, tuple)\n        real_to_real, _ = k_neighbors_real\n\n        nbrs_synth = NearestNeighbors(n_neighbors=1, n_jobs=-1, p=2).fit(x_syn)\n        k_neighbors_synth = nbrs_synth.kneighbors(x)\n        assert isinstance(k_neighbors_synth, tuple)\n        real_to_synth, real_to_synth_args = k_neighbors_synth\n\n        # Let us find closest real point to any real point, excluding itself (therefore 1 instead of 0)\n        real_to_real = real_to_real[:, 1].squeeze()\n        real_to_synth = real_to_synth.squeeze()\n        real_to_synth_args = real_to_synth_args.squeeze()\n\n        real_synth_closest = x_syn[real_to_synth_args]\n\n        real_synth_closest_d = np.sqrt(np.sum((real_synth_closest - synth_center) ** 2, axis=1))\n        closest_synth_radii = np.quantile(real_synth_closest_d, alphas)\n\n        for k in range(len(radii)):\n            precision_audit_mask = synth_to_center &lt;= radii[k]\n            alpha_precision = np.mean(precision_audit_mask)\n\n            beta_coverage = np.mean(\n                ((real_to_synth &lt;= real_to_real) * (real_synth_closest_d &lt;= closest_synth_radii[k]))\n            )\n\n            alpha_precision_curve.append(alpha_precision)\n            beta_coverage_curve.append(beta_coverage)\n\n        # See which one is bigger\n\n        authen = real_to_real[real_to_synth_args] &lt; real_to_synth\n        authenticity = np.mean(authen)\n\n        delta_precision_alpha = 1.0 - np.sum(np.abs(np.array(alphas) - np.array(alpha_precision_curve))) / np.sum(\n            alphas\n        )\n\n        if delta_precision_alpha &lt; 0:\n            raise RuntimeError(\"negative value detected for Delta_precision_alpha\")\n\n        delta_coverage_beta = 1.0 - np.sum(np.abs(np.array(alphas) - np.array(beta_coverage_curve))) / np.sum(alphas)\n\n        if delta_coverage_beta &lt; 0:\n            raise RuntimeError(\"negative value detected for Delta_coverage_beta\")\n\n        return (\n            alphas.tolist(),\n            alpha_precision_curve,\n            beta_coverage_curve,\n            delta_precision_alpha,\n            delta_coverage_beta,\n            authenticity.astype(float),\n        )\n\n    def _normalize_covariates(\n        self,\n        x: DataLoader,\n        x_syn: DataLoader,\n    ) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"\n        This is an internal method to replicate the old, naive method for evaluating AlphaPrecision.\n\n        Args:\n            x (DataLoader): The ground truth dataset.\n            x_syn (DataLoader): The synthetic dataset.\n\n        Returns:\n            Tuple[pd.DataFrame, pd.DataFrame]: normalized version of the datasets\n        \"\"\"\n        x_gt_norm = x.dataframe().copy()\n        x_syn_norm = x_syn.dataframe().copy()\n        if self._task_type != \"survival_analysis\":\n            if hasattr(x, \"target_column\"):\n                x_gt_norm = x_gt_norm.drop(columns=[x.target_column])\n            if hasattr(x_syn, \"target_column\"):\n                x_syn_norm = x_syn_norm.drop(columns=[x_syn.target_column])\n        scaler = MinMaxScaler().fit(x_gt_norm)\n        if hasattr(x, \"target_column\"):\n            x_gt_norm_df = pd.DataFrame(\n                scaler.transform(x_gt_norm),\n                columns=[col for col in x.train().dataframe().columns if col != x.target_column],\n            )\n        else:\n            x_gt_norm_df = pd.DataFrame(scaler.transform(x_gt_norm), columns=x.train().dataframe().columns)\n\n        if hasattr(x_syn, \"target_column\"):\n            x_syn_norm_df = pd.DataFrame(\n                scaler.transform(x_syn_norm),\n                columns=[col for col in x_syn.dataframe().columns if col != x_syn.target_column],\n            )\n        else:\n            x_syn_norm_df = pd.DataFrame(scaler.transform(x_syn_norm), columns=x_syn.dataframe().columns)\n\n        return x_gt_norm_df, x_syn_norm_df\n\n    def _evaluate(\n        self,\n        x: DataLoader,\n        x_syn: DataLoader,\n    ) -&gt; dict:\n        \"\"\"\n        Run the full evaluation pipeline, including both naive and non-naive metrics.\n\n        Args:\n            x (DataLoader): The ground truth dataset.\n            x_syn (DataLoader): The synthetic dataset.\n\n        Returns:\n            Dictionary of metric type and value\n        \"\"\"\n        results = {}\n\n        x_ = x.numpy().reshape(len(x), -1)\n        x_syn_ = x_syn.numpy().reshape(len(x_syn), -1)\n\n        # OneClass representation\n        emb = \"_OC\"\n        oneclass_model = self._get_oneclass_model(x_)\n        x_ = self._oneclass_predict(oneclass_model, x_)\n        x_syn_ = self._oneclass_predict(oneclass_model, x_syn_)\n        emb_center = oneclass_model.c.detach().cpu().numpy()\n\n        (\n            _,\n            _,\n            _,\n            delta_precision_alpha,\n            delta_coverage_beta,\n            authenticity,\n        ) = self.metrics(x_, x_syn_, emb_center=emb_center)\n\n        results[f\"delta_precision_alpha{emb}\"] = delta_precision_alpha\n        results[f\"delta_coverage_beta{emb}\"] = delta_coverage_beta\n        results[f\"authenticity{emb}\"] = authenticity\n\n        X_df, X_syn_df = self._normalize_covariates(x, x_syn)\n        (\n            _,\n            _,\n            _,\n            delta_precision_alpha_naive,\n            delta_coverage_beta_naive,\n            authenticity_naive,\n        ) = self.metrics(X_df.to_numpy(), X_syn_df.to_numpy(), emb_center=None)\n\n        results[\"delta_precision_alpha_naive\"] = delta_precision_alpha_naive\n        results[\"delta_coverage_beta_naive\"] = delta_coverage_beta_naive\n        results[\"authenticity_naive\"] = authenticity_naive\n\n        return results\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(**kwargs)\n</code></pre> <p>Evaluates the alpha-precision, beta-recall, and authenticity scores.</p> <p>The class evaluates the synthetic data using a tuple of three metrics:</p> <pre><code>alpha-precision, beta-recall, and authenticity.\n</code></pre> <p>Note that these metrics can be evaluated for each synthetic data point (which are useful for auditing and post-processing). Here we average the scores to reflect the overall quality of the data. The formal definitions can be found in the reference below:</p> <p>Alaa, Ahmed, Boris Van Breugel, Evgeny S. Saveliev, and Mihaela van der Schaar. \"How faithful is your synthetic data? sample-level metrics for evaluating and auditing generative models.\" In International Conference on Machine Learning, pp. 290-306. PMLR, 2022.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/statistical_eval.py</code> <pre><code>def __init__(self, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Evaluates the alpha-precision, beta-recall, and authenticity scores.\n\n    The class evaluates the synthetic data using a tuple of three metrics:\n\n        alpha-precision, beta-recall, and authenticity.\n\n    Note that these metrics can be evaluated for each synthetic data point (which are useful for auditing and\n    post-processing). Here we average the scores to reflect the overall quality of the data.\n    The formal definitions can be found in the reference below:\n\n    Alaa, Ahmed, Boris Van Breugel, Evgeny S. Saveliev, and Mihaela van der Schaar. \"How faithful is your synthetic\n    data? sample-level metrics for evaluating and auditing generative models.\"\n    In International Conference on Machine Learning, pp. 290-306. PMLR, 2022.\n    \"\"\"\n    super().__init__(default_metric=\"authenticity_OC\", **kwargs)\n</code></pre> <code></code> name <code>staticmethod</code> \u00b6 <pre><code>name()\n</code></pre> <p>Return name.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/statistical_eval.py</code> <pre><code>@staticmethod\ndef name() -&gt; str:\n    \"\"\"Return name.\"\"\"\n    return \"alpha_precision\"\n</code></pre> <code></code> direction <code>staticmethod</code> \u00b6 <pre><code>direction()\n</code></pre> <p>Return optimization direction.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/statistical_eval.py</code> <pre><code>@staticmethod\ndef direction() -&gt; str:\n    \"\"\"Return optimization direction.\"\"\"\n    return \"maximize\"\n</code></pre> <code></code> metrics \u00b6 <pre><code>metrics(x, x_syn, emb_center=None)\n</code></pre> <p>Compute the alpha-precision, beta-recall, and authenticity scores provided real data (x) and synthetic data (x_syn). If <code>emb_center</code> is provided this are \"non-naive\" metrics. If it is none, these constitute \"naive\" scores.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Real data</p> required <code>x_syn</code> <code>ndarray</code> <p>Synthetically generated data.</p> required <code>emb_center</code> <code>ndarray | None</code> <p>Center for the embeddings of the data. If None, we just use the mean of the features of x). Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Raised if the datasets are not the same sizes.</p> <code>RuntimeError</code> <p>Raised if there is an invalid score for delta_precision_alpha.</p> <code>RuntimeError</code> <p>Raised if there is an invalid score for delta_coverage_beta.</p> <p>Returns:</p> Type Description <code>list[float]</code> <p>alphas, alpha_precision_curve, beta_coverage_curve, delta_precision_alpha, delta_coverage_beta,</p> <code>list[float]</code> <p>authenticity.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/synthcity/statistical_eval.py</code> <pre><code>def metrics(\n    self,\n    x: np.ndarray,\n    x_syn: np.ndarray,\n    emb_center: np.ndarray | None = None,\n) -&gt; tuple[list[float], list[float], list[float], float, float, float]:\n    \"\"\"\n    Compute the alpha-precision, beta-recall, and authenticity scores provided real data (x) and synthetic\n    data (x_syn). If ``emb_center`` is provided this are \"non-naive\" metrics. If it is none, these constitute\n    \"naive\" scores.\n\n    Args:\n        x: Real data\n        x_syn: Synthetically generated data.\n        emb_center: Center for the embeddings of the data. If None, we just use the mean of the features of x).\n            Defaults to None.\n\n    Raises:\n        RuntimeError: Raised if the datasets are not the same sizes.\n        RuntimeError: Raised if there is an invalid score for delta_precision_alpha.\n        RuntimeError: Raised if there is an invalid score for delta_coverage_beta.\n\n    Returns:\n        alphas, alpha_precision_curve, beta_coverage_curve, delta_precision_alpha, delta_coverage_beta,\n        authenticity.\n    \"\"\"\n    if len(x) != len(x_syn):\n        raise RuntimeError(\"The real and synthetic data must have the same length\")\n\n    if emb_center is None:\n        emb_center = np.mean(x, axis=0)\n\n    n_steps = 30\n    alphas = np.linspace(0, 1, n_steps)\n\n    radii = np.quantile(np.sqrt(np.sum((x - emb_center) ** 2, axis=1)), alphas)\n\n    synth_center = np.mean(x_syn, axis=0)\n\n    alpha_precision_curve: list[float] = []\n    beta_coverage_curve: list[float] = []\n\n    synth_to_center = np.sqrt(np.sum((x_syn - emb_center) ** 2, axis=1))\n\n    nbrs_real = NearestNeighbors(n_neighbors=2, n_jobs=-1, p=2).fit(x)\n    k_neighbors_real = nbrs_real.kneighbors(x)\n    assert isinstance(k_neighbors_real, tuple)\n    real_to_real, _ = k_neighbors_real\n\n    nbrs_synth = NearestNeighbors(n_neighbors=1, n_jobs=-1, p=2).fit(x_syn)\n    k_neighbors_synth = nbrs_synth.kneighbors(x)\n    assert isinstance(k_neighbors_synth, tuple)\n    real_to_synth, real_to_synth_args = k_neighbors_synth\n\n    # Let us find closest real point to any real point, excluding itself (therefore 1 instead of 0)\n    real_to_real = real_to_real[:, 1].squeeze()\n    real_to_synth = real_to_synth.squeeze()\n    real_to_synth_args = real_to_synth_args.squeeze()\n\n    real_synth_closest = x_syn[real_to_synth_args]\n\n    real_synth_closest_d = np.sqrt(np.sum((real_synth_closest - synth_center) ** 2, axis=1))\n    closest_synth_radii = np.quantile(real_synth_closest_d, alphas)\n\n    for k in range(len(radii)):\n        precision_audit_mask = synth_to_center &lt;= radii[k]\n        alpha_precision = np.mean(precision_audit_mask)\n\n        beta_coverage = np.mean(\n            ((real_to_synth &lt;= real_to_real) * (real_synth_closest_d &lt;= closest_synth_radii[k]))\n        )\n\n        alpha_precision_curve.append(alpha_precision)\n        beta_coverage_curve.append(beta_coverage)\n\n    # See which one is bigger\n\n    authen = real_to_real[real_to_synth_args] &lt; real_to_synth\n    authenticity = np.mean(authen)\n\n    delta_precision_alpha = 1.0 - np.sum(np.abs(np.array(alphas) - np.array(alpha_precision_curve))) / np.sum(\n        alphas\n    )\n\n    if delta_precision_alpha &lt; 0:\n        raise RuntimeError(\"negative value detected for Delta_precision_alpha\")\n\n    delta_coverage_beta = 1.0 - np.sum(np.abs(np.array(alphas) - np.array(beta_coverage_curve))) / np.sum(alphas)\n\n    if delta_coverage_beta &lt; 0:\n        raise RuntimeError(\"negative value detected for Delta_coverage_beta\")\n\n    return (\n        alphas.tolist(),\n        alpha_precision_curve,\n        beta_coverage_curve,\n        delta_precision_alpha,\n        delta_coverage_beta,\n        authenticity.astype(float),\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.generation_quality.utils","title":"utils","text":""},{"location":"api/#midst_toolkit.evaluation.generation_quality.utils.create_quality_metrics_directory","title":"create_quality_metrics_directory","text":"<pre><code>create_quality_metrics_directory(save_directory)\n</code></pre> <p>Helper function for creating a directory at the specified path to whole metrics results. If the directory already exists, this function will log a warning and no-op.</p> <p>Parameters:</p> Name Type Description Default <code>save_directory</code> <code>Path</code> <p>Path of the directory to create.</p> required Source code in <code>src/midst_toolkit/evaluation/generation_quality/utils.py</code> <pre><code>def create_quality_metrics_directory(save_directory: Path) -&gt; None:\n    \"\"\"\n    Helper function for creating a directory at the specified path to whole metrics results. If the directory already\n    exists, this function will log a warning and no-op.\n\n    Args:\n        save_directory: Path of the directory to create.\n    \"\"\"\n    if not os.path.exists(save_directory):\n        os.makedirs(save_directory)\n    else:\n        log(WARNING, f\"Path: {save_directory} already exists. Make sure this is intended.\")\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.generation_quality.utils.dump_metrics_dict","title":"dump_metrics_dict","text":"<pre><code>dump_metrics_dict(metrics_dict, file_path)\n</code></pre> <p>Write the provided metrics dictionary to the provided <code>file_path</code> argument. The metrics dictionary is written in a specific format.</p> <p>Parameters:</p> Name Type Description Default <code>metrics_dict</code> <code>dict[str, float]</code> <p>Dictionary of metrics with string key values and associated floats representing metrics calculations</p> required <code>file_path</code> <code>Path</code> <p>Path to which the metrics are written. The file will be created or overwritten if it exists</p> required Source code in <code>src/midst_toolkit/evaluation/generation_quality/utils.py</code> <pre><code>def dump_metrics_dict(metrics_dict: dict[str, float], file_path: Path) -&gt; None:\n    \"\"\"\n    Write the provided metrics dictionary to the provided ``file_path`` argument. The metrics dictionary is written\n    in a specific format.\n\n    Args:\n        metrics_dict: Dictionary of metrics with string key values and associated floats representing metrics\n            calculations\n        file_path: Path to which the metrics are written. The file will be created or overwritten if it exists\n    \"\"\"\n    if os.path.exists(file_path):\n        log(WARNING, f\"File at path {file_path} already exists.\")\n    with open(file_path, \"w\") as f:\n        for metric_key, metric_value in metrics_dict.items():\n            f.write(f\"Metric Name: {metric_key}\\t Metric Value: {metric_value}\\n\")\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.generation_quality.utils.extract_columns_based_on_meta_info","title":"extract_columns_based_on_meta_info","text":"<pre><code>extract_columns_based_on_meta_info(data, meta_info)\n</code></pre> <p>Given a set of meta information, which should be in JSON format with keys 'num_col_idx', 'cat_col_idx', 'target_col_idx', and 'task_type', the provided dataframe is filtered to the correct set of columns for evaluation using the meta information.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be filtered using the meta information</p> required <code>meta_info</code> <code>dict[str, Any]</code> <p>JSON with meta information about the columns and their corresponding types that should be considered. At minimum, it should have the keys keys 'num_col_idx', 'cat_col_idx', 'target_col_idx', and 'task_type'</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Filtered dataframes. The first dataframe is the filtered set of columns associated with numerical data. The</p> <code>DataFrame</code> <p>second is the filtered set of columns associated with categorical data.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/utils.py</code> <pre><code>def extract_columns_based_on_meta_info(\n    data: pd.DataFrame, meta_info: dict[str, Any]\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Given a set of meta information, which should be in JSON format with keys 'num_col_idx', 'cat_col_idx',\n    'target_col_idx', and 'task_type', the provided dataframe is filtered to the correct set of columns for evaluation\n    using the meta information.\n\n    Args:\n        data: Dataframe to be filtered using the meta information\n        meta_info: JSON with meta information about the columns and their corresponding types that should be\n            considered. At minimum, it should have the keys keys 'num_col_idx', 'cat_col_idx', 'target_col_idx',\n            and 'task_type'\n\n    Returns:\n        Filtered dataframes. The first dataframe is the filtered set of columns associated with numerical data. The\n        second is the filtered set of columns associated with categorical data.\n    \"\"\"\n    # TODO: Consider creating a meta_info class that formalizes the structure of the meta_info produced when\n    # Training the diffusion generators.\n\n    # Enumerate columns and replace column name with index\n    data.columns = range(len(data.columns))\n\n    # Get numerical and categorical column indices from meta info\n    # NOTE: numerical and categorical columns are the only admissible/generate-able types\"\n    numerical_column_idx = meta_info[\"num_col_idx\"]\n    categorical_column_idx = meta_info[\"cat_col_idx\"]\n\n    # Target columns are also part of the generation, just need to add it to the right \"category\"\n    target_col_idx = meta_info[\"target_col_idx\"]\n    task_type = TaskType(meta_info[\"task_type\"])\n    if task_type == TaskType.REGRESSION:\n        numerical_column_idx = numerical_column_idx + target_col_idx\n    else:\n        categorical_column_idx = categorical_column_idx + target_col_idx\n\n    numerical_data = data[numerical_column_idx]\n    categorical_data = data[categorical_column_idx]\n\n    return numerical_data, categorical_data\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.generation_quality.utils.one_hot_encode_categoricals_and_merge_with_numerical","title":"one_hot_encode_categoricals_and_merge_with_numerical","text":"<pre><code>one_hot_encode_categoricals_and_merge_with_numerical(\n    real_categorical_data,\n    synthetic_categorical_data,\n    real_numerical_data,\n    synthetic_numerical_data,\n)\n</code></pre> <p>Performs one-hot encoding on the real and synthetic data contained in numpy arrays. The <code>real_categorical_data</code> is used to fit the one-hot encoder, which is then applied to the data in <code>synthetic_categorical_data</code>. The resulting, one-hot encoded, numpy arrays are then concatenated together numerical then one-hots for both the synthetic and real data.</p> <p>Parameters:</p> Name Type Description Default <code>real_categorical_data</code> <code>ndarray</code> <p>Categorical data from the real dataset.</p> required <code>synthetic_categorical_data</code> <code>ndarray</code> <p>Categorical data from the synthetically generated dataset.</p> required <code>real_numerical_data</code> <code>ndarray</code> <p>Numerical data from the real dataset.</p> required <code>synthetic_numerical_data</code> <code>ndarray</code> <p>Numerical data from the synthetically generated dataset.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Two pandas dataframes representing the numerical and categorical data concatenated together. First dataframe</p> <code>DataFrame</code> <p>is the real data, second is the synthetic data.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/utils.py</code> <pre><code>def one_hot_encode_categoricals_and_merge_with_numerical(\n    real_categorical_data: np.ndarray,\n    synthetic_categorical_data: np.ndarray,\n    real_numerical_data: np.ndarray,\n    synthetic_numerical_data: np.ndarray,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Performs one-hot encoding on the real and synthetic data contained in numpy arrays. The ``real_categorical_data``\n    is used to fit the one-hot encoder, which is then applied to the data in ``synthetic_categorical_data``. The\n    resulting, one-hot encoded, numpy arrays are then concatenated together numerical then one-hots for both the\n    synthetic and real data.\n\n    Args:\n        real_categorical_data: Categorical data from the real dataset.\n        synthetic_categorical_data: Categorical data from the synthetically generated dataset.\n        real_numerical_data: Numerical data from the real dataset.\n        synthetic_numerical_data: Numerical data from the synthetically generated dataset.\n\n    Returns:\n        Two pandas dataframes representing the numerical and categorical data concatenated together. First dataframe\n        is the real data, second is the synthetic data.\n    \"\"\"\n    encoder = OneHotEncoder()\n    one_hot_real_data = encoder.fit_transform(real_categorical_data).toarray()\n    one_hot_synthetic_data = encoder.transform(synthetic_categorical_data).toarray()\n\n    real_dataframe = pd.DataFrame(np.concatenate((real_numerical_data, one_hot_real_data), axis=1)).astype(float)\n\n    synthetic_dataframe = pd.DataFrame(\n        np.concatenate((synthetic_numerical_data, one_hot_synthetic_data), axis=1)\n    ).astype(float)\n\n    return real_dataframe, synthetic_dataframe\n</code></pre>"},{"location":"api/#midst_toolkit.models","title":"models","text":"<p>PLACEHOLDER.</p>"},{"location":"api/#midst_toolkit.models.clavaddpm","title":"clavaddpm","text":"<p>PLACEHOLDER.</p>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils","title":"diffusion_utils","text":"<p>PLACEHOLDER.</p>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.FoundNANsError","title":"FoundNANsError","text":"<p>               Bases: <code>BaseException</code></p> <p>Found NANs during sampling.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>class FoundNANsError(BaseException):\n    \"\"\"Found NANs during sampling.\"\"\"\n\n    def __init__(self, message=\"Found NANs during sampling.\"):\n        # ruff: noqa: D107\n        super(FoundNANsError, self).__init__(message)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.normal_kl","title":"normal_kl","text":"<pre><code>normal_kl(mean1, logvar1, mean2, logvar2)\n</code></pre> <p>Compute the KL divergence between two gaussians.</p> <p>Shapes are automatically broadcasted, so batches can be compared to scalars, among other use cases.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def normal_kl(\n    mean1: Tensor | float,\n    logvar1: Tensor | float,\n    mean2: Tensor | float,\n    logvar2: Tensor | float,\n) -&gt; Tensor:\n    \"\"\"\n    Compute the KL divergence between two gaussians.\n\n    Shapes are automatically broadcasted, so batches can be compared to\n    scalars, among other use cases.\n    \"\"\"\n    tensor = None\n    for obj in (mean1, logvar1, mean2, logvar2):\n        if isinstance(obj, torch.Tensor):\n            tensor = obj\n            break\n    assert tensor is not None, \"at least one argument must be a Tensor\"\n\n    # Force variances to be Tensors. Broadcasting helps convert scalars to\n    # Tensors, but it does not work for torch.exp().\n    logvar1, logvar2 = [x if isinstance(x, torch.Tensor) else torch.tensor(x).to(tensor) for x in (logvar1, logvar2)]\n\n    return 0.5 * (\n        -1.0 + logvar2 - logvar1 + torch.exp(logvar1 - logvar2) + ((mean1 - mean2) ** 2) * torch.exp(-logvar2)\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.approx_standard_normal_cdf","title":"approx_standard_normal_cdf","text":"<pre><code>approx_standard_normal_cdf(x)\n</code></pre> <p>A fast approximation of the cumulative distribution function of the standard normal.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def approx_standard_normal_cdf(x: Tensor) -&gt; Tensor:\n    \"\"\"\n    A fast approximation of the cumulative distribution function of the\n    standard normal.\n    \"\"\"\n    return 0.5 * (1.0 + torch.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * torch.pow(x, 3))))\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.discretized_gaussian_log_likelihood","title":"discretized_gaussian_log_likelihood","text":"<pre><code>discretized_gaussian_log_likelihood(\n    x, *, means, log_scales\n)\n</code></pre> <p>Compute the log-likelihood of a Gaussian distribution discretizing to a given image.</p> <p>:param x: the target images. It is assumed that this was uint8 values,           rescaled to the range [-1, 1]. :param means: the Gaussian mean Tensor. :param log_scales: the Gaussian log stddev Tensor. :return: a tensor like x of log probabilities (in nats).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def discretized_gaussian_log_likelihood(x: Tensor, *, means: Tensor, log_scales: Tensor) -&gt; Tensor:\n    \"\"\"\n    Compute the log-likelihood of a Gaussian distribution discretizing to a\n    given image.\n\n    :param x: the target images. It is assumed that this was uint8 values,\n              rescaled to the range [-1, 1].\n    :param means: the Gaussian mean Tensor.\n    :param log_scales: the Gaussian log stddev Tensor.\n    :return: a tensor like x of log probabilities (in nats).\n    \"\"\"\n    assert x.shape == means.shape == log_scales.shape\n    centered_x = x - means\n    inv_stdv = torch.exp(-log_scales)\n    plus_in = inv_stdv * (centered_x + 1.0 / 255.0)\n    cdf_plus = approx_standard_normal_cdf(plus_in)\n    min_in = inv_stdv * (centered_x - 1.0 / 255.0)\n    cdf_min = approx_standard_normal_cdf(min_in)\n    log_cdf_plus = torch.log(cdf_plus.clamp(min=1e-12))\n    log_one_minus_cdf_min = torch.log((1.0 - cdf_min).clamp(min=1e-12))\n    cdf_delta = cdf_plus - cdf_min\n    log_probs = torch.where(\n        x &lt; -0.999,\n        log_cdf_plus,\n        torch.where(x &gt; 0.999, log_one_minus_cdf_min, torch.log(cdf_delta.clamp(min=1e-12))),\n    )\n    assert log_probs.shape == x.shape\n    return log_probs\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.sum_except_batch","title":"sum_except_batch","text":"<pre><code>sum_except_batch(x, num_dims=1)\n</code></pre> <p>Sums all dimensions except the first.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Tensor, shape (batch_size, ...)</p> required <code>num_dims</code> <code>int</code> <p>int, number of batch dims (default=1)</p> <code>1</code> <p>Returns:</p> Name Type Description <code>x_sum</code> <code>Tensor</code> <p>Tensor, shape (batch_size,)</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def sum_except_batch(x: Tensor, num_dims: int = 1) -&gt; Tensor:\n    \"\"\"\n    Sums all dimensions except the first.\n\n    Args:\n        x: Tensor, shape (batch_size, ...)\n        num_dims: int, number of batch dims (default=1)\n\n    Returns:\n        x_sum: Tensor, shape (batch_size,)\n    \"\"\"\n    return x.reshape(*x.shape[:num_dims], -1).sum(-1)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.mean_flat","title":"mean_flat","text":"<pre><code>mean_flat(tensor)\n</code></pre> <p>Take the mean over all non-batch dimensions.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def mean_flat(tensor: Tensor) -&gt; Tensor:\n    \"\"\"Take the mean over all non-batch dimensions.\"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion","title":"gaussian_multinomial_diffusion","text":"<p>Based on the code below.</p> <p>https://github.com/openai/guided-diffusion/blob/main/guided_diffusion https://github.com/ehoogeboom/multinomial_diffusion</p>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.GaussianMultinomialDiffusion","title":"GaussianMultinomialDiffusion","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>class GaussianMultinomialDiffusion(torch.nn.Module):\n    def __init__(\n        # ruff: noqa: PLR0915\n        self,\n        num_classes: np.ndarray,\n        num_numerical_features: int,\n        denoise_fn: torch.nn.Module,\n        num_timesteps: int = 1000,\n        gaussian_loss_type: str = \"mse\",\n        gaussian_parametrization: str = \"eps\",\n        multinomial_loss_type: str = \"vb_stochastic\",\n        parametrization: str = \"x0\",\n        scheduler: str = \"cosine\",\n        device: torch.device | None = None,\n    ):\n        # ruff: noqa: D107\n        if device is None:\n            device = torch.device(\"cpu\")\n\n        super(GaussianMultinomialDiffusion, self).__init__()\n        assert multinomial_loss_type in (\"vb_stochastic\", \"vb_all\")\n        assert parametrization in (\"x0\", \"direct\")\n\n        if multinomial_loss_type == \"vb_all\":\n            print(\n                \"Computing the loss using the bound on _all_ timesteps.\"\n                \" This is expensive both in terms of memory and computation.\"\n            )\n\n        self.num_numerical_features = num_numerical_features\n        self.num_classes = num_classes  # it as a vector [K1, K2, ..., Km]\n        self.num_classes_expanded = torch.from_numpy(\n            np.concatenate([num_classes[i].repeat(num_classes[i]) for i in range(len(num_classes))])\n        ).to(device)\n\n        self.slices_for_classes = [np.arange(self.num_classes[0])]\n        offsets: np.ndarray = np.cumsum(self.num_classes)\n        for i in range(1, len(offsets)):\n            self.slices_for_classes.append(np.arange(offsets[i - 1], offsets[i]))\n        self.offsets = torch.from_numpy(np.append([0], offsets)).to(device)\n\n        self._denoise_fn = denoise_fn\n        self.gaussian_loss_type = gaussian_loss_type\n        self.gaussian_parametrization = gaussian_parametrization\n        self.multinomial_loss_type = multinomial_loss_type\n        self.num_timesteps = num_timesteps\n        self.parametrization = parametrization\n        self.scheduler = scheduler\n        self.device = device\n        self.alphas: Tensor\n        self.alphas_cumprod: Tensor\n        self.alphas_cumprod_next: Tensor\n        self.alphas_cumprod_prev: Tensor\n        self.sqrt_alphas_cumprod: Tensor\n        self.sqrt_one_minus_alphas_cumprod: Tensor\n        self.log_cumprod_alpha: Tensor\n        self.log_alpha: Tensor\n        self.log_1_min_alpha: Tensor\n        self.log_1_min_cumprod_alpha: Tensor\n        self.sqrt_recipm1_alphas_cumprod: Tensor\n        self.sqrt_recip_alphas_cumprod: Tensor\n        self.Lt_history: Tensor\n        self.Lt_count: Tensor\n\n        a = 1.0 - get_named_beta_schedule(scheduler, num_timesteps)\n        alphas = torch.tensor(a.astype(\"float64\"))\n        betas = 1.0 - alphas\n\n        log_alpha: Tensor = np.log(alphas)  # type: ignore[assignment]\n        log_cumprod_alpha: Tensor = np.cumsum(log_alpha)  # type: ignore[assignment]\n\n        log_1_min_alpha: Tensor = log_1_min_a(log_alpha)\n        log_1_min_cumprod_alpha: Tensor = log_1_min_a(log_cumprod_alpha)\n\n        alphas_cumprod: Tensor = np.cumprod(alphas, axis=0)  # type: ignore[assignment]\n        alphas_cumprod_prev = torch.tensor(np.append(1.0, alphas_cumprod[:-1]))\n        alphas_cumprod_next = torch.tensor(np.append(alphas_cumprod[1:], 0.0))\n        sqrt_alphas_cumprod: Tensor = np.sqrt(alphas_cumprod)  # type: ignore[assignment]\n        sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - alphas_cumprod)\n        sqrt_recip_alphas_cumprod: Tensor = np.sqrt(1.0 / alphas_cumprod)\n        sqrt_recipm1_alphas_cumprod: Tensor = np.sqrt(1.0 / alphas_cumprod - 1)\n\n        # Gaussian diffusion\n\n        self.posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n        self.posterior_log_variance_clipped = (\n            torch.from_numpy(np.log(np.append(self.posterior_variance[1], self.posterior_variance[1:])))\n            .float()\n            .to(device)\n        )\n        self.posterior_mean_coef1 = (betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)).float().to(device)\n        self.posterior_mean_coef2 = (\n            ((1.0 - alphas_cumprod_prev) * np.sqrt(alphas.numpy()) / (1.0 - alphas_cumprod)).float().to(device)\n        )\n\n        assert log_add_exp(log_alpha, log_1_min_alpha).abs().sum().item() &lt; 1.0e-5\n        assert log_add_exp(log_cumprod_alpha, log_1_min_cumprod_alpha).abs().sum().item() &lt; 1e-5\n        diff: Tensor = cast(Tensor, np.cumsum(log_alpha) - log_cumprod_alpha)\n        assert diff.abs().sum().item() &lt; 1.0e-5\n\n        # Convert to float32 and register buffers.\n        self.register_buffer(\"alphas\", alphas.float().to(device))\n        self.register_buffer(\"log_alpha\", log_alpha.float().to(device))\n        self.register_buffer(\"log_1_min_alpha\", log_1_min_alpha.float().to(device))\n        self.register_buffer(\"log_1_min_cumprod_alpha\", log_1_min_cumprod_alpha.float().to(device))\n        self.register_buffer(\"log_cumprod_alpha\", log_cumprod_alpha.float().to(device))\n        self.register_buffer(\"alphas_cumprod\", alphas_cumprod.float().to(device))\n        self.register_buffer(\"alphas_cumprod_prev\", alphas_cumprod_prev.float().to(device))\n        self.register_buffer(\"alphas_cumprod_next\", alphas_cumprod_next.float().to(device))\n        self.register_buffer(\"sqrt_alphas_cumprod\", sqrt_alphas_cumprod.float().to(device))\n        self.register_buffer(\n            \"sqrt_one_minus_alphas_cumprod\",\n            sqrt_one_minus_alphas_cumprod.float().to(device),\n        )\n        self.register_buffer(\"sqrt_recip_alphas_cumprod\", sqrt_recip_alphas_cumprod.float().to(device))\n        self.register_buffer(\n            \"sqrt_recipm1_alphas_cumprod\",\n            sqrt_recipm1_alphas_cumprod.float().to(device),\n        )\n\n        self.register_buffer(\"Lt_history\", torch.zeros(num_timesteps))\n        self.register_buffer(\"Lt_count\", torch.zeros(num_timesteps))\n\n    # Gaussian part\n    def gaussian_q_mean_variance(self, x_start: Tensor, t: Tensor) -&gt; tuple[Tensor, Tensor, Tensor]:\n        mean = extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n        variance = extract(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract(self.log_1_min_cumprod_alpha, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def gaussian_q_sample(self, x_start: Tensor, t: Tensor, noise: Tensor | None = None) -&gt; Tensor:\n        if noise is None:\n            noise = torch.randn_like(x_start)\n        assert noise.shape == x_start.shape\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n            + extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n        )\n\n    def gaussian_q_posterior_mean_variance(\n        self,\n        x_start: Tensor,\n        x_t: Tensor,\n        t: Tensor,\n    ) -&gt; tuple[Tensor, Tensor, Tensor]:\n        assert x_start.shape == x_t.shape\n        posterior_mean = (\n            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start\n            + extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n        assert (\n            posterior_mean.shape[0]\n            == posterior_variance.shape[0]\n            == posterior_log_variance_clipped.shape[0]\n            == x_start.shape[0]\n        )\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def gaussian_p_mean_variance(\n        self,\n        model_output: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        denoised_fn: Callable | None = None,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Tensor]:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        B, C = x.shape[:2]\n        assert t.shape == (B,)\n\n        model_variance = torch.cat(\n            [\n                self.posterior_variance[1].unsqueeze(0).to(x.device),\n                (1.0 - self.alphas)[1:],\n            ],\n            dim=0,\n        )\n        # model_variance = self.posterior_variance.to(x.device)\n        model_log_variance = torch.log(model_variance)\n\n        model_variance = extract(model_variance, t, x.shape)\n        model_log_variance = extract(model_log_variance, t, x.shape)\n\n        if self.gaussian_parametrization == \"eps\":\n            pred_xstart = self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n        elif self.gaussian_parametrization == \"x0\":\n            pred_xstart = model_output\n        else:\n            raise NotImplementedError\n\n        model_mean, _, _ = self.gaussian_q_posterior_mean_variance(x_start=pred_xstart, x_t=x, t=t)\n\n        assert model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape, (\n            f\"{model_mean.shape}, {model_log_variance.shape}, {pred_xstart.shape}, {x.shape}\"\n        )\n\n        return {\n            \"mean\": model_mean,\n            \"variance\": model_variance,\n            \"log_variance\": model_log_variance,\n            \"pred_xstart\": pred_xstart,\n        }\n\n    def _vb_terms_bpd(\n        self,\n        model_output: Tensor,\n        x_start: Tensor,\n        x_t: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Tensor]:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        (\n            true_mean,\n            _,\n            true_log_variance_clipped,\n        ) = self.gaussian_q_posterior_mean_variance(x_start=x_start, x_t=x_t, t=t)\n        out = self.gaussian_p_mean_variance(\n            model_output, x_t, t, clip_denoised=clip_denoised, model_kwargs=model_kwargs\n        )\n        kl = normal_kl(true_mean, true_log_variance_clipped, out[\"mean\"], out[\"log_variance\"])\n        kl = mean_flat(kl) / np.log(2.0)\n\n        decoder_nll = -discretized_gaussian_log_likelihood(\n            x_start, means=out[\"mean\"], log_scales=0.5 * out[\"log_variance\"]\n        )\n        assert decoder_nll.shape == x_start.shape\n        decoder_nll = mean_flat(decoder_nll) / np.log(2.0)\n\n        # At the first timestep return the decoder NLL,\n        # otherwise return KL(q(x_{t-1}|x_t,x_0) || p(x_{t-1}|x_t))\n        output = torch.where((t == 0), decoder_nll, kl)\n        return {\n            \"output\": output,\n            \"pred_xstart\": out[\"pred_xstart\"],\n            \"out_mean\": out[\"mean\"],\n            \"true_mean\": true_mean,\n        }\n\n    def _prior_gaussian(self, x_start: Tensor) -&gt; Tensor:\n        \"\"\"\n        Get the prior KL term for the variational lower-bound, measured in\n        bits-per-dim.\n\n        This term can't be optimized, as it only depends on the encoder.\n\n        :param x_start: the [N x C x ...] tensor of inputs.\n        :return: a batch of [N] KL values (in bits), one per batch element.\n        \"\"\"\n        batch_size = x_start.shape[0]\n        t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n        qt_mean, _, qt_log_variance = self.gaussian_q_mean_variance(x_start, t)\n        kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0)\n        return mean_flat(kl_prior) / np.log(2.0)\n\n    def _gaussian_loss(\n        self,\n        model_out: Tensor,\n        x_start: Tensor,\n        x_t: Tensor,\n        t: Tensor,\n        noise: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; Tensor:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        terms = {}\n        if self.gaussian_loss_type == \"mse\":\n            terms[\"loss\"] = mean_flat((noise - model_out) ** 2)\n        elif self.gaussian_loss_type == \"kl\":\n            terms[\"loss\"] = self._vb_terms_bpd(\n                model_output=model_out,\n                x_start=x_start,\n                x_t=x_t,\n                t=t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n            )[\"output\"]\n\n        return terms[\"loss\"]\n\n    def _predict_xstart_from_eps(self, x_t: Tensor, t: Tensor, eps: Tensor) -&gt; Tensor:\n        assert x_t.shape == eps.shape\n        return (\n            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n            - extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * eps\n        )\n\n    def _predict_eps_from_xstart(self, x_t: Tensor, t: Tensor, pred_xstart: Tensor) -&gt; Tensor:\n        return (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / extract(\n            self.sqrt_recipm1_alphas_cumprod, t, x_t.shape\n        )\n\n    def condition_mean(\n        self,\n        cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n        p_mean_var: dict[str, Tensor],\n        x: Tensor,\n        t: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; Tensor:\n        \"\"\"\n        Compute the mean for the previous step, given a function cond_fn that\n        computes the gradient of a conditional log probability with respect to\n        x. In particular, cond_fn computes grad(log(p(y|x))), and we want to\n        condition on y.\n\n        This uses the conditioning strategy from Sohl-Dickstein et al. (2015).\n        \"\"\"\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        gradient = cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n        return p_mean_var[\"mean\"].float() + p_mean_var[\"variance\"] * gradient.float()\n\n    def condition_score(\n        self,\n        cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n        p_mean_var: dict[str, Tensor],\n        x: Tensor,\n        t: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Tensor]:\n        \"\"\"\n        Compute what the p_mean_variance output would have been, should the\n        model's score function be conditioned by cond_fn.\n\n        See condition_mean() for details on cond_fn.\n\n        Unlike condition_mean(), this instead uses the conditioning strategy\n        from Song et al (2020).\n        \"\"\"\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        alpha_bar = extract(self.alphas_cumprod, t, x.shape)\n\n        eps = self._predict_eps_from_xstart(x, t, p_mean_var[\"pred_xstart\"])\n        eps = eps - (1 - alpha_bar).sqrt() * cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n\n        out = p_mean_var.copy()\n        out[\"pred_xstart\"] = self._predict_xstart_from_eps(x, t, eps)\n        out[\"mean\"], _, _ = self.gaussian_q_posterior_mean_variance(x_start=out[\"pred_xstart\"], x_t=x, t=t)\n        return out\n\n    def gaussian_p_sample(\n        self,\n        model_out: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        denoised_fn: Callable | None = None,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; dict[str, Tensor]:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        out = self.gaussian_p_mean_variance(\n            model_out,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n        )\n        noise = torch.randn_like(x)\n        nonzero_mask = (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))  # no noise when t == 0\n\n        if cond_fn is not None:\n            out[\"mean\"] = self.condition_mean(cond_fn, out, x, t, model_kwargs=model_kwargs)\n\n        sample = out[\"mean\"] + nonzero_mask * torch.exp(0.5 * out[\"log_variance\"]) * noise\n        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    # Multinomial part\n\n    def multinomial_kl(self, log_prob1: Tensor, log_prob2: Tensor) -&gt; Tensor:\n        return (log_prob1.exp() * (log_prob1 - log_prob2)).sum(dim=1)\n\n    def q_pred_one_timestep(self, log_x_t: Tensor, t: Tensor) -&gt; Tensor:\n        log_alpha_t = extract(self.log_alpha, t, log_x_t.shape)\n        log_1_min_alpha_t = extract(self.log_1_min_alpha, t, log_x_t.shape)\n\n        # alpha_t * E[xt] + (1 - alpha_t) 1 / K\n        return log_add_exp(\n            log_x_t + log_alpha_t,\n            log_1_min_alpha_t - torch.log(self.num_classes_expanded),\n        )\n\n    def q_pred(self, log_x_start: Tensor, t: Tensor) -&gt; Tensor:\n        log_cumprod_alpha_t = extract(self.log_cumprod_alpha, t, log_x_start.shape)\n        log_1_min_cumprod_alpha = extract(self.log_1_min_cumprod_alpha, t, log_x_start.shape)\n\n        return log_add_exp(\n            log_x_start + log_cumprod_alpha_t,\n            log_1_min_cumprod_alpha - torch.log(self.num_classes_expanded),\n        )\n\n    def predict_start(self, model_out: Tensor, log_x_t: Tensor, t: Tensor, out_dict: dict[str, Tensor]) -&gt; Tensor:\n        # model_out = self._denoise_fn(x_t, t.to(x_t.device), **out_dict)\n\n        assert model_out.size(0) == log_x_t.size(0)\n        assert self.num_classes is not None\n        assert model_out.size(1) == self.num_classes.sum(), f\"{model_out.size()}\"\n\n        log_pred = torch.empty_like(model_out)\n        for ix in self.slices_for_classes:\n            log_pred[:, ix] = F.log_softmax(model_out[:, ix], dim=1)\n        return log_pred\n\n    def q_posterior(self, log_x_start: Tensor, log_x_t: Tensor, t: Tensor) -&gt; Tensor:\n        # q(xt-1 | xt, x0) = q(xt | xt-1, x0) * q(xt-1 | x0) / q(xt | x0)\n        # where q(xt | xt-1, x0) = q(xt | xt-1).\n\n        # EV_log_qxt_x0 = self.q_pred(log_x_start, t)\n\n        # print('sum exp', EV_log_qxt_x0.exp().sum(1).mean())\n        # assert False\n\n        # log_qxt_x0 = (log_x_t.exp() * EV_log_qxt_x0).sum(dim=1)\n        t_minus_1 = t - 1\n        # Remove negative values, will not be used anyway for final decoder\n        t_minus_1 = torch.where(t_minus_1 &lt; 0, torch.zeros_like(t_minus_1), t_minus_1)\n        log_EV_qxtmin_x0 = self.q_pred(log_x_start, t_minus_1)\n\n        num_axes = (1,) * (len(log_x_start.size()) - 1)\n        t_broadcast = t.to(log_x_start.device).view(-1, *num_axes) * torch.ones_like(log_x_start)\n        log_EV_qxtmin_x0 = torch.where(t_broadcast == 0, log_x_start, log_EV_qxtmin_x0.to(torch.float32))\n\n        # unnormed_logprobs = log_EV_qxtmin_x0 +\n        #                     log q_pred_one_timestep(x_t, t)\n        # Note: _NOT_ x_tmin1, which is how the formula is typically used!!!\n        # Not very easy to see why this is true. But it is :)\n        unnormed_logprobs = log_EV_qxtmin_x0 + self.q_pred_one_timestep(log_x_t, t)\n\n        return unnormed_logprobs - sliced_logsumexp(unnormed_logprobs, self.offsets)\n\n    def p_pred(self, model_out: Tensor, log_x: Tensor, t: Tensor, out_dict: dict[str, Tensor]) -&gt; Tensor:\n        if self.parametrization == \"x0\":\n            log_x_recon = self.predict_start(model_out, log_x, t=t, out_dict=out_dict)\n            log_model_pred = self.q_posterior(log_x_start=log_x_recon, log_x_t=log_x, t=t)\n        elif self.parametrization == \"direct\":\n            log_model_pred = self.predict_start(model_out, log_x, t=t, out_dict=out_dict)\n        else:\n            raise ValueError\n        return log_model_pred\n\n    @torch.no_grad()\n    def p_sample(self, model_out: Tensor, log_x: Tensor, t: Tensor, out_dict: dict[str, Tensor]) -&gt; Tensor:\n        model_log_prob = self.p_pred(model_out, log_x=log_x, t=t, out_dict=out_dict)\n        return self.log_sample_categorical(model_log_prob)\n\n    # Dead code\n    # @torch.no_grad()\n    # def p_sample_loop(self, shape, out_dict):\n    #     b = shape[0]\n    #     # start with random normal image.\n    #     img = torch.randn(shape, device=device)\n\n    #     for i in reversed(range(1, self.num_timesteps)):\n    #         img = self.p_sample(img, torch.full((b,), i, device=self.device, dtype=torch.long), out_dict)\n    #     return img\n\n    # @torch.no_grad()\n    # def _sample(self, image_size, out_dict, batch_size=16):\n    #     return self.p_sample_loop((batch_size, 3, image_size, image_size), out_dict)\n\n    # Dead code\n    # @torch.no_grad()\n    # def interpolate(self, x1: Tensor, x2: Tensor, t: Tensor | None = None, lam: float = 0.5) -&gt; Tensor:\n    #     b, *_, device = *x1.shape, x1.device\n    #     t = default(t, self.num_timesteps - 1)\n\n    #     assert x1.shape == x2.shape\n\n    #     t_batched = torch.stack([torch.tensor(t, device=device)] * b)\n    #     xt1, xt2 = map(lambda x: self.q_sample(x, t=t_batched), (x1, x2))\n\n    #     img = (1 - lam) * xt1 + lam * xt2\n    #     for i in reversed(range(0, t)):\n    #         img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long))\n\n    #     return img\n\n    def log_sample_categorical(self, logits: Tensor) -&gt; Tensor:\n        full_sample = []\n        for i in range(len(self.num_classes)):\n            one_class_logits = logits[:, self.slices_for_classes[i]]\n            uniform = torch.rand_like(one_class_logits)\n            gumbel_noise = -torch.log(-torch.log(uniform + 1e-30) + 1e-30)\n            sample = (gumbel_noise + one_class_logits).argmax(dim=1)\n            full_sample.append(sample.unsqueeze(1))\n        full_sample_tensor = torch.cat(full_sample, dim=1)\n        return index_to_log_onehot(full_sample_tensor, torch.from_numpy(self.num_classes))\n\n    def q_sample(self, log_x_start: Tensor, t: Tensor) -&gt; Tensor:\n        log_EV_qxt_x0 = self.q_pred(log_x_start, t)\n        # ruff: noqa: N806\n        return self.log_sample_categorical(log_EV_qxt_x0)\n\n    # Dead code\n    # def nll(self, log_x_start, out_dict):\n    #     b = log_x_start.size(0)\n    #     device = log_x_start.device\n    #     loss = 0\n    #     for t in range(0, self.num_timesteps):\n    #         t_array = (torch.ones(b, device=device) * t).long()\n\n    #         kl = self.compute_Lt(\n    #             log_x_start=log_x_start,\n    #             log_x_t=self.q_sample(log_x_start=log_x_start, t=t_array),\n    #             t=t_array,\n    #             out_dict=out_dict,\n    #         )\n\n    #         loss += kl\n\n    #     loss += self.kl_prior(log_x_start)\n\n    #     return loss\n\n    def kl_prior(self, log_x_start: Tensor) -&gt; Tensor:\n        b = log_x_start.size(0)\n        device = log_x_start.device\n        ones = torch.ones(b, device=device).long()\n\n        log_qxT_prob = self.q_pred(log_x_start, t=(self.num_timesteps - 1) * ones)\n        # ruff: noqa: N806\n        log_half_prob = -torch.log(self.num_classes_expanded * torch.ones_like(log_qxT_prob))\n\n        kl_prior = self.multinomial_kl(log_qxT_prob, log_half_prob)\n        return sum_except_batch(kl_prior)\n\n    def compute_Lt(\n        # ruff: noqa: N802\n        self,\n        model_out: Tensor,\n        log_x_start: Tensor,\n        log_x_t: Tensor,\n        t: Tensor,\n        out_dict: dict[str, Tensor],\n        detach_mean: bool = False,\n    ) -&gt; Tensor:\n        log_true_prob = self.q_posterior(log_x_start=log_x_start, log_x_t=log_x_t, t=t)\n        log_model_prob = self.p_pred(model_out, log_x=log_x_t, t=t, out_dict=out_dict)\n\n        if detach_mean:\n            log_model_prob = log_model_prob.detach()\n\n        kl = self.multinomial_kl(log_true_prob, log_model_prob)\n        kl = sum_except_batch(kl)\n\n        decoder_nll = -log_categorical(log_x_start, log_model_prob)\n        decoder_nll = sum_except_batch(decoder_nll)\n\n        mask = (t == torch.zeros_like(t)).float()\n        return mask * decoder_nll + (1.0 - mask) * kl\n\n    def sample_time(self, b: int, device: torch.device, method: str = \"uniform\") -&gt; tuple[Tensor, Tensor]:\n        if method == \"importance\":\n            if not (self.Lt_count &gt; 10).all():\n                return self.sample_time(b, device, method=\"uniform\")\n\n            Lt_sqrt = torch.sqrt(self.Lt_history + 1e-10) + 0.0001\n            # ruff: noqa: N806\n            Lt_sqrt[0] = Lt_sqrt[1]  # Overwrite decoder term with L1.\n            pt_all = (Lt_sqrt / Lt_sqrt.sum()).to(device)\n\n            t = torch.multinomial(pt_all, num_samples=b, replacement=True).to(device)\n\n            pt = pt_all.gather(dim=0, index=t)\n\n            return t, pt\n\n        if method == \"uniform\":\n            t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n\n            pt = torch.ones_like(t).float() / self.num_timesteps\n            return t, pt\n        raise ValueError\n\n    def _multinomial_loss(\n        self,\n        model_out: Tensor,\n        log_x_start: Tensor,\n        log_x_t: Tensor,\n        t: Tensor,\n        pt: Tensor,\n        out_dict: dict[str, Tensor],\n    ) -&gt; Tensor:\n        if self.multinomial_loss_type == \"vb_stochastic\":\n            kl = self.compute_Lt(model_out, log_x_start, log_x_t, t, out_dict)\n            kl_prior = self.kl_prior(log_x_start)\n            # Upweigh loss term of the kl\n            return kl / pt + kl_prior\n\n        if self.multinomial_loss_type == \"vb_all\":\n            # Expensive, dont do it ;).\n            # DEPRECATED\n            # return -self.nll(log_x_start)\n            raise ValueError(\"multinomial_loss_type == 'vb_all' is deprecated.\")\n        raise ValueError\n\n    # Dead code\n    # def log_prob(self, x, out_dict):\n    #     b, device = x.size(0), x.device\n    #     if self.training:\n    #         return self._multinomial_loss(x, out_dict)\n\n    #     log_x_start = index_to_log_onehot(x, self.num_classes)\n\n    #     t, pt = self.sample_time(b, device, \"importance\")\n\n    #     kl = self.compute_Lt(log_x_start, self.q_sample(log_x_start=log_x_start, t=t), t, out_dict)\n\n    #     kl_prior = self.kl_prior(log_x_start)\n\n    #     # Upweigh loss term of the kl\n    #     loss = kl / pt + kl_prior\n\n    #     return -loss\n\n    def mixed_loss(self, x: Tensor, out_dict: dict[str, Tensor]) -&gt; tuple[Tensor, Tensor]:\n        b = x.shape[0]\n        device = x.device\n        t, pt = self.sample_time(b, device, \"uniform\")\n\n        x_num = x[:, : self.num_numerical_features]\n        x_cat = x[:, self.num_numerical_features :]\n\n        x_num_t = x_num\n        log_x_cat_t = x_cat\n        if x_num.shape[1] &gt; 0:\n            noise = torch.randn_like(x_num)\n            x_num_t = self.gaussian_q_sample(x_num, t, noise=noise)\n        if x_cat.shape[1] &gt; 0:\n            log_x_cat = index_to_log_onehot(x_cat.long(), torch.from_numpy(self.num_classes))\n            log_x_cat_t = self.q_sample(log_x_start=log_x_cat, t=t)\n\n        x_in = torch.cat([x_num_t, log_x_cat_t], dim=1)\n\n        model_out = self._denoise_fn(x_in, t, **out_dict)\n\n        model_out_num = model_out[:, : self.num_numerical_features]\n        model_out_cat = model_out[:, self.num_numerical_features :]\n\n        loss_multi = torch.zeros((1,)).float()\n        loss_gauss = torch.zeros((1,)).float()\n        if x_cat.shape[1] &gt; 0:\n            loss_multi = self._multinomial_loss(model_out_cat, log_x_cat, log_x_cat_t, t, pt, out_dict) / len(\n                self.num_classes\n            )\n\n        if x_num.shape[1] &gt; 0:\n            loss_gauss = self._gaussian_loss(model_out_num, x_num, x_num_t, t, noise)\n\n        # loss_multi = torch.where(out_dict['y'] == 1, loss_multi, 2 * loss_multi)\n        # loss_gauss = torch.where(out_dict['y'] == 1, loss_gauss, 2 * loss_gauss)\n\n        return loss_multi.mean(), loss_gauss.mean()\n\n    @torch.no_grad()\n    def mixed_elbo(self, x0, out_dict):\n        b = x0.size(0)\n        device = x0.device\n\n        x_num = x0[:, : self.num_numerical_features]\n        x_cat = x0[:, self.num_numerical_features :]\n        has_cat = x_cat.shape[1] &gt; 0\n        if has_cat:\n            log_x_cat = index_to_log_onehot(x_cat.long(), torch.from_numpy(self.num_classes)).to(device)\n\n        gaussian_loss = []\n        xstart_mse = []\n        mse = []\n        # mu_mse = []\n        out_mean = []\n        true_mean = []\n        multinomial_loss = []\n        for t in range(self.num_timesteps):\n            t_array = (torch.ones(b, device=device) * t).long()\n            noise = torch.randn_like(x_num)\n\n            x_num_t = self.gaussian_q_sample(x_start=x_num, t=t_array, noise=noise)\n            log_x_cat_t = self.q_sample(log_x_start=log_x_cat, t=t_array) if has_cat else x_cat\n\n            model_out = self._denoise_fn(torch.cat([x_num_t, log_x_cat_t], dim=1), t_array, **out_dict)\n\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n\n            kl = torch.tensor([0.0])\n            if has_cat:\n                kl = self.compute_Lt(\n                    model_out=model_out_cat,\n                    log_x_start=log_x_cat,\n                    log_x_t=log_x_cat_t,\n                    t=t_array,\n                    out_dict=out_dict,\n                )\n\n            out = self._vb_terms_bpd(\n                model_out_num,\n                x_start=x_num,\n                x_t=x_num_t,\n                t=t_array,\n                clip_denoised=False,\n            )\n\n            multinomial_loss.append(kl)\n            gaussian_loss.append(out[\"output\"])\n            xstart_mse.append(mean_flat((out[\"pred_xstart\"] - x_num) ** 2))\n            # mu_mse.append(mean_flat(out[\"mean_mse\"]))\n            out_mean.append(mean_flat(out[\"out_mean\"]))\n            true_mean.append(mean_flat(out[\"true_mean\"]))\n\n            eps = self._predict_eps_from_xstart(x_num_t, t_array, out[\"pred_xstart\"])\n            mse.append(mean_flat((eps - noise) ** 2))\n\n        gaussian_loss_tensor = torch.stack(gaussian_loss, dim=1)\n        multinomial_loss_tensor = torch.stack(multinomial_loss, dim=1)\n        xstart_mse_tensor = torch.stack(xstart_mse, dim=1)\n        mse_tensor = torch.stack(mse, dim=1)\n        # mu_mse = torch.stack(mu_mse, dim=1)\n        out_mean_tensor = torch.stack(out_mean, dim=1)\n        true_mean_tensor = torch.stack(true_mean, dim=1)\n\n        prior_gauss = self._prior_gaussian(x_num)\n\n        prior_multin = torch.tensor([0.0])\n        if has_cat:\n            prior_multin = self.kl_prior(log_x_cat)\n\n        total_gauss = gaussian_loss_tensor.sum(dim=1) + prior_gauss\n        total_multin = multinomial_loss_tensor.sum(dim=1) + prior_multin\n        return {\n            \"total_gaussian\": total_gauss,\n            \"total_multinomial\": total_multin,\n            \"losses_gaussian\": gaussian_loss_tensor,\n            \"losses_multinimial\": multinomial_loss_tensor,\n            \"xstart_mse\": xstart_mse_tensor,\n            \"mse\": mse_tensor,\n            # \"mu_mse\": mu_mse\n            \"out_mean\": out_mean_tensor,\n            \"true_mean\": true_mean_tensor,\n        }\n\n    @torch.no_grad()\n    def gaussian_ddim_step(\n        self,\n        model_out_num: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        denoised_fn: Callable | None = None,\n        eta: float = 0.0,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; Tensor:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        out = self.gaussian_p_mean_variance(\n            model_out_num,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=None,\n        )\n\n        if cond_fn is not None:\n            out = self.condition_score(cond_fn, out, x, t, model_kwargs=model_kwargs)\n\n        eps = self._predict_eps_from_xstart(x, t, out[\"pred_xstart\"])\n\n        alpha_bar = extract(self.alphas_cumprod, t, x.shape)\n        alpha_bar_prev = extract(self.alphas_cumprod_prev, t, x.shape)\n        sigma = eta * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar)) * torch.sqrt(1 - alpha_bar / alpha_bar_prev)\n\n        noise = torch.randn_like(x)\n        mean_pred = out[\"pred_xstart\"] * torch.sqrt(alpha_bar_prev) + torch.sqrt(1 - alpha_bar_prev - sigma**2) * eps\n        nonzero_mask = (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))  # no noise when t == 0\n        return mean_pred + nonzero_mask * sigma * noise\n\n    @torch.no_grad()\n    def gaussian_ddim_sample(self, noise, T, out_dict, eta=0.0, model_kwargs=None, cond_fn=None):\n        # ruff: noqa: D102, N803\n        x = noise\n        b = x.shape[0]\n        device = x.device\n        for t in reversed(range(T)):\n            print(f\"Sample timestep {t:4d}\", end=\"\\r\")\n            t_array = (torch.ones(b, device=device) * t).long()\n            out_num = self._denoise_fn(x, t_array, **out_dict)\n            x = self.gaussian_ddim_step(out_num, x, t_array, model_kwargs=model_kwargs, cond_fn=cond_fn)\n        print()\n        return x\n\n    @torch.no_grad()\n    def gaussian_ddim_reverse_step(\n        self,\n        model_out_num: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        eta: float = 0.0,\n    ) -&gt; Tensor:\n        # ruff: noqa: D102\n        assert eta == 0.0, \"Eta must be zero.\"\n        out = self.gaussian_p_mean_variance(\n            model_out_num,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=None,\n            model_kwargs=None,\n        )\n\n        eps = (extract(self.sqrt_recip_alphas_cumprod, t, x.shape) * x - out[\"pred_xstart\"]) / extract(\n            self.sqrt_recipm1_alphas_cumprod, t, x.shape\n        )\n        alpha_bar_next = extract(self.alphas_cumprod_next, t, x.shape)\n\n        return out[\"pred_xstart\"] * torch.sqrt(alpha_bar_next) + torch.sqrt(1 - alpha_bar_next) * eps\n\n    @torch.no_grad()\n    def gaussian_ddim_reverse_sample(\n        self,\n        x,\n        T,\n        # ruff: noqa: N803\n        out_dict,\n    ):\n        # ruff: noqa: D102\n        b = x.shape[0]\n        device = x.device\n        for t in range(T):\n            print(f\"Reverse timestep {t:4d}\", end=\"\\r\")\n            t_array = (torch.ones(b, device=device) * t).long()\n            out_num = self._denoise_fn(x, t_array, **out_dict)\n            x = self.gaussian_ddim_reverse_step(out_num, x, t_array, eta=0.0)\n        print()\n\n        return x\n\n    @torch.no_grad()\n    def multinomial_ddim_step(\n        self,\n        model_out_cat: Tensor,\n        log_x_t: Tensor,\n        t: Tensor,\n        out_dict: dict[str, Tensor],\n        eta: float = 0.0,\n    ) -&gt; Tensor:\n        # ruff: noqa: D102\n        # not ddim, essentially\n        log_x0 = self.predict_start(model_out_cat, log_x_t=log_x_t, t=t, out_dict=out_dict)\n\n        alpha_bar = extract(self.alphas_cumprod, t, log_x_t.shape)\n        alpha_bar_prev = extract(self.alphas_cumprod_prev, t, log_x_t.shape)\n        sigma = eta * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar)) * torch.sqrt(1 - alpha_bar / alpha_bar_prev)\n\n        coef1 = sigma\n        coef2 = alpha_bar_prev - sigma * alpha_bar\n        coef3 = 1 - coef1 - coef2\n\n        log_ps = torch.stack(\n            [\n                torch.log(coef1) + log_x_t,\n                torch.log(coef2) + log_x0,\n                torch.log(coef3) - torch.log(self.num_classes_expanded),\n            ],\n            dim=2,\n        )\n\n        log_prob = torch.logsumexp(log_ps, dim=2)\n\n        return self.log_sample_categorical(log_prob)\n\n    @torch.no_grad()\n    def sample_ddim(\n        self,\n        num_samples: int,\n        y_dist: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; tuple[Tensor, dict[str, Tensor]]:\n        # ruff: noqa: D102\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        b = num_samples\n        z_norm = torch.randn((b, self.num_numerical_features), device=self.device)\n\n        assert self.num_classes is not None\n        has_cat = self.num_classes[0] != 0\n        log_z = torch.zeros((b, 0), device=self.device).float()\n        if has_cat:\n            uniform_logits = torch.zeros((b, len(self.num_classes_expanded)), device=self.device)\n            log_z = self.log_sample_categorical(uniform_logits)\n\n        y = torch.multinomial(y_dist, num_samples=b, replacement=True)\n        out_dict = {\"y\": y.long().to(self.device)}\n        for i in reversed(range(0, self.num_timesteps)):\n            print(f\"Sample timestep {i:4d}\", end=\"\\r\")\n            t = torch.full((b,), i, device=self.device, dtype=torch.long)\n            model_out = self._denoise_fn(torch.cat([z_norm, log_z], dim=1).float(), t, **out_dict)\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n            z_norm = self.gaussian_ddim_step(\n                model_out_num,\n                z_norm,\n                t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n                cond_fn=cond_fn,\n            )\n            if has_cat:\n                log_z = self.multinomial_ddim_step(model_out_cat, log_z, t, out_dict)\n\n        print()\n        z_ohe = torch.exp(log_z).round()\n        z_cat = log_z\n        if has_cat:\n            z_cat = ohe_to_categories(z_ohe, torch.from_numpy(self.num_classes))\n        sample = torch.cat([z_norm, z_cat], dim=1).cpu()\n        return sample, out_dict\n\n    @torch.no_grad()\n    def conditional_sample(\n        self,\n        ys: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; tuple[Tensor, dict[str, Tensor]]:\n        # ruff: noqa: D102\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        b = len(ys)\n        z_norm = torch.randn((b, self.num_numerical_features), device=self.device)\n        assert self.num_classes is not None\n        has_cat = self.num_classes[0] != 0\n        log_z = torch.zeros((b, 0), device=self.device).float()\n\n        out_dict = {\"y\": ys.long().to(self.device)}\n        for i in reversed(range(0, self.num_timesteps)):\n            print(f\"Sample timestep {i:4d}\", end=\"\\r\")\n            t = torch.full((b,), i, device=self.device, dtype=torch.long)\n            model_out = self._denoise_fn(torch.cat([z_norm, log_z], dim=1).float(), t, **out_dict)\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n            z_norm = self.gaussian_p_sample(\n                model_out_num,\n                z_norm,\n                t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n                cond_fn=cond_fn,\n            )[\"sample\"]\n            if has_cat:\n                log_z = self.p_sample(model_out_cat, log_z, t, out_dict)\n\n        print()\n        z_ohe = torch.exp(log_z).round()\n        z_cat = log_z\n        if has_cat:\n            z_cat = ohe_to_categories(z_ohe, torch.from_numpy(self.num_classes))\n        sample = torch.cat([z_norm, z_cat], dim=1).cpu()\n        return sample, out_dict\n\n    @torch.no_grad()\n    def sample(\n        self,\n        num_samples: int,\n        y_dist: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; tuple[Tensor, dict[str, Tensor]]:\n        # ruff: noqa: D102\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        b = num_samples\n        z_norm = torch.randn((b, self.num_numerical_features), device=self.device)\n\n        assert self.num_classes is not None\n        has_cat = self.num_classes[0] != 0\n        log_z = torch.zeros((b, 0), device=self.device).float()\n        if has_cat:\n            uniform_logits = torch.zeros((b, len(self.num_classes_expanded)), device=self.device)\n            log_z = self.log_sample_categorical(uniform_logits)\n\n        y = torch.multinomial(y_dist, num_samples=b, replacement=True)\n        out_dict = {\"y\": y.long().to(self.device)}\n        for i in reversed(range(0, self.num_timesteps)):\n            print(f\"Sample timestep {i:4d}\", end=\"\\r\")\n            t = torch.full((b,), i, device=self.device, dtype=torch.long)\n            model_out = self._denoise_fn(torch.cat([z_norm, log_z], dim=1).float(), t, **out_dict)\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n            z_norm = self.gaussian_p_sample(\n                model_out_num,\n                z_norm,\n                t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n                cond_fn=cond_fn,\n            )[\"sample\"]\n            if has_cat:\n                log_z = self.p_sample(model_out_cat, log_z, t, out_dict)\n\n        print()\n        z_ohe = torch.exp(log_z).round()\n        z_cat = log_z\n        if has_cat:\n            z_cat = ohe_to_categories(z_ohe, torch.from_numpy(self.num_classes))\n        sample = torch.cat([z_norm, z_cat], dim=1).cpu()\n        return sample, out_dict\n\n    def sample_all(\n        self,\n        num_samples,\n        batch_size,\n        y_dist,\n        ddim=False,\n        model_kwargs=None,\n        cond_fn=None,\n    ):\n        # ruff: noqa: D102\n        if ddim:\n            print(\"Sample using DDIM.\")\n            sample_fn = self.sample_ddim\n        else:\n            sample_fn = self.sample\n\n        b = batch_size\n\n        all_y = []\n        all_samples = []\n        num_generated = 0\n        while num_generated &lt; num_samples:\n            sample, out_dict = sample_fn(b, y_dist, model_kwargs=model_kwargs, cond_fn=cond_fn)\n            mask_nan = torch.any(sample.isnan(), dim=1)\n            sample = sample[~mask_nan]\n            out_dict[\"y\"] = out_dict[\"y\"][~mask_nan]\n\n            all_samples.append(sample)\n            all_y.append(out_dict[\"y\"].cpu())\n            if sample.shape[0] != b:\n                raise FoundNANsError\n            num_generated += sample.shape[0]\n\n        x_gen = torch.cat(all_samples, dim=0)[:num_samples]\n        y_gen = torch.cat(all_y, dim=0)[:num_samples]\n\n        return x_gen, y_gen\n</code></pre> <code></code> condition_mean \u00b6 <pre><code>condition_mean(\n    cond_fn, p_mean_var, x, t, model_kwargs=None\n)\n</code></pre> <p>Compute the mean for the previous step, given a function cond_fn that computes the gradient of a conditional log probability with respect to x. In particular, cond_fn computes grad(log(p(y|x))), and we want to condition on y.</p> <p>This uses the conditioning strategy from Sohl-Dickstein et al. (2015).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def condition_mean(\n    self,\n    cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n    p_mean_var: dict[str, Tensor],\n    x: Tensor,\n    t: Tensor,\n    model_kwargs: dict[str, Any] | None = None,\n) -&gt; Tensor:\n    \"\"\"\n    Compute the mean for the previous step, given a function cond_fn that\n    computes the gradient of a conditional log probability with respect to\n    x. In particular, cond_fn computes grad(log(p(y|x))), and we want to\n    condition on y.\n\n    This uses the conditioning strategy from Sohl-Dickstein et al. (2015).\n    \"\"\"\n    if model_kwargs is None:\n        model_kwargs = {}\n\n    gradient = cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n    return p_mean_var[\"mean\"].float() + p_mean_var[\"variance\"] * gradient.float()\n</code></pre> <code></code> condition_score \u00b6 <pre><code>condition_score(\n    cond_fn, p_mean_var, x, t, model_kwargs=None\n)\n</code></pre> <p>Compute what the p_mean_variance output would have been, should the model's score function be conditioned by cond_fn.</p> <p>See condition_mean() for details on cond_fn.</p> <p>Unlike condition_mean(), this instead uses the conditioning strategy from Song et al (2020).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def condition_score(\n    self,\n    cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n    p_mean_var: dict[str, Tensor],\n    x: Tensor,\n    t: Tensor,\n    model_kwargs: dict[str, Any] | None = None,\n) -&gt; dict[str, Tensor]:\n    \"\"\"\n    Compute what the p_mean_variance output would have been, should the\n    model's score function be conditioned by cond_fn.\n\n    See condition_mean() for details on cond_fn.\n\n    Unlike condition_mean(), this instead uses the conditioning strategy\n    from Song et al (2020).\n    \"\"\"\n    if model_kwargs is None:\n        model_kwargs = {}\n\n    alpha_bar = extract(self.alphas_cumprod, t, x.shape)\n\n    eps = self._predict_eps_from_xstart(x, t, p_mean_var[\"pred_xstart\"])\n    eps = eps - (1 - alpha_bar).sqrt() * cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n\n    out = p_mean_var.copy()\n    out[\"pred_xstart\"] = self._predict_xstart_from_eps(x, t, eps)\n    out[\"mean\"], _, _ = self.gaussian_q_posterior_mean_variance(x_start=out[\"pred_xstart\"], x_t=x, t=t)\n    return out\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.get_named_beta_schedule","title":"get_named_beta_schedule","text":"<pre><code>get_named_beta_schedule(\n    schedule_name, num_diffusion_timesteps\n)\n</code></pre> <p>Get a pre-defined beta schedule for the given name. The beta schedule library consists of beta schedules which remain similar in the limit of num_diffusion_timesteps. Beta schedules may be added, but should not be removed or changed once they are committed to maintain backwards compatibility.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def get_named_beta_schedule(schedule_name: str, num_diffusion_timesteps: int) -&gt; np.ndarray:\n    \"\"\"\n    Get a pre-defined beta schedule for the given name.\n    The beta schedule library consists of beta schedules which remain similar\n    in the limit of num_diffusion_timesteps.\n    Beta schedules may be added, but should not be removed or changed once\n    they are committed to maintain backwards compatibility.\n    \"\"\"\n    if schedule_name == \"linear\":\n        # Linear schedule from Ho et al, extended to work for any number of\n        # diffusion steps.\n        scale = 1000 / num_diffusion_timesteps\n        beta_start = scale * 0.0001\n        beta_end = scale * 0.02\n        return np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64)\n    if schedule_name == \"cosine\":\n        return betas_for_alpha_bar(\n            num_diffusion_timesteps,\n            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n        )\n    raise NotImplementedError(f\"unknown beta schedule: {schedule_name}\")\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.betas_for_alpha_bar","title":"betas_for_alpha_bar","text":"<pre><code>betas_for_alpha_bar(\n    num_diffusion_timesteps, alpha_bar, max_beta=0.999\n)\n</code></pre> <p>Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of (1-beta) over time from t = [0,1]. :param num_diffusion_timesteps: the number of betas to produce. :param alpha_bar: a lambda that takes an argument t from 0 to 1 and                   produces the cumulative product of (1-beta) up to that                   part of the diffusion process. :param max_beta: the maximum beta to use; use values lower than 1 to                  prevent singularities.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def betas_for_alpha_bar(num_diffusion_timesteps: int, alpha_bar: Callable, max_beta: float = 0.999) -&gt; np.ndarray:\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function,\n    which defines the cumulative product of (1-beta) over time from t = [0,1].\n    :param num_diffusion_timesteps: the number of betas to produce.\n    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n                      produces the cumulative product of (1-beta) up to that\n                      part of the diffusion process.\n    :param max_beta: the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n    \"\"\"\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return np.array(betas)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model","title":"model","text":""},{"location":"api/#midst_toolkit.models.clavaddpm.model.ScheduleSampler","title":"ScheduleSampler","text":"<p>               Bases: <code>ABC</code></p> <p>A distribution over timesteps in the diffusion process, intended to reduce variance of the objective.</p> <p>By default, samplers perform unbiased importance sampling, in which the objective's mean is unchanged. However, subclasses may override sample() to change how the resampled terms are reweighted, allowing for actual changes in the objective.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class ScheduleSampler(ABC):\n    \"\"\"\n    A distribution over timesteps in the diffusion process, intended to reduce\n    variance of the objective.\n\n    By default, samplers perform unbiased importance sampling, in which the\n    objective's mean is unchanged.\n    However, subclasses may override sample() to change how the resampled\n    terms are reweighted, allowing for actual changes in the objective.\n    \"\"\"\n\n    @abstractmethod\n    def weights(self) -&gt; Tensor:\n        \"\"\"\n        Get a numpy array of weights, one per diffusion step.\n\n        The weights needn't be normalized, but must be positive.\n        \"\"\"\n\n    def sample(self, batch_size: int, device: str) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"\n        Importance-sample timesteps for a batch.\n\n        :param batch_size: the number of timesteps.\n        :param device: the torch device to save to.\n        :return: a tuple (timesteps, weights):\n                 - timesteps: a tensor of timestep indices.\n                 - weights: a tensor of weights to scale the resulting losses.\n        \"\"\"\n        w = self.weights().cpu().numpy()\n        p = w / np.sum(w)\n        indices_np = np.random.choice(len(p), size=(batch_size,), p=p)\n        indices = torch.from_numpy(indices_np).long().to(device)\n        weights_np = 1 / (len(p) * p[indices_np])\n        weights = torch.from_numpy(weights_np).float().to(device)\n        return indices, weights\n</code></pre> <code></code> weights <code>abstractmethod</code> \u00b6 <pre><code>weights()\n</code></pre> <p>Get a numpy array of weights, one per diffusion step.</p> <p>The weights needn't be normalized, but must be positive.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>@abstractmethod\ndef weights(self) -&gt; Tensor:\n    \"\"\"\n    Get a numpy array of weights, one per diffusion step.\n\n    The weights needn't be normalized, but must be positive.\n    \"\"\"\n</code></pre> <code></code> sample \u00b6 <pre><code>sample(batch_size, device)\n</code></pre> <p>Importance-sample timesteps for a batch.</p> <p>:param batch_size: the number of timesteps. :param device: the torch device to save to. :return: a tuple (timesteps, weights):          - timesteps: a tensor of timestep indices.          - weights: a tensor of weights to scale the resulting losses.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def sample(self, batch_size: int, device: str) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"\n    Importance-sample timesteps for a batch.\n\n    :param batch_size: the number of timesteps.\n    :param device: the torch device to save to.\n    :return: a tuple (timesteps, weights):\n             - timesteps: a tensor of timestep indices.\n             - weights: a tensor of weights to scale the resulting losses.\n    \"\"\"\n    w = self.weights().cpu().numpy()\n    p = w / np.sum(w)\n    indices_np = np.random.choice(len(p), size=(batch_size,), p=p)\n    indices = torch.from_numpy(indices_np).long().to(device)\n    weights_np = 1 / (len(p) * p[indices_np])\n    weights = torch.from_numpy(weights_np).float().to(device)\n    return indices, weights\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.LossAwareSampler","title":"LossAwareSampler","text":"<p>               Bases: <code>ScheduleSampler</code></p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class LossAwareSampler(ScheduleSampler):\n    def update_with_local_losses(self, local_ts: Tensor, local_losses: Tensor) -&gt; None:\n        \"\"\"\n        Update the reweighting using losses from a model.\n\n        Call this method from each rank with a batch of timesteps and the\n        corresponding losses for each of those timesteps.\n        This method will perform synchronization to make sure all of the ranks\n        maintain the exact same reweighting.\n\n        :param local_ts: an integer Tensor of timesteps.\n        :param local_losses: a 1D Tensor of losses.\n        \"\"\"\n        batch_sizes = [\n            torch.tensor([0], dtype=torch.int32, device=local_ts.device)\n            for _ in range(torch.distributed.get_world_size())\n        ]\n        torch.distributed.all_gather(\n            batch_sizes,\n            torch.tensor([len(local_ts)], dtype=torch.int32, device=local_ts.device),\n        )\n\n        # Pad all_gather batches to be the maximum batch size.\n        max_bs = max([int(x.item()) for x in batch_sizes])\n\n        timestep_batches = [torch.zeros(max_bs).to(local_ts) for bs in batch_sizes]\n        loss_batches = [torch.zeros(max_bs).to(local_losses) for bs in batch_sizes]\n        torch.distributed.all_gather(timestep_batches, local_ts)\n        torch.distributed.all_gather(loss_batches, local_losses)\n        timesteps = [x.item() for y, bs in zip(timestep_batches, batch_sizes) for x in y[:bs]]\n        losses = [x.item() for y, bs in zip(loss_batches, batch_sizes) for x in y[:bs]]\n        self.update_with_all_losses(timesteps, losses)\n\n    @abstractmethod\n    def update_with_all_losses(self, ts: list[int], losses: list[float]) -&gt; None:\n        \"\"\"\n        Update the reweighting using losses from a model.\n\n        Sub-classes should override this method to update the reweighting\n        using losses from the model.\n\n        This method directly updates the reweighting without synchronizing\n        between workers. It is called by update_with_local_losses from all\n        ranks with identical arguments. Thus, it should have deterministic\n        behavior to maintain state across workers.\n\n        :param ts: a list of int timesteps.\n        :param losses: a list of float losses, one per timestep.\n        \"\"\"\n</code></pre> <code></code> update_with_local_losses \u00b6 <pre><code>update_with_local_losses(local_ts, local_losses)\n</code></pre> <p>Update the reweighting using losses from a model.</p> <p>Call this method from each rank with a batch of timesteps and the corresponding losses for each of those timesteps. This method will perform synchronization to make sure all of the ranks maintain the exact same reweighting.</p> <p>:param local_ts: an integer Tensor of timesteps. :param local_losses: a 1D Tensor of losses.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def update_with_local_losses(self, local_ts: Tensor, local_losses: Tensor) -&gt; None:\n    \"\"\"\n    Update the reweighting using losses from a model.\n\n    Call this method from each rank with a batch of timesteps and the\n    corresponding losses for each of those timesteps.\n    This method will perform synchronization to make sure all of the ranks\n    maintain the exact same reweighting.\n\n    :param local_ts: an integer Tensor of timesteps.\n    :param local_losses: a 1D Tensor of losses.\n    \"\"\"\n    batch_sizes = [\n        torch.tensor([0], dtype=torch.int32, device=local_ts.device)\n        for _ in range(torch.distributed.get_world_size())\n    ]\n    torch.distributed.all_gather(\n        batch_sizes,\n        torch.tensor([len(local_ts)], dtype=torch.int32, device=local_ts.device),\n    )\n\n    # Pad all_gather batches to be the maximum batch size.\n    max_bs = max([int(x.item()) for x in batch_sizes])\n\n    timestep_batches = [torch.zeros(max_bs).to(local_ts) for bs in batch_sizes]\n    loss_batches = [torch.zeros(max_bs).to(local_losses) for bs in batch_sizes]\n    torch.distributed.all_gather(timestep_batches, local_ts)\n    torch.distributed.all_gather(loss_batches, local_losses)\n    timesteps = [x.item() for y, bs in zip(timestep_batches, batch_sizes) for x in y[:bs]]\n    losses = [x.item() for y, bs in zip(loss_batches, batch_sizes) for x in y[:bs]]\n    self.update_with_all_losses(timesteps, losses)\n</code></pre> <code></code> update_with_all_losses <code>abstractmethod</code> \u00b6 <pre><code>update_with_all_losses(ts, losses)\n</code></pre> <p>Update the reweighting using losses from a model.</p> <p>Sub-classes should override this method to update the reweighting using losses from the model.</p> <p>This method directly updates the reweighting without synchronizing between workers. It is called by update_with_local_losses from all ranks with identical arguments. Thus, it should have deterministic behavior to maintain state across workers.</p> <p>:param ts: a list of int timesteps. :param losses: a list of float losses, one per timestep.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>@abstractmethod\ndef update_with_all_losses(self, ts: list[int], losses: list[float]) -&gt; None:\n    \"\"\"\n    Update the reweighting using losses from a model.\n\n    Sub-classes should override this method to update the reweighting\n    using losses from the model.\n\n    This method directly updates the reweighting without synchronizing\n    between workers. It is called by update_with_local_losses from all\n    ranks with identical arguments. Thus, it should have deterministic\n    behavior to maintain state across workers.\n\n    :param ts: a list of int timesteps.\n    :param losses: a list of float losses, one per timestep.\n    \"\"\"\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.FastTensorDataLoader","title":"FastTensorDataLoader","text":"<p>Defines a faster dataloader for PyTorch tensors.</p> <p>A DataLoader-like object for a set of tensors that can be much faster than TensorDataset + DataLoader because dataloader grabs individual indices of the dataset and calls cat (slow). Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class FastTensorDataLoader:\n    \"\"\"\n    Defines a faster dataloader for PyTorch tensors.\n\n    A DataLoader-like object for a set of tensors that can be much faster than\n    TensorDataset + DataLoader because dataloader grabs individual indices of\n    the dataset and calls cat (slow).\n    Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6\n    \"\"\"\n\n    def __init__(self, *tensors: Tensor, batch_size: int = 32, shuffle: bool = False):\n        \"\"\"\n        Initialize a FastTensorDataLoader.\n        :param *tensors: tensors to store. Must have the same length @ dim 0.\n        :param batch_size: batch size to load.\n        :param shuffle: if True, shuffle the data *in-place* whenever an\n            iterator is created out of this object.\n        :returns: A FastTensorDataLoader.\n        \"\"\"\n        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n        self.tensors = tensors\n\n        self.dataset_len = self.tensors[0].shape[0]\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n\n        # Calculate # batches\n        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n        if remainder &gt; 0:\n            n_batches += 1\n        self.n_batches = n_batches\n\n    def __iter__(self):\n        # ruff: noqa: D105\n        if self.shuffle:\n            r = torch.randperm(self.dataset_len)\n            self.tensors = [t[r] for t in self.tensors]  # type: ignore[assignment]\n        self.i = 0\n        return self\n\n    def __next__(self):\n        # ruff: noqa: D105\n        if self.i &gt;= self.dataset_len:\n            raise StopIteration\n        batch = tuple(t[self.i : self.i + self.batch_size] for t in self.tensors)\n        self.i += self.batch_size\n        return batch\n\n    def __len__(self):\n        # ruff: noqa: D105\n        return self.n_batches\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(*tensors, batch_size=32, shuffle=False)\n</code></pre> <p>Initialize a FastTensorDataLoader. :param tensors: tensors to store. Must have the same length @ dim 0. :param batch_size: batch size to load. :param shuffle: if True, shuffle the data in-place* whenever an     iterator is created out of this object. :returns: A FastTensorDataLoader.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def __init__(self, *tensors: Tensor, batch_size: int = 32, shuffle: bool = False):\n    \"\"\"\n    Initialize a FastTensorDataLoader.\n    :param *tensors: tensors to store. Must have the same length @ dim 0.\n    :param batch_size: batch size to load.\n    :param shuffle: if True, shuffle the data *in-place* whenever an\n        iterator is created out of this object.\n    :returns: A FastTensorDataLoader.\n    \"\"\"\n    assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n    self.tensors = tensors\n\n    self.dataset_len = self.tensors[0].shape[0]\n    self.batch_size = batch_size\n    self.shuffle = shuffle\n\n    # Calculate # batches\n    n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n    if remainder &gt; 0:\n        n_batches += 1\n    self.n_batches = n_batches\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.MLP","title":"MLP","text":"<p>               Bases: <code>Module</code></p> <p>The MLP model used in [gorishniy2021revisiting].</p> <p>The following scheme describes the architecture:</p> <p>.. code-block:: text</p> <pre><code>  MLP: (in) -&gt; Block -&gt; ... -&gt; Block -&gt; Linear -&gt; (out)\nBlock: (in) -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; (out)\n</code></pre> <p>Examples:</p> <p>.. testcode::</p> <pre><code>x = torch.randn(4, 2)\nmodule = MLP.make_baseline(x.shape[1], [3, 5], 0.1, 1)\nassert module(x).shape == (len(x), 1)\n</code></pre> References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class MLP(nn.Module):\n    \"\"\"The MLP model used in [gorishniy2021revisiting].\n\n    The following scheme describes the architecture:\n\n    .. code-block:: text\n\n          MLP: (in) -&gt; Block -&gt; ... -&gt; Block -&gt; Linear -&gt; (out)\n        Block: (in) -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; (out)\n\n    Examples:\n        .. testcode::\n\n            x = torch.randn(4, 2)\n            module = MLP.make_baseline(x.shape[1], [3, 5], 0.1, 1)\n            assert module(x).shape == (len(x), 1)\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n\n    class Block(nn.Module):\n        \"\"\"The main building block of `MLP`.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            d_in: int,\n            d_out: int,\n            bias: bool,\n            activation: ModuleType,\n            dropout: float,\n        ) -&gt; None:\n            super().__init__()\n            self.linear = nn.Linear(d_in, d_out, bias)\n            self.activation = _make_nn_module(activation)\n            self.dropout = nn.Dropout(dropout)\n\n        def forward(self, x: Tensor) -&gt; Tensor:\n            return self.dropout(self.activation(self.linear(x)))\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        d_layers: list[int],\n        dropouts: float | list[float],\n        activation: str | Callable[[], nn.Module],\n        d_out: int,\n    ) -&gt; None:\n        \"\"\"\n        Note:\n            `make_baseline` is the recommended constructor.\n        \"\"\"\n        super().__init__()\n        if isinstance(dropouts, float):\n            dropouts = [dropouts] * len(d_layers)\n        assert len(d_layers) == len(dropouts)\n        assert activation not in [\"ReGLU\", \"GEGLU\"]\n\n        self.blocks = nn.ModuleList(\n            [\n                MLP.Block(\n                    d_in=d_layers[i - 1] if i else d_in,\n                    d_out=d,\n                    bias=True,\n                    activation=activation,\n                    dropout=dropout,\n                )\n                for i, (d, dropout) in enumerate(zip(d_layers, dropouts))\n            ]\n        )\n        self.head = nn.Linear(d_layers[-1] if d_layers else d_in, d_out)\n\n    @classmethod\n    def make_baseline(\n        cls,\n        d_in: int,\n        d_layers: list[int],\n        dropout: float,\n        d_out: int,\n    ) -&gt; Self:\n        \"\"\"Create a \"baseline\" `MLP`.\n\n        This variation of MLP was used in [gorishniy2021revisiting]. Features:\n\n        * all linear layers except for the first one and the last one are of the same dimension\n        * the dropout rate is the same for all dropout layers\n\n        Args:\n            d_in: the input size\n            d_layers: the dimensions of the linear layers. If there are more than two\n                layers, then all of them except for the first and the last ones must\n                have the same dimension.\n            dropout: the dropout rate for all hidden layers\n            d_out: the output size\n        Returns:\n            MLP\n\n        References:\n            * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n            Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n        \"\"\"\n        assert isinstance(dropout, float)\n        if len(d_layers) &gt; 2:\n            assert len(set(d_layers[1:-1])) == 1, (\n                \"if d_layers contains more than two elements, then\"\n                \" all elements except for the first and the last ones must be equal.\"\n            )\n        return cls(\n            d_in=d_in,\n            d_layers=d_layers,\n            dropouts=dropout,\n            activation=\"ReLU\",\n            d_out=d_out,\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = x.float()\n        for block in self.blocks:\n            x = block(x)\n        return self.head(x)\n</code></pre> <code></code> Block \u00b6 <p>               Bases: <code>Module</code></p> <p>The main building block of <code>MLP</code>.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class Block(nn.Module):\n    \"\"\"The main building block of `MLP`.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        d_out: int,\n        bias: bool,\n        activation: ModuleType,\n        dropout: float,\n    ) -&gt; None:\n        super().__init__()\n        self.linear = nn.Linear(d_in, d_out, bias)\n        self.activation = _make_nn_module(activation)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        return self.dropout(self.activation(self.linear(x)))\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(*, d_in, d_layers, dropouts, activation, d_out)\n</code></pre> Note <p><code>make_baseline</code> is the recommended constructor.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def __init__(\n    self,\n    *,\n    d_in: int,\n    d_layers: list[int],\n    dropouts: float | list[float],\n    activation: str | Callable[[], nn.Module],\n    d_out: int,\n) -&gt; None:\n    \"\"\"\n    Note:\n        `make_baseline` is the recommended constructor.\n    \"\"\"\n    super().__init__()\n    if isinstance(dropouts, float):\n        dropouts = [dropouts] * len(d_layers)\n    assert len(d_layers) == len(dropouts)\n    assert activation not in [\"ReGLU\", \"GEGLU\"]\n\n    self.blocks = nn.ModuleList(\n        [\n            MLP.Block(\n                d_in=d_layers[i - 1] if i else d_in,\n                d_out=d,\n                bias=True,\n                activation=activation,\n                dropout=dropout,\n            )\n            for i, (d, dropout) in enumerate(zip(d_layers, dropouts))\n        ]\n    )\n    self.head = nn.Linear(d_layers[-1] if d_layers else d_in, d_out)\n</code></pre> <code></code> make_baseline <code>classmethod</code> \u00b6 <pre><code>make_baseline(d_in, d_layers, dropout, d_out)\n</code></pre> <p>Create a \"baseline\" <code>MLP</code>.</p> <p>This variation of MLP was used in [gorishniy2021revisiting]. Features:</p> <ul> <li>all linear layers except for the first one and the last one are of the same dimension</li> <li>the dropout rate is the same for all dropout layers</li> </ul> <p>Parameters:</p> Name Type Description Default <code>d_in</code> <code>int</code> <p>the input size</p> required <code>d_layers</code> <code>list[int]</code> <p>the dimensions of the linear layers. If there are more than two layers, then all of them except for the first and the last ones must have the same dimension.</p> required <code>dropout</code> <code>float</code> <p>the dropout rate for all hidden layers</p> required <code>d_out</code> <code>int</code> <p>the output size</p> required <p>Returns:     MLP</p> References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>@classmethod\ndef make_baseline(\n    cls,\n    d_in: int,\n    d_layers: list[int],\n    dropout: float,\n    d_out: int,\n) -&gt; Self:\n    \"\"\"Create a \"baseline\" `MLP`.\n\n    This variation of MLP was used in [gorishniy2021revisiting]. Features:\n\n    * all linear layers except for the first one and the last one are of the same dimension\n    * the dropout rate is the same for all dropout layers\n\n    Args:\n        d_in: the input size\n        d_layers: the dimensions of the linear layers. If there are more than two\n            layers, then all of them except for the first and the last ones must\n            have the same dimension.\n        dropout: the dropout rate for all hidden layers\n        d_out: the output size\n    Returns:\n        MLP\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n    assert isinstance(dropout, float)\n    if len(d_layers) &gt; 2:\n        assert len(set(d_layers[1:-1])) == 1, (\n            \"if d_layers contains more than two elements, then\"\n            \" all elements except for the first and the last ones must be equal.\"\n        )\n    return cls(\n        d_in=d_in,\n        d_layers=d_layers,\n        dropouts=dropout,\n        activation=\"ReLU\",\n        d_out=d_out,\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ResNet","title":"ResNet","text":"<p>               Bases: <code>Module</code></p> <p>The ResNet model used in [gorishniy2021revisiting].</p> <p>The following scheme describes the architecture: .. code-block:: text     ResNet: (in) -&gt; Linear -&gt; Block -&gt; ... -&gt; Block -&gt; Head -&gt; (out)              |-&gt; Norm -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; Linear -&gt; Dropout -&gt;|              |                                                                  |      Block: (in) ------------------------------------------------------------&gt; Add -&gt; (out)       Head: (in) -&gt; Norm -&gt; Activation -&gt; Linear -&gt; (out)</p> <p>Examples:</p> <p>.. testcode::     x = torch.randn(4, 2)     module = ResNet.make_baseline(         d_in=x.shape[1],         n_blocks=2,         d_main=3,         d_hidden=4,         dropout_first=0.25,         dropout_second=0.0,         d_out=1     )     assert module(x).shape == (len(x), 1)</p> References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class ResNet(nn.Module):\n    \"\"\"\n    The ResNet model used in [gorishniy2021revisiting].\n\n    The following scheme describes the architecture:\n    .. code-block:: text\n        ResNet: (in) -&gt; Linear -&gt; Block -&gt; ... -&gt; Block -&gt; Head -&gt; (out)\n                 |-&gt; Norm -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; Linear -&gt; Dropout -&gt;|\n                 |                                                                  |\n         Block: (in) ------------------------------------------------------------&gt; Add -&gt; (out)\n          Head: (in) -&gt; Norm -&gt; Activation -&gt; Linear -&gt; (out)\n\n    Examples:\n        .. testcode::\n            x = torch.randn(4, 2)\n            module = ResNet.make_baseline(\n                d_in=x.shape[1],\n                n_blocks=2,\n                d_main=3,\n                d_hidden=4,\n                dropout_first=0.25,\n                dropout_second=0.0,\n                d_out=1\n            )\n            assert module(x).shape == (len(x), 1)\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n\n    class Block(nn.Module):\n        \"\"\"The main building block of `ResNet`.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            d_main: int,\n            d_hidden: int,\n            bias_first: bool,\n            bias_second: bool,\n            dropout_first: float,\n            dropout_second: float,\n            normalization: ModuleType,\n            activation: ModuleType,\n            skip_connection: bool,\n        ) -&gt; None:\n            super().__init__()\n            self.normalization = _make_nn_module(normalization, d_main)\n            self.linear_first = nn.Linear(d_main, d_hidden, bias_first)\n            self.activation = _make_nn_module(activation)\n            self.dropout_first = nn.Dropout(dropout_first)\n            self.linear_second = nn.Linear(d_hidden, d_main, bias_second)\n            self.dropout_second = nn.Dropout(dropout_second)\n            self.skip_connection = skip_connection\n\n        def forward(self, x: Tensor) -&gt; Tensor:\n            x_input = x\n            x = self.normalization(x)\n            x = self.linear_first(x)\n            x = self.activation(x)\n            x = self.dropout_first(x)\n            x = self.linear_second(x)\n            x = self.dropout_second(x)\n            if self.skip_connection:\n                x = x_input + x\n            return x\n\n    class Head(nn.Module):\n        \"\"\"The final module of `ResNet`.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            d_in: int,\n            d_out: int,\n            bias: bool,\n            normalization: ModuleType,\n            activation: ModuleType,\n        ) -&gt; None:\n            super().__init__()\n            self.normalization = _make_nn_module(normalization, d_in)\n            self.activation = _make_nn_module(activation)\n            self.linear = nn.Linear(d_in, d_out, bias)\n\n        def forward(self, x: Tensor) -&gt; Tensor:\n            if self.normalization is not None:\n                x = self.normalization(x)\n            x = self.activation(x)\n            return self.linear(x)\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        n_blocks: int,\n        d_main: int,\n        d_hidden: int,\n        dropout_first: float,\n        dropout_second: float,\n        normalization: ModuleType,\n        activation: ModuleType,\n        d_out: int,\n    ) -&gt; None:\n        \"\"\"\n        Note:\n            `make_baseline` is the recommended constructor.\n        \"\"\"\n        super().__init__()\n\n        self.first_layer = nn.Linear(d_in, d_main)\n        if d_main is None:\n            d_main = d_in\n        self.blocks = nn.Sequential(\n            *[\n                ResNet.Block(\n                    d_main=d_main,\n                    d_hidden=d_hidden,\n                    bias_first=True,\n                    bias_second=True,\n                    dropout_first=dropout_first,\n                    dropout_second=dropout_second,\n                    normalization=normalization,\n                    activation=activation,\n                    skip_connection=True,\n                )\n                for _ in range(n_blocks)\n            ]\n        )\n        self.head = ResNet.Head(\n            d_in=d_main,\n            d_out=d_out,\n            bias=True,\n            normalization=normalization,\n            activation=activation,\n        )\n\n    @classmethod\n    def make_baseline(\n        cls,\n        *,\n        d_in: int,\n        n_blocks: int,\n        d_main: int,\n        d_hidden: int,\n        dropout_first: float,\n        dropout_second: float,\n        d_out: int,\n    ) -&gt; Self:\n        \"\"\"\n        Create a \"baseline\" `ResNet`. This variation of ResNet was used in [gorishniy2021revisiting].\n\n        Args:\n            d_in: the input size\n            n_blocks: the number of Blocks\n            d_main: the input size (or, equivalently, the output size) of each Block\n            d_hidden: the output size of the first linear layer in each Block\n            dropout_first: the dropout rate of the first dropout layer in each Block.\n            dropout_second: the dropout rate of the second dropout layer in each Block.\n            d_out: Output dimension.\n\n        References:\n            * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n            Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n        \"\"\"\n        return cls(\n            d_in=d_in,\n            n_blocks=n_blocks,\n            d_main=d_main,\n            d_hidden=d_hidden,\n            dropout_first=dropout_first,\n            dropout_second=dropout_second,\n            normalization=\"BatchNorm1d\",\n            activation=\"ReLU\",\n            d_out=d_out,\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = x.float()\n        x = self.first_layer(x)\n        x = self.blocks(x)\n        return self.head(x)\n</code></pre> <code></code> Block \u00b6 <p>               Bases: <code>Module</code></p> <p>The main building block of <code>ResNet</code>.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class Block(nn.Module):\n    \"\"\"The main building block of `ResNet`.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        d_main: int,\n        d_hidden: int,\n        bias_first: bool,\n        bias_second: bool,\n        dropout_first: float,\n        dropout_second: float,\n        normalization: ModuleType,\n        activation: ModuleType,\n        skip_connection: bool,\n    ) -&gt; None:\n        super().__init__()\n        self.normalization = _make_nn_module(normalization, d_main)\n        self.linear_first = nn.Linear(d_main, d_hidden, bias_first)\n        self.activation = _make_nn_module(activation)\n        self.dropout_first = nn.Dropout(dropout_first)\n        self.linear_second = nn.Linear(d_hidden, d_main, bias_second)\n        self.dropout_second = nn.Dropout(dropout_second)\n        self.skip_connection = skip_connection\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x_input = x\n        x = self.normalization(x)\n        x = self.linear_first(x)\n        x = self.activation(x)\n        x = self.dropout_first(x)\n        x = self.linear_second(x)\n        x = self.dropout_second(x)\n        if self.skip_connection:\n            x = x_input + x\n        return x\n</code></pre> <code></code> Head \u00b6 <p>               Bases: <code>Module</code></p> <p>The final module of <code>ResNet</code>.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class Head(nn.Module):\n    \"\"\"The final module of `ResNet`.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        d_out: int,\n        bias: bool,\n        normalization: ModuleType,\n        activation: ModuleType,\n    ) -&gt; None:\n        super().__init__()\n        self.normalization = _make_nn_module(normalization, d_in)\n        self.activation = _make_nn_module(activation)\n        self.linear = nn.Linear(d_in, d_out, bias)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        if self.normalization is not None:\n            x = self.normalization(x)\n        x = self.activation(x)\n        return self.linear(x)\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(\n    *,\n    d_in,\n    n_blocks,\n    d_main,\n    d_hidden,\n    dropout_first,\n    dropout_second,\n    normalization,\n    activation,\n    d_out,\n)\n</code></pre> Note <p><code>make_baseline</code> is the recommended constructor.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def __init__(\n    self,\n    *,\n    d_in: int,\n    n_blocks: int,\n    d_main: int,\n    d_hidden: int,\n    dropout_first: float,\n    dropout_second: float,\n    normalization: ModuleType,\n    activation: ModuleType,\n    d_out: int,\n) -&gt; None:\n    \"\"\"\n    Note:\n        `make_baseline` is the recommended constructor.\n    \"\"\"\n    super().__init__()\n\n    self.first_layer = nn.Linear(d_in, d_main)\n    if d_main is None:\n        d_main = d_in\n    self.blocks = nn.Sequential(\n        *[\n            ResNet.Block(\n                d_main=d_main,\n                d_hidden=d_hidden,\n                bias_first=True,\n                bias_second=True,\n                dropout_first=dropout_first,\n                dropout_second=dropout_second,\n                normalization=normalization,\n                activation=activation,\n                skip_connection=True,\n            )\n            for _ in range(n_blocks)\n        ]\n    )\n    self.head = ResNet.Head(\n        d_in=d_main,\n        d_out=d_out,\n        bias=True,\n        normalization=normalization,\n        activation=activation,\n    )\n</code></pre> <code></code> make_baseline <code>classmethod</code> \u00b6 <pre><code>make_baseline(\n    *,\n    d_in,\n    n_blocks,\n    d_main,\n    d_hidden,\n    dropout_first,\n    dropout_second,\n    d_out,\n)\n</code></pre> <p>Create a \"baseline\" <code>ResNet</code>. This variation of ResNet was used in [gorishniy2021revisiting].</p> <p>Parameters:</p> Name Type Description Default <code>d_in</code> <code>int</code> <p>the input size</p> required <code>n_blocks</code> <code>int</code> <p>the number of Blocks</p> required <code>d_main</code> <code>int</code> <p>the input size (or, equivalently, the output size) of each Block</p> required <code>d_hidden</code> <code>int</code> <p>the output size of the first linear layer in each Block</p> required <code>dropout_first</code> <code>float</code> <p>the dropout rate of the first dropout layer in each Block.</p> required <code>dropout_second</code> <code>float</code> <p>the dropout rate of the second dropout layer in each Block.</p> required <code>d_out</code> <code>int</code> <p>Output dimension.</p> required References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>@classmethod\ndef make_baseline(\n    cls,\n    *,\n    d_in: int,\n    n_blocks: int,\n    d_main: int,\n    d_hidden: int,\n    dropout_first: float,\n    dropout_second: float,\n    d_out: int,\n) -&gt; Self:\n    \"\"\"\n    Create a \"baseline\" `ResNet`. This variation of ResNet was used in [gorishniy2021revisiting].\n\n    Args:\n        d_in: the input size\n        n_blocks: the number of Blocks\n        d_main: the input size (or, equivalently, the output size) of each Block\n        d_hidden: the output size of the first linear layer in each Block\n        dropout_first: the dropout rate of the first dropout layer in each Block.\n        dropout_second: the dropout rate of the second dropout layer in each Block.\n        d_out: Output dimension.\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n    return cls(\n        d_in=d_in,\n        n_blocks=n_blocks,\n        d_main=d_main,\n        d_hidden=d_hidden,\n        dropout_first=dropout_first,\n        dropout_second=dropout_second,\n        normalization=\"BatchNorm1d\",\n        activation=\"ReLU\",\n        d_out=d_out,\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ReGLU","title":"ReGLU","text":"<p>               Bases: <code>Module</code></p> <p>The ReGLU activation function from [shazeer2020glu].</p> <p>Examples:</p> <p>.. testcode::</p> <pre><code>module = ReGLU()\nx = torch.randn(3, 4)\nassert module(x).shape == (3, 2)\n</code></pre> References <ul> <li>[shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class ReGLU(nn.Module):\n    \"\"\"The ReGLU activation function from [shazeer2020glu].\n\n    Examples:\n        .. testcode::\n\n            module = ReGLU()\n            x = torch.randn(3, 4)\n            assert module(x).shape == (3, 2)\n\n    References:\n        * [shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        # ruff: noqa: D102\n        return reglu(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.GEGLU","title":"GEGLU","text":"<p>               Bases: <code>Module</code></p> <p>The GEGLU activation function from [shazeer2020glu].</p> <p>Examples:</p> <p>.. testcode::</p> <pre><code>module = GEGLU()\nx = torch.randn(3, 4)\nassert module(x).shape == (3, 2)\n</code></pre> References <ul> <li>[shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class GEGLU(nn.Module):\n    \"\"\"The GEGLU activation function from [shazeer2020glu].\n\n    Examples:\n        .. testcode::\n\n            module = GEGLU()\n            x = torch.randn(3, 4)\n            assert module(x).shape == (3, 2)\n\n    References:\n        * [shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        # ruff: noqa: D102\n        return geglu(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.make_dataset_from_df","title":"make_dataset_from_df","text":"<pre><code>make_dataset_from_df(\n    df, T, is_y_cond, df_info, ratios=None, std=0\n)\n</code></pre> <p>The order of the generated dataset: (y, X_num, X_cat).</p> is_y_cond <p>concat: y is concatenated to X, the model learn a joint distribution of (y, X) embedding: y is not concatenated to X. During computations, y is embedded     and added to the latent vector of X none: y column is completely ignored</p> <p>How does is_y_cond affect the generation of y? is_y_cond:     concat: the model synthesizes (y, X) directly, so y is just the first column     embedding: y is first sampled using empirical distribution of y. The model only         synthesizes X. When returning the generated data, we return the generated X         and the sampled y. (y is sampled from empirical distribution, instead of being         generated by the model)         Note that in this way, y is still not independent of X, because the model has been         adding the embedding of y to the latent vector of X during computations.     none:         y is synthesized using y's empirical distribution. X is generated by the model.         In this case, y is completely independent of X.</p> <p>Note: For now, n_classes has to be set to 0. This is because our matrix is the concatenation of (X_num, X_cat). In this case, if we have is_y_cond == 'concat', we can guarantee that y is the first column of the matrix. However, if we have n_classes &gt; 0, then y is not the first column of the matrix.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def make_dataset_from_df(\n    # ruff: noqa: PLR0915, PLR0912\n    df: pd.DataFrame,\n    T: Transformations,\n    is_y_cond: str,\n    df_info: pd.DataFrame,\n    ratios: list[float] | None = None,\n    std: float = 0,\n) -&gt; tuple[Dataset, dict[int, LabelEncoder], list[int]]:\n    \"\"\"\n    The order of the generated dataset: (y, X_num, X_cat).\n\n    is_y_cond:\n        concat: y is concatenated to X, the model learn a joint distribution of (y, X)\n        embedding: y is not concatenated to X. During computations, y is embedded\n            and added to the latent vector of X\n        none: y column is completely ignored\n\n    How does is_y_cond affect the generation of y?\n    is_y_cond:\n        concat: the model synthesizes (y, X) directly, so y is just the first column\n        embedding: y is first sampled using empirical distribution of y. The model only\n            synthesizes X. When returning the generated data, we return the generated X\n            and the sampled y. (y is sampled from empirical distribution, instead of being\n            generated by the model)\n            Note that in this way, y is still not independent of X, because the model has been\n            adding the embedding of y to the latent vector of X during computations.\n        none:\n            y is synthesized using y's empirical distribution. X is generated by the model.\n            In this case, y is completely independent of X.\n\n    Note: For now, n_classes has to be set to 0. This is because our matrix is the concatenation\n    of (X_num, X_cat). In this case, if we have is_y_cond == 'concat', we can guarantee that y\n    is the first column of the matrix.\n    However, if we have n_classes &gt; 0, then y is not the first column of the matrix.\n    \"\"\"\n    if ratios is None:\n        ratios = [0.7, 0.2, 0.1]\n\n    train_val_df, test_df = train_test_split(df, test_size=ratios[2], random_state=42)\n    train_df, val_df = train_test_split(train_val_df, test_size=ratios[1] / (ratios[0] + ratios[1]), random_state=42)\n\n    cat_column_orders = []\n    num_column_orders = []\n    index_to_column = list(df.columns)\n    column_to_index = {col: i for i, col in enumerate(index_to_column)}\n\n    if df_info[\"n_classes\"] &gt; 0:\n        X_cat: dict[str, np.ndarray] | None = {} if df_info[\"cat_cols\"] is not None or is_y_cond == \"concat\" else None\n        X_num: dict[str, np.ndarray] | None = {} if df_info[\"num_cols\"] is not None else None\n        y = {}\n\n        cat_cols_with_y = []\n        if df_info[\"cat_cols\"] is not None:\n            cat_cols_with_y += df_info[\"cat_cols\"]\n        if is_y_cond == \"concat\":\n            cat_cols_with_y = [df_info[\"y_col\"]] + cat_cols_with_y\n\n        if len(cat_cols_with_y) &gt; 0:\n            X_cat[\"train\"] = train_df[cat_cols_with_y].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"val\"] = val_df[cat_cols_with_y].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"test\"] = test_df[cat_cols_with_y].to_numpy(dtype=np.str_)  # type: ignore[index]\n\n        y[\"train\"] = train_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"val\"] = val_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"test\"] = test_df[df_info[\"y_col\"]].values.astype(np.float32)\n\n        if df_info[\"num_cols\"] is not None:\n            X_num[\"train\"] = train_df[df_info[\"num_cols\"]].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"val\"] = val_df[df_info[\"num_cols\"]].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"test\"] = test_df[df_info[\"num_cols\"]].values.astype(np.float32)  # type: ignore[index]\n\n        cat_column_orders = [column_to_index[col] for col in cat_cols_with_y]\n        num_column_orders = [column_to_index[col] for col in df_info[\"num_cols\"]]\n\n    else:\n        X_cat = {} if df_info[\"cat_cols\"] is not None else None\n        X_num = {} if df_info[\"num_cols\"] is not None or is_y_cond == \"concat\" else None\n        y = {}\n\n        num_cols_with_y = []\n        if df_info[\"num_cols\"] is not None:\n            num_cols_with_y += df_info[\"num_cols\"]\n        if is_y_cond == \"concat\":\n            num_cols_with_y = [df_info[\"y_col\"]] + num_cols_with_y\n\n        if len(num_cols_with_y) &gt; 0:\n            X_num[\"train\"] = train_df[num_cols_with_y].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"val\"] = val_df[num_cols_with_y].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"test\"] = test_df[num_cols_with_y].values.astype(np.float32)  # type: ignore[index]\n\n        y[\"train\"] = train_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"val\"] = val_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"test\"] = test_df[df_info[\"y_col\"]].values.astype(np.float32)\n\n        if df_info[\"cat_cols\"] is not None:\n            X_cat[\"train\"] = train_df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"val\"] = val_df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"test\"] = test_df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)  # type: ignore[index]\n\n        cat_column_orders = [column_to_index[col] for col in df_info[\"cat_cols\"]]\n        num_column_orders = [column_to_index[col] for col in num_cols_with_y]\n\n    column_orders = num_column_orders + cat_column_orders\n    column_orders = [index_to_column[index] for index in column_orders]\n\n    label_encoders = {}\n    if X_cat is not None and len(df_info[\"cat_cols\"]) &gt; 0:\n        X_cat_all = np.vstack((X_cat[\"train\"], X_cat[\"val\"], X_cat[\"test\"]))\n        X_cat_converted = []\n        for col_index in range(X_cat_all.shape[1]):\n            label_encoder = LabelEncoder()\n            X_cat_converted.append(label_encoder.fit_transform(X_cat_all[:, col_index]).astype(float))\n            if std &gt; 0:\n                # add noise\n                X_cat_converted[-1] += np.random.normal(0, std, X_cat_converted[-1].shape)\n            label_encoders[col_index] = label_encoder\n\n        X_cat_converted = np.vstack(X_cat_converted).T  # type: ignore[assignment]\n\n        train_num = X_cat[\"train\"].shape[0]\n        val_num = X_cat[\"val\"].shape[0]\n        # test_num = X_cat[\"test\"].shape[0]\n\n        X_cat[\"train\"] = X_cat_converted[:train_num, :]  # type: ignore[call-overload]\n        X_cat[\"val\"] = X_cat_converted[train_num : train_num + val_num, :]  # type: ignore[call-overload]\n        X_cat[\"test\"] = X_cat_converted[train_num + val_num :, :]  # type: ignore[call-overload]\n\n        if X_num and len(X_num) &gt; 0:\n            X_num[\"train\"] = np.concatenate((X_num[\"train\"], X_cat[\"train\"]), axis=1)\n            X_num[\"val\"] = np.concatenate((X_num[\"val\"], X_cat[\"val\"]), axis=1)\n            X_num[\"test\"] = np.concatenate((X_num[\"test\"], X_cat[\"test\"]), axis=1)\n        else:\n            X_num = X_cat\n            X_cat = None\n\n    D = Dataset(\n        # ruff: noqa: N806\n        X_num,\n        None,\n        y,\n        y_info={},\n        task_type=TaskType(df_info[\"task_type\"]),\n        n_classes=df_info[\"n_classes\"],\n    )\n\n    return transform_dataset(D, T, None), label_encoders, column_orders\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.update_ema","title":"update_ema","text":"<pre><code>update_ema(target_params, source_params, rate=0.999)\n</code></pre> <p>Update target parameters to be closer to those of source parameters using an exponential moving average. :param target_params: the target parameter sequence. :param source_params: the source parameter sequence. :param rate: the EMA rate (closer to 1 means slower).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def update_ema(\n    target_params: Iterator[nn.Parameter],\n    source_params: Iterator[nn.Parameter],\n    rate: float = 0.999,\n) -&gt; None:\n    \"\"\"\n    Update target parameters to be closer to those of source parameters using\n    an exponential moving average.\n    :param target_params: the target parameter sequence.\n    :param source_params: the source parameter sequence.\n    :param rate: the EMA rate (closer to 1 means slower).\n    \"\"\"\n    for targ, src in zip(target_params, source_params):\n        targ.detach().mul_(rate).add_(src.detach(), alpha=1 - rate)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.create_named_schedule_sampler","title":"create_named_schedule_sampler","text":"<pre><code>create_named_schedule_sampler(name, diffusion)\n</code></pre> <p>Create a ScheduleSampler from a library of pre-defined samplers.</p> <p>:param name: the name of the sampler. :param diffusion: the diffusion object to sample for.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def create_named_schedule_sampler(name: str, diffusion: GaussianMultinomialDiffusion) -&gt; ScheduleSampler:\n    \"\"\"\n    Create a ScheduleSampler from a library of pre-defined samplers.\n\n    :param name: the name of the sampler.\n    :param diffusion: the diffusion object to sample for.\n    \"\"\"\n    if name == \"uniform\":\n        return UniformSampler(diffusion)\n    if name == \"loss-second-moment\":\n        return LossSecondMomentResampler(diffusion)\n    raise NotImplementedError(f\"unknown schedule sampler: {name}\")\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.timestep_embedding","title":"timestep_embedding","text":"<pre><code>timestep_embedding(timesteps, dim, max_period=10000)\n</code></pre> <p>Create sinusoidal timestep embeddings.</p> <p>:param timesteps: a 1-D Tensor of N indices, one per batch element.                   These may be fractional. :param dim: the dimension of the output. :param max_period: controls the minimum frequency of the embeddings. :return: an [N x dim] Tensor of positional embeddings.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def timestep_embedding(timesteps: Tensor, dim: int, max_period: int = 10000) -&gt; Tensor:\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2\n    freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(\n        device=timesteps.device\n    )\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.reglu","title":"reglu","text":"<pre><code>reglu(x)\n</code></pre> <p>The ReGLU activation function from [1].</p> References <p>[1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def reglu(x: Tensor) -&gt; Tensor:\n    \"\"\"The ReGLU activation function from [1].\n\n    References:\n        [1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n    assert x.shape[-1] % 2 == 0\n    a, b = x.chunk(2, dim=-1)\n    return a * F.relu(b)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.geglu","title":"geglu","text":"<pre><code>geglu(x)\n</code></pre> <p>The GEGLU activation function from [1].</p> References <p>[1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def geglu(x: Tensor) -&gt; Tensor:\n    \"\"\"The GEGLU activation function from [1].\n\n    References:\n        [1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n    assert x.shape[-1] % 2 == 0\n    a, b = x.chunk(2, dim=-1)\n    return a * F.gelu(b)\n</code></pre>"},{"location":"api/#data-loaders-module","title":"Data Loaders Module","text":""},{"location":"api/#midst_toolkit.core.data_loaders","title":"midst_toolkit.core.data_loaders","text":""},{"location":"api/#logger-module","title":"Logger Module","text":""},{"location":"api/#midst_toolkit.core.logger","title":"midst_toolkit.core.logger","text":"<p>Logger copied from OpenAI baselines to avoid extra RL-based dependencies.</p> <p>https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/logger.py</p>"},{"location":"api/#midst_toolkit.core.logger.TensorBoardOutputFormat","title":"TensorBoardOutputFormat","text":"<p>               Bases: <code>KVWriter</code></p> <p>Dumps key/value pairs into TensorBoard's numeric format.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>class TensorBoardOutputFormat(KVWriter):\n    \"\"\"Dumps key/value pairs into TensorBoard's numeric format.\"\"\"\n\n    def __init__(self, dir: str):\n        os.makedirs(dir, exist_ok=True)\n        self.dir = dir\n        self.step = 1\n        prefix = \"events\"\n        path = osp.join(osp.abspath(dir), prefix)\n        import tensorflow as tf\n        from tensorflow.core.util import event_pb2\n        from tensorflow.python import pywrap_tensorflow\n        from tensorflow.python.util import compat\n\n        self.tf = tf\n        self.event_pb2 = event_pb2\n        self.pywrap_tensorflow = pywrap_tensorflow\n        self.writer = pywrap_tensorflow.EventsWriter(compat.as_bytes(path))\n\n    def writekvs(self, kvs: dict[str, Any]) -&gt; None:\n        def summary_val(k: str, v: Any) -&gt; Any:\n            kwargs = {\"tag\": k, \"simple_value\": float(v)}\n            return self.tf.Summary.Value(**kwargs)\n\n        summary = self.tf.Summary(value=[summary_val(k, v) for k, v in kvs.items()])\n        event = self.event_pb2.Event(wall_time=time.time(), summary=summary)\n        event.step = self.step  # is there any reason why you'd want to specify the step?\n        self.writer.WriteEvent(event)\n        self.writer.Flush()\n        self.step += 1\n\n    def close(self) -&gt; None:\n        if self.writer:\n            self.writer.Close()\n            self.writer = None\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.logkv","title":"logkv","text":"<pre><code>logkv(key, val)\n</code></pre> <p>Log a value of some diagnostic.</p> <p>Call this once for each diagnostic quantity, each iteration If called many times, last value will be used.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def logkv(key: str, val: Any) -&gt; None:\n    \"\"\"\n    Log a value of some diagnostic.\n\n    Call this once for each diagnostic quantity, each iteration\n    If called many times, last value will be used.\n    \"\"\"\n    get_current().logkv(key, val)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.logkv_mean","title":"logkv_mean","text":"<pre><code>logkv_mean(key, val)\n</code></pre> <p>The same as logkv(), but if called many times, values averaged.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def logkv_mean(key: str, val: Any) -&gt; None:\n    \"\"\"The same as logkv(), but if called many times, values averaged.\"\"\"\n    get_current().logkv_mean(key, val)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.logkvs","title":"logkvs","text":"<pre><code>logkvs(d)\n</code></pre> <p>Log a dictionary of key-value pairs.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def logkvs(d: dict[str, Any]) -&gt; None:\n    \"\"\"Log a dictionary of key-value pairs.\"\"\"\n    for k, v in d.items():\n        logkv(k, v)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.dumpkvs","title":"dumpkvs","text":"<pre><code>dumpkvs()\n</code></pre> <p>Write all of the diagnostics from the current iteration.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def dumpkvs() -&gt; dict[str, Any]:\n    \"\"\"Write all of the diagnostics from the current iteration.\"\"\"\n    return get_current().dumpkvs()\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.log","title":"log","text":"<pre><code>log(*args, level=INFO)\n</code></pre> <p>Logs the args in the desired level.</p> <p>Write the sequence of args, with no separators, to the console and output files (if you've configured an output file).</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def log(*args: Iterable[Any], level: int = INFO) -&gt; None:\n    \"\"\"\n    Logs the args in the desired level.\n\n    Write the sequence of args, with no separators, to the console and output\n    files (if you've configured an output file).\n    \"\"\"\n    get_current().log(*args, level=level)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.set_level","title":"set_level","text":"<pre><code>set_level(level)\n</code></pre> <p>Set logging threshold on current logger.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def set_level(level: int) -&gt; None:\n    \"\"\"Set logging threshold on current logger.\"\"\"\n    get_current().set_level(level)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.get_dir","title":"get_dir","text":"<pre><code>get_dir()\n</code></pre> <p>Get directory that log files are being written to.</p> <p>will be None if there is no output directory (i.e., if you didn't call start)</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def get_dir() -&gt; str:\n    \"\"\"\n    Get directory that log files are being written to.\n\n    will be None if there is no output directory (i.e., if you didn't call start)\n    \"\"\"\n    return get_current().get_dir()\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.profile","title":"profile","text":"<pre><code>profile(n)\n</code></pre> <p>Usage.</p> <p>@profile(\"my_func\") def my_func(): code</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def profile(n):\n    \"\"\"\n    Usage.\n\n    @profile(\"my_func\")\n    def my_func(): code\n    \"\"\"\n\n    def decorator_with_name(func):\n        def func_wrapper(*args, **kwargs):\n            with profile_kv(n):\n                return func(*args, **kwargs)\n\n        return func_wrapper\n\n    return decorator_with_name\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.mpi_weighted_mean","title":"mpi_weighted_mean","text":"<pre><code>mpi_weighted_mean(comm, local_name2valcount)\n</code></pre> <p>Copied from below.</p> <p>https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/common/mpi_util.py#L110 Perform a weighted average over dicts that are each on a different node Input: local_name2valcount: dict mapping key -&gt; (value, count) Returns: key -&gt; mean</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def mpi_weighted_mean(comm: Any, local_name2valcount: dict[str, tuple[float, float]]) -&gt; dict[str, float]:\n    \"\"\"\n    Copied from below.\n\n    https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/common/mpi_util.py#L110\n    Perform a weighted average over dicts that are each on a different node\n    Input: local_name2valcount: dict mapping key -&gt; (value, count)\n    Returns: key -&gt; mean\n    \"\"\"\n    all_name2valcount = comm.gather(local_name2valcount)\n    if comm.rank == 0:\n        name2sum: defaultdict[str, float] = defaultdict(float)\n        name2count: defaultdict[str, float] = defaultdict(float)\n        for n2vc in all_name2valcount:\n            for name, (val, count) in n2vc.items():\n                try:\n                    val = float(val)\n                except ValueError:\n                    if comm.rank == 0:\n                        warnings.warn(\"WARNING: tried to compute mean on non-float {}={}\".format(name, val))\n                        # ruff: noqa: B028\n                else:\n                    name2sum[name] += val * count\n                    name2count[name] += count\n        return {name: name2sum[name] / name2count[name] for name in name2sum}\n    return {}\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.configure","title":"configure","text":"<pre><code>configure(\n    dir=None, format_strs=None, comm=None, log_suffix=\"\"\n)\n</code></pre> <p>If comm is provided, average all numerical stats across that comm.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def configure(\n    dir: str | None = None,\n    format_strs: list[str] | None = None,\n    comm: Any | None = None,\n    log_suffix: str = \"\",\n) -&gt; None:\n    \"\"\"If comm is provided, average all numerical stats across that comm.\"\"\"\n    if dir is None:\n        dir = os.getenv(\"OPENAI_LOGDIR\")\n    if dir is None:\n        dir = osp.join(\n            tempfile.gettempdir(),\n            datetime.datetime.now().strftime(\"openai-%Y-%m-%d-%H-%M-%S-%f\"),\n        )\n    assert isinstance(dir, str)\n    dir = os.path.expanduser(dir)\n    os.makedirs(os.path.expanduser(dir), exist_ok=True)\n\n    rank = get_rank_without_mpi_import()\n    if rank &gt; 0:\n        log_suffix = log_suffix + \"-rank%03i\" % rank\n\n    if format_strs is None:\n        if rank == 0:\n            format_strs = os.getenv(\"OPENAI_LOG_FORMAT\", \"stdout,log,csv\").split(\",\")\n        else:\n            format_strs = os.getenv(\"OPENAI_LOG_FORMAT_MPI\", \"log\").split(\",\")\n    format_strs_filter = filter(None, format_strs)\n    output_formats = [make_output_format(f, dir, log_suffix) for f in format_strs_filter]\n\n    Logger.CURRENT = Logger(dir=dir, output_formats=output_formats, comm=comm)  # type: ignore[assignment]\n    if output_formats:\n        log(\"Logging to %s\" % dir)\n</code></pre>"},{"location":"api/#diffusion-utils-module","title":"Diffusion Utils Module","text":""},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils","title":"midst_toolkit.models.clavaddpm.diffusion_utils","text":"<p>PLACEHOLDER.</p>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.FoundNANsError","title":"FoundNANsError","text":"<p>               Bases: <code>BaseException</code></p> <p>Found NANs during sampling.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>class FoundNANsError(BaseException):\n    \"\"\"Found NANs during sampling.\"\"\"\n\n    def __init__(self, message=\"Found NANs during sampling.\"):\n        # ruff: noqa: D107\n        super(FoundNANsError, self).__init__(message)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.normal_kl","title":"normal_kl","text":"<pre><code>normal_kl(mean1, logvar1, mean2, logvar2)\n</code></pre> <p>Compute the KL divergence between two gaussians.</p> <p>Shapes are automatically broadcasted, so batches can be compared to scalars, among other use cases.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def normal_kl(\n    mean1: Tensor | float,\n    logvar1: Tensor | float,\n    mean2: Tensor | float,\n    logvar2: Tensor | float,\n) -&gt; Tensor:\n    \"\"\"\n    Compute the KL divergence between two gaussians.\n\n    Shapes are automatically broadcasted, so batches can be compared to\n    scalars, among other use cases.\n    \"\"\"\n    tensor = None\n    for obj in (mean1, logvar1, mean2, logvar2):\n        if isinstance(obj, torch.Tensor):\n            tensor = obj\n            break\n    assert tensor is not None, \"at least one argument must be a Tensor\"\n\n    # Force variances to be Tensors. Broadcasting helps convert scalars to\n    # Tensors, but it does not work for torch.exp().\n    logvar1, logvar2 = [x if isinstance(x, torch.Tensor) else torch.tensor(x).to(tensor) for x in (logvar1, logvar2)]\n\n    return 0.5 * (\n        -1.0 + logvar2 - logvar1 + torch.exp(logvar1 - logvar2) + ((mean1 - mean2) ** 2) * torch.exp(-logvar2)\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.approx_standard_normal_cdf","title":"approx_standard_normal_cdf","text":"<pre><code>approx_standard_normal_cdf(x)\n</code></pre> <p>A fast approximation of the cumulative distribution function of the standard normal.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def approx_standard_normal_cdf(x: Tensor) -&gt; Tensor:\n    \"\"\"\n    A fast approximation of the cumulative distribution function of the\n    standard normal.\n    \"\"\"\n    return 0.5 * (1.0 + torch.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * torch.pow(x, 3))))\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.discretized_gaussian_log_likelihood","title":"discretized_gaussian_log_likelihood","text":"<pre><code>discretized_gaussian_log_likelihood(\n    x, *, means, log_scales\n)\n</code></pre> <p>Compute the log-likelihood of a Gaussian distribution discretizing to a given image.</p> <p>:param x: the target images. It is assumed that this was uint8 values,           rescaled to the range [-1, 1]. :param means: the Gaussian mean Tensor. :param log_scales: the Gaussian log stddev Tensor. :return: a tensor like x of log probabilities (in nats).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def discretized_gaussian_log_likelihood(x: Tensor, *, means: Tensor, log_scales: Tensor) -&gt; Tensor:\n    \"\"\"\n    Compute the log-likelihood of a Gaussian distribution discretizing to a\n    given image.\n\n    :param x: the target images. It is assumed that this was uint8 values,\n              rescaled to the range [-1, 1].\n    :param means: the Gaussian mean Tensor.\n    :param log_scales: the Gaussian log stddev Tensor.\n    :return: a tensor like x of log probabilities (in nats).\n    \"\"\"\n    assert x.shape == means.shape == log_scales.shape\n    centered_x = x - means\n    inv_stdv = torch.exp(-log_scales)\n    plus_in = inv_stdv * (centered_x + 1.0 / 255.0)\n    cdf_plus = approx_standard_normal_cdf(plus_in)\n    min_in = inv_stdv * (centered_x - 1.0 / 255.0)\n    cdf_min = approx_standard_normal_cdf(min_in)\n    log_cdf_plus = torch.log(cdf_plus.clamp(min=1e-12))\n    log_one_minus_cdf_min = torch.log((1.0 - cdf_min).clamp(min=1e-12))\n    cdf_delta = cdf_plus - cdf_min\n    log_probs = torch.where(\n        x &lt; -0.999,\n        log_cdf_plus,\n        torch.where(x &gt; 0.999, log_one_minus_cdf_min, torch.log(cdf_delta.clamp(min=1e-12))),\n    )\n    assert log_probs.shape == x.shape\n    return log_probs\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.sum_except_batch","title":"sum_except_batch","text":"<pre><code>sum_except_batch(x, num_dims=1)\n</code></pre> <p>Sums all dimensions except the first.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Tensor, shape (batch_size, ...)</p> required <code>num_dims</code> <code>int</code> <p>int, number of batch dims (default=1)</p> <code>1</code> <p>Returns:</p> Name Type Description <code>x_sum</code> <code>Tensor</code> <p>Tensor, shape (batch_size,)</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def sum_except_batch(x: Tensor, num_dims: int = 1) -&gt; Tensor:\n    \"\"\"\n    Sums all dimensions except the first.\n\n    Args:\n        x: Tensor, shape (batch_size, ...)\n        num_dims: int, number of batch dims (default=1)\n\n    Returns:\n        x_sum: Tensor, shape (batch_size,)\n    \"\"\"\n    return x.reshape(*x.shape[:num_dims], -1).sum(-1)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.mean_flat","title":"mean_flat","text":"<pre><code>mean_flat(tensor)\n</code></pre> <p>Take the mean over all non-batch dimensions.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def mean_flat(tensor: Tensor) -&gt; Tensor:\n    \"\"\"Take the mean over all non-batch dimensions.\"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n</code></pre>"},{"location":"api/#gaussian-multinomial-diffusion-module","title":"Gaussian Multinomial Diffusion Module","text":""},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion","title":"midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion","text":"<p>Based on the code below.</p> <p>https://github.com/openai/guided-diffusion/blob/main/guided_diffusion https://github.com/ehoogeboom/multinomial_diffusion</p>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.GaussianMultinomialDiffusion","title":"GaussianMultinomialDiffusion","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>class GaussianMultinomialDiffusion(torch.nn.Module):\n    def __init__(\n        # ruff: noqa: PLR0915\n        self,\n        num_classes: np.ndarray,\n        num_numerical_features: int,\n        denoise_fn: torch.nn.Module,\n        num_timesteps: int = 1000,\n        gaussian_loss_type: str = \"mse\",\n        gaussian_parametrization: str = \"eps\",\n        multinomial_loss_type: str = \"vb_stochastic\",\n        parametrization: str = \"x0\",\n        scheduler: str = \"cosine\",\n        device: torch.device | None = None,\n    ):\n        # ruff: noqa: D107\n        if device is None:\n            device = torch.device(\"cpu\")\n\n        super(GaussianMultinomialDiffusion, self).__init__()\n        assert multinomial_loss_type in (\"vb_stochastic\", \"vb_all\")\n        assert parametrization in (\"x0\", \"direct\")\n\n        if multinomial_loss_type == \"vb_all\":\n            print(\n                \"Computing the loss using the bound on _all_ timesteps.\"\n                \" This is expensive both in terms of memory and computation.\"\n            )\n\n        self.num_numerical_features = num_numerical_features\n        self.num_classes = num_classes  # it as a vector [K1, K2, ..., Km]\n        self.num_classes_expanded = torch.from_numpy(\n            np.concatenate([num_classes[i].repeat(num_classes[i]) for i in range(len(num_classes))])\n        ).to(device)\n\n        self.slices_for_classes = [np.arange(self.num_classes[0])]\n        offsets: np.ndarray = np.cumsum(self.num_classes)\n        for i in range(1, len(offsets)):\n            self.slices_for_classes.append(np.arange(offsets[i - 1], offsets[i]))\n        self.offsets = torch.from_numpy(np.append([0], offsets)).to(device)\n\n        self._denoise_fn = denoise_fn\n        self.gaussian_loss_type = gaussian_loss_type\n        self.gaussian_parametrization = gaussian_parametrization\n        self.multinomial_loss_type = multinomial_loss_type\n        self.num_timesteps = num_timesteps\n        self.parametrization = parametrization\n        self.scheduler = scheduler\n        self.device = device\n        self.alphas: Tensor\n        self.alphas_cumprod: Tensor\n        self.alphas_cumprod_next: Tensor\n        self.alphas_cumprod_prev: Tensor\n        self.sqrt_alphas_cumprod: Tensor\n        self.sqrt_one_minus_alphas_cumprod: Tensor\n        self.log_cumprod_alpha: Tensor\n        self.log_alpha: Tensor\n        self.log_1_min_alpha: Tensor\n        self.log_1_min_cumprod_alpha: Tensor\n        self.sqrt_recipm1_alphas_cumprod: Tensor\n        self.sqrt_recip_alphas_cumprod: Tensor\n        self.Lt_history: Tensor\n        self.Lt_count: Tensor\n\n        a = 1.0 - get_named_beta_schedule(scheduler, num_timesteps)\n        alphas = torch.tensor(a.astype(\"float64\"))\n        betas = 1.0 - alphas\n\n        log_alpha: Tensor = np.log(alphas)  # type: ignore[assignment]\n        log_cumprod_alpha: Tensor = np.cumsum(log_alpha)  # type: ignore[assignment]\n\n        log_1_min_alpha: Tensor = log_1_min_a(log_alpha)\n        log_1_min_cumprod_alpha: Tensor = log_1_min_a(log_cumprod_alpha)\n\n        alphas_cumprod: Tensor = np.cumprod(alphas, axis=0)  # type: ignore[assignment]\n        alphas_cumprod_prev = torch.tensor(np.append(1.0, alphas_cumprod[:-1]))\n        alphas_cumprod_next = torch.tensor(np.append(alphas_cumprod[1:], 0.0))\n        sqrt_alphas_cumprod: Tensor = np.sqrt(alphas_cumprod)  # type: ignore[assignment]\n        sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - alphas_cumprod)\n        sqrt_recip_alphas_cumprod: Tensor = np.sqrt(1.0 / alphas_cumprod)\n        sqrt_recipm1_alphas_cumprod: Tensor = np.sqrt(1.0 / alphas_cumprod - 1)\n\n        # Gaussian diffusion\n\n        self.posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n        self.posterior_log_variance_clipped = (\n            torch.from_numpy(np.log(np.append(self.posterior_variance[1], self.posterior_variance[1:])))\n            .float()\n            .to(device)\n        )\n        self.posterior_mean_coef1 = (betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)).float().to(device)\n        self.posterior_mean_coef2 = (\n            ((1.0 - alphas_cumprod_prev) * np.sqrt(alphas.numpy()) / (1.0 - alphas_cumprod)).float().to(device)\n        )\n\n        assert log_add_exp(log_alpha, log_1_min_alpha).abs().sum().item() &lt; 1.0e-5\n        assert log_add_exp(log_cumprod_alpha, log_1_min_cumprod_alpha).abs().sum().item() &lt; 1e-5\n        diff: Tensor = cast(Tensor, np.cumsum(log_alpha) - log_cumprod_alpha)\n        assert diff.abs().sum().item() &lt; 1.0e-5\n\n        # Convert to float32 and register buffers.\n        self.register_buffer(\"alphas\", alphas.float().to(device))\n        self.register_buffer(\"log_alpha\", log_alpha.float().to(device))\n        self.register_buffer(\"log_1_min_alpha\", log_1_min_alpha.float().to(device))\n        self.register_buffer(\"log_1_min_cumprod_alpha\", log_1_min_cumprod_alpha.float().to(device))\n        self.register_buffer(\"log_cumprod_alpha\", log_cumprod_alpha.float().to(device))\n        self.register_buffer(\"alphas_cumprod\", alphas_cumprod.float().to(device))\n        self.register_buffer(\"alphas_cumprod_prev\", alphas_cumprod_prev.float().to(device))\n        self.register_buffer(\"alphas_cumprod_next\", alphas_cumprod_next.float().to(device))\n        self.register_buffer(\"sqrt_alphas_cumprod\", sqrt_alphas_cumprod.float().to(device))\n        self.register_buffer(\n            \"sqrt_one_minus_alphas_cumprod\",\n            sqrt_one_minus_alphas_cumprod.float().to(device),\n        )\n        self.register_buffer(\"sqrt_recip_alphas_cumprod\", sqrt_recip_alphas_cumprod.float().to(device))\n        self.register_buffer(\n            \"sqrt_recipm1_alphas_cumprod\",\n            sqrt_recipm1_alphas_cumprod.float().to(device),\n        )\n\n        self.register_buffer(\"Lt_history\", torch.zeros(num_timesteps))\n        self.register_buffer(\"Lt_count\", torch.zeros(num_timesteps))\n\n    # Gaussian part\n    def gaussian_q_mean_variance(self, x_start: Tensor, t: Tensor) -&gt; tuple[Tensor, Tensor, Tensor]:\n        mean = extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n        variance = extract(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract(self.log_1_min_cumprod_alpha, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def gaussian_q_sample(self, x_start: Tensor, t: Tensor, noise: Tensor | None = None) -&gt; Tensor:\n        if noise is None:\n            noise = torch.randn_like(x_start)\n        assert noise.shape == x_start.shape\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n            + extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n        )\n\n    def gaussian_q_posterior_mean_variance(\n        self,\n        x_start: Tensor,\n        x_t: Tensor,\n        t: Tensor,\n    ) -&gt; tuple[Tensor, Tensor, Tensor]:\n        assert x_start.shape == x_t.shape\n        posterior_mean = (\n            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start\n            + extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n        assert (\n            posterior_mean.shape[0]\n            == posterior_variance.shape[0]\n            == posterior_log_variance_clipped.shape[0]\n            == x_start.shape[0]\n        )\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def gaussian_p_mean_variance(\n        self,\n        model_output: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        denoised_fn: Callable | None = None,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Tensor]:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        B, C = x.shape[:2]\n        assert t.shape == (B,)\n\n        model_variance = torch.cat(\n            [\n                self.posterior_variance[1].unsqueeze(0).to(x.device),\n                (1.0 - self.alphas)[1:],\n            ],\n            dim=0,\n        )\n        # model_variance = self.posterior_variance.to(x.device)\n        model_log_variance = torch.log(model_variance)\n\n        model_variance = extract(model_variance, t, x.shape)\n        model_log_variance = extract(model_log_variance, t, x.shape)\n\n        if self.gaussian_parametrization == \"eps\":\n            pred_xstart = self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n        elif self.gaussian_parametrization == \"x0\":\n            pred_xstart = model_output\n        else:\n            raise NotImplementedError\n\n        model_mean, _, _ = self.gaussian_q_posterior_mean_variance(x_start=pred_xstart, x_t=x, t=t)\n\n        assert model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape, (\n            f\"{model_mean.shape}, {model_log_variance.shape}, {pred_xstart.shape}, {x.shape}\"\n        )\n\n        return {\n            \"mean\": model_mean,\n            \"variance\": model_variance,\n            \"log_variance\": model_log_variance,\n            \"pred_xstart\": pred_xstart,\n        }\n\n    def _vb_terms_bpd(\n        self,\n        model_output: Tensor,\n        x_start: Tensor,\n        x_t: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Tensor]:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        (\n            true_mean,\n            _,\n            true_log_variance_clipped,\n        ) = self.gaussian_q_posterior_mean_variance(x_start=x_start, x_t=x_t, t=t)\n        out = self.gaussian_p_mean_variance(\n            model_output, x_t, t, clip_denoised=clip_denoised, model_kwargs=model_kwargs\n        )\n        kl = normal_kl(true_mean, true_log_variance_clipped, out[\"mean\"], out[\"log_variance\"])\n        kl = mean_flat(kl) / np.log(2.0)\n\n        decoder_nll = -discretized_gaussian_log_likelihood(\n            x_start, means=out[\"mean\"], log_scales=0.5 * out[\"log_variance\"]\n        )\n        assert decoder_nll.shape == x_start.shape\n        decoder_nll = mean_flat(decoder_nll) / np.log(2.0)\n\n        # At the first timestep return the decoder NLL,\n        # otherwise return KL(q(x_{t-1}|x_t,x_0) || p(x_{t-1}|x_t))\n        output = torch.where((t == 0), decoder_nll, kl)\n        return {\n            \"output\": output,\n            \"pred_xstart\": out[\"pred_xstart\"],\n            \"out_mean\": out[\"mean\"],\n            \"true_mean\": true_mean,\n        }\n\n    def _prior_gaussian(self, x_start: Tensor) -&gt; Tensor:\n        \"\"\"\n        Get the prior KL term for the variational lower-bound, measured in\n        bits-per-dim.\n\n        This term can't be optimized, as it only depends on the encoder.\n\n        :param x_start: the [N x C x ...] tensor of inputs.\n        :return: a batch of [N] KL values (in bits), one per batch element.\n        \"\"\"\n        batch_size = x_start.shape[0]\n        t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n        qt_mean, _, qt_log_variance = self.gaussian_q_mean_variance(x_start, t)\n        kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0)\n        return mean_flat(kl_prior) / np.log(2.0)\n\n    def _gaussian_loss(\n        self,\n        model_out: Tensor,\n        x_start: Tensor,\n        x_t: Tensor,\n        t: Tensor,\n        noise: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; Tensor:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        terms = {}\n        if self.gaussian_loss_type == \"mse\":\n            terms[\"loss\"] = mean_flat((noise - model_out) ** 2)\n        elif self.gaussian_loss_type == \"kl\":\n            terms[\"loss\"] = self._vb_terms_bpd(\n                model_output=model_out,\n                x_start=x_start,\n                x_t=x_t,\n                t=t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n            )[\"output\"]\n\n        return terms[\"loss\"]\n\n    def _predict_xstart_from_eps(self, x_t: Tensor, t: Tensor, eps: Tensor) -&gt; Tensor:\n        assert x_t.shape == eps.shape\n        return (\n            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n            - extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * eps\n        )\n\n    def _predict_eps_from_xstart(self, x_t: Tensor, t: Tensor, pred_xstart: Tensor) -&gt; Tensor:\n        return (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / extract(\n            self.sqrt_recipm1_alphas_cumprod, t, x_t.shape\n        )\n\n    def condition_mean(\n        self,\n        cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n        p_mean_var: dict[str, Tensor],\n        x: Tensor,\n        t: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; Tensor:\n        \"\"\"\n        Compute the mean for the previous step, given a function cond_fn that\n        computes the gradient of a conditional log probability with respect to\n        x. In particular, cond_fn computes grad(log(p(y|x))), and we want to\n        condition on y.\n\n        This uses the conditioning strategy from Sohl-Dickstein et al. (2015).\n        \"\"\"\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        gradient = cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n        return p_mean_var[\"mean\"].float() + p_mean_var[\"variance\"] * gradient.float()\n\n    def condition_score(\n        self,\n        cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n        p_mean_var: dict[str, Tensor],\n        x: Tensor,\n        t: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Tensor]:\n        \"\"\"\n        Compute what the p_mean_variance output would have been, should the\n        model's score function be conditioned by cond_fn.\n\n        See condition_mean() for details on cond_fn.\n\n        Unlike condition_mean(), this instead uses the conditioning strategy\n        from Song et al (2020).\n        \"\"\"\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        alpha_bar = extract(self.alphas_cumprod, t, x.shape)\n\n        eps = self._predict_eps_from_xstart(x, t, p_mean_var[\"pred_xstart\"])\n        eps = eps - (1 - alpha_bar).sqrt() * cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n\n        out = p_mean_var.copy()\n        out[\"pred_xstart\"] = self._predict_xstart_from_eps(x, t, eps)\n        out[\"mean\"], _, _ = self.gaussian_q_posterior_mean_variance(x_start=out[\"pred_xstart\"], x_t=x, t=t)\n        return out\n\n    def gaussian_p_sample(\n        self,\n        model_out: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        denoised_fn: Callable | None = None,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; dict[str, Tensor]:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        out = self.gaussian_p_mean_variance(\n            model_out,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n        )\n        noise = torch.randn_like(x)\n        nonzero_mask = (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))  # no noise when t == 0\n\n        if cond_fn is not None:\n            out[\"mean\"] = self.condition_mean(cond_fn, out, x, t, model_kwargs=model_kwargs)\n\n        sample = out[\"mean\"] + nonzero_mask * torch.exp(0.5 * out[\"log_variance\"]) * noise\n        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    # Multinomial part\n\n    def multinomial_kl(self, log_prob1: Tensor, log_prob2: Tensor) -&gt; Tensor:\n        return (log_prob1.exp() * (log_prob1 - log_prob2)).sum(dim=1)\n\n    def q_pred_one_timestep(self, log_x_t: Tensor, t: Tensor) -&gt; Tensor:\n        log_alpha_t = extract(self.log_alpha, t, log_x_t.shape)\n        log_1_min_alpha_t = extract(self.log_1_min_alpha, t, log_x_t.shape)\n\n        # alpha_t * E[xt] + (1 - alpha_t) 1 / K\n        return log_add_exp(\n            log_x_t + log_alpha_t,\n            log_1_min_alpha_t - torch.log(self.num_classes_expanded),\n        )\n\n    def q_pred(self, log_x_start: Tensor, t: Tensor) -&gt; Tensor:\n        log_cumprod_alpha_t = extract(self.log_cumprod_alpha, t, log_x_start.shape)\n        log_1_min_cumprod_alpha = extract(self.log_1_min_cumprod_alpha, t, log_x_start.shape)\n\n        return log_add_exp(\n            log_x_start + log_cumprod_alpha_t,\n            log_1_min_cumprod_alpha - torch.log(self.num_classes_expanded),\n        )\n\n    def predict_start(self, model_out: Tensor, log_x_t: Tensor, t: Tensor, out_dict: dict[str, Tensor]) -&gt; Tensor:\n        # model_out = self._denoise_fn(x_t, t.to(x_t.device), **out_dict)\n\n        assert model_out.size(0) == log_x_t.size(0)\n        assert self.num_classes is not None\n        assert model_out.size(1) == self.num_classes.sum(), f\"{model_out.size()}\"\n\n        log_pred = torch.empty_like(model_out)\n        for ix in self.slices_for_classes:\n            log_pred[:, ix] = F.log_softmax(model_out[:, ix], dim=1)\n        return log_pred\n\n    def q_posterior(self, log_x_start: Tensor, log_x_t: Tensor, t: Tensor) -&gt; Tensor:\n        # q(xt-1 | xt, x0) = q(xt | xt-1, x0) * q(xt-1 | x0) / q(xt | x0)\n        # where q(xt | xt-1, x0) = q(xt | xt-1).\n\n        # EV_log_qxt_x0 = self.q_pred(log_x_start, t)\n\n        # print('sum exp', EV_log_qxt_x0.exp().sum(1).mean())\n        # assert False\n\n        # log_qxt_x0 = (log_x_t.exp() * EV_log_qxt_x0).sum(dim=1)\n        t_minus_1 = t - 1\n        # Remove negative values, will not be used anyway for final decoder\n        t_minus_1 = torch.where(t_minus_1 &lt; 0, torch.zeros_like(t_minus_1), t_minus_1)\n        log_EV_qxtmin_x0 = self.q_pred(log_x_start, t_minus_1)\n\n        num_axes = (1,) * (len(log_x_start.size()) - 1)\n        t_broadcast = t.to(log_x_start.device).view(-1, *num_axes) * torch.ones_like(log_x_start)\n        log_EV_qxtmin_x0 = torch.where(t_broadcast == 0, log_x_start, log_EV_qxtmin_x0.to(torch.float32))\n\n        # unnormed_logprobs = log_EV_qxtmin_x0 +\n        #                     log q_pred_one_timestep(x_t, t)\n        # Note: _NOT_ x_tmin1, which is how the formula is typically used!!!\n        # Not very easy to see why this is true. But it is :)\n        unnormed_logprobs = log_EV_qxtmin_x0 + self.q_pred_one_timestep(log_x_t, t)\n\n        return unnormed_logprobs - sliced_logsumexp(unnormed_logprobs, self.offsets)\n\n    def p_pred(self, model_out: Tensor, log_x: Tensor, t: Tensor, out_dict: dict[str, Tensor]) -&gt; Tensor:\n        if self.parametrization == \"x0\":\n            log_x_recon = self.predict_start(model_out, log_x, t=t, out_dict=out_dict)\n            log_model_pred = self.q_posterior(log_x_start=log_x_recon, log_x_t=log_x, t=t)\n        elif self.parametrization == \"direct\":\n            log_model_pred = self.predict_start(model_out, log_x, t=t, out_dict=out_dict)\n        else:\n            raise ValueError\n        return log_model_pred\n\n    @torch.no_grad()\n    def p_sample(self, model_out: Tensor, log_x: Tensor, t: Tensor, out_dict: dict[str, Tensor]) -&gt; Tensor:\n        model_log_prob = self.p_pred(model_out, log_x=log_x, t=t, out_dict=out_dict)\n        return self.log_sample_categorical(model_log_prob)\n\n    # Dead code\n    # @torch.no_grad()\n    # def p_sample_loop(self, shape, out_dict):\n    #     b = shape[0]\n    #     # start with random normal image.\n    #     img = torch.randn(shape, device=device)\n\n    #     for i in reversed(range(1, self.num_timesteps)):\n    #         img = self.p_sample(img, torch.full((b,), i, device=self.device, dtype=torch.long), out_dict)\n    #     return img\n\n    # @torch.no_grad()\n    # def _sample(self, image_size, out_dict, batch_size=16):\n    #     return self.p_sample_loop((batch_size, 3, image_size, image_size), out_dict)\n\n    # Dead code\n    # @torch.no_grad()\n    # def interpolate(self, x1: Tensor, x2: Tensor, t: Tensor | None = None, lam: float = 0.5) -&gt; Tensor:\n    #     b, *_, device = *x1.shape, x1.device\n    #     t = default(t, self.num_timesteps - 1)\n\n    #     assert x1.shape == x2.shape\n\n    #     t_batched = torch.stack([torch.tensor(t, device=device)] * b)\n    #     xt1, xt2 = map(lambda x: self.q_sample(x, t=t_batched), (x1, x2))\n\n    #     img = (1 - lam) * xt1 + lam * xt2\n    #     for i in reversed(range(0, t)):\n    #         img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long))\n\n    #     return img\n\n    def log_sample_categorical(self, logits: Tensor) -&gt; Tensor:\n        full_sample = []\n        for i in range(len(self.num_classes)):\n            one_class_logits = logits[:, self.slices_for_classes[i]]\n            uniform = torch.rand_like(one_class_logits)\n            gumbel_noise = -torch.log(-torch.log(uniform + 1e-30) + 1e-30)\n            sample = (gumbel_noise + one_class_logits).argmax(dim=1)\n            full_sample.append(sample.unsqueeze(1))\n        full_sample_tensor = torch.cat(full_sample, dim=1)\n        return index_to_log_onehot(full_sample_tensor, torch.from_numpy(self.num_classes))\n\n    def q_sample(self, log_x_start: Tensor, t: Tensor) -&gt; Tensor:\n        log_EV_qxt_x0 = self.q_pred(log_x_start, t)\n        # ruff: noqa: N806\n        return self.log_sample_categorical(log_EV_qxt_x0)\n\n    # Dead code\n    # def nll(self, log_x_start, out_dict):\n    #     b = log_x_start.size(0)\n    #     device = log_x_start.device\n    #     loss = 0\n    #     for t in range(0, self.num_timesteps):\n    #         t_array = (torch.ones(b, device=device) * t).long()\n\n    #         kl = self.compute_Lt(\n    #             log_x_start=log_x_start,\n    #             log_x_t=self.q_sample(log_x_start=log_x_start, t=t_array),\n    #             t=t_array,\n    #             out_dict=out_dict,\n    #         )\n\n    #         loss += kl\n\n    #     loss += self.kl_prior(log_x_start)\n\n    #     return loss\n\n    def kl_prior(self, log_x_start: Tensor) -&gt; Tensor:\n        b = log_x_start.size(0)\n        device = log_x_start.device\n        ones = torch.ones(b, device=device).long()\n\n        log_qxT_prob = self.q_pred(log_x_start, t=(self.num_timesteps - 1) * ones)\n        # ruff: noqa: N806\n        log_half_prob = -torch.log(self.num_classes_expanded * torch.ones_like(log_qxT_prob))\n\n        kl_prior = self.multinomial_kl(log_qxT_prob, log_half_prob)\n        return sum_except_batch(kl_prior)\n\n    def compute_Lt(\n        # ruff: noqa: N802\n        self,\n        model_out: Tensor,\n        log_x_start: Tensor,\n        log_x_t: Tensor,\n        t: Tensor,\n        out_dict: dict[str, Tensor],\n        detach_mean: bool = False,\n    ) -&gt; Tensor:\n        log_true_prob = self.q_posterior(log_x_start=log_x_start, log_x_t=log_x_t, t=t)\n        log_model_prob = self.p_pred(model_out, log_x=log_x_t, t=t, out_dict=out_dict)\n\n        if detach_mean:\n            log_model_prob = log_model_prob.detach()\n\n        kl = self.multinomial_kl(log_true_prob, log_model_prob)\n        kl = sum_except_batch(kl)\n\n        decoder_nll = -log_categorical(log_x_start, log_model_prob)\n        decoder_nll = sum_except_batch(decoder_nll)\n\n        mask = (t == torch.zeros_like(t)).float()\n        return mask * decoder_nll + (1.0 - mask) * kl\n\n    def sample_time(self, b: int, device: torch.device, method: str = \"uniform\") -&gt; tuple[Tensor, Tensor]:\n        if method == \"importance\":\n            if not (self.Lt_count &gt; 10).all():\n                return self.sample_time(b, device, method=\"uniform\")\n\n            Lt_sqrt = torch.sqrt(self.Lt_history + 1e-10) + 0.0001\n            # ruff: noqa: N806\n            Lt_sqrt[0] = Lt_sqrt[1]  # Overwrite decoder term with L1.\n            pt_all = (Lt_sqrt / Lt_sqrt.sum()).to(device)\n\n            t = torch.multinomial(pt_all, num_samples=b, replacement=True).to(device)\n\n            pt = pt_all.gather(dim=0, index=t)\n\n            return t, pt\n\n        if method == \"uniform\":\n            t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n\n            pt = torch.ones_like(t).float() / self.num_timesteps\n            return t, pt\n        raise ValueError\n\n    def _multinomial_loss(\n        self,\n        model_out: Tensor,\n        log_x_start: Tensor,\n        log_x_t: Tensor,\n        t: Tensor,\n        pt: Tensor,\n        out_dict: dict[str, Tensor],\n    ) -&gt; Tensor:\n        if self.multinomial_loss_type == \"vb_stochastic\":\n            kl = self.compute_Lt(model_out, log_x_start, log_x_t, t, out_dict)\n            kl_prior = self.kl_prior(log_x_start)\n            # Upweigh loss term of the kl\n            return kl / pt + kl_prior\n\n        if self.multinomial_loss_type == \"vb_all\":\n            # Expensive, dont do it ;).\n            # DEPRECATED\n            # return -self.nll(log_x_start)\n            raise ValueError(\"multinomial_loss_type == 'vb_all' is deprecated.\")\n        raise ValueError\n\n    # Dead code\n    # def log_prob(self, x, out_dict):\n    #     b, device = x.size(0), x.device\n    #     if self.training:\n    #         return self._multinomial_loss(x, out_dict)\n\n    #     log_x_start = index_to_log_onehot(x, self.num_classes)\n\n    #     t, pt = self.sample_time(b, device, \"importance\")\n\n    #     kl = self.compute_Lt(log_x_start, self.q_sample(log_x_start=log_x_start, t=t), t, out_dict)\n\n    #     kl_prior = self.kl_prior(log_x_start)\n\n    #     # Upweigh loss term of the kl\n    #     loss = kl / pt + kl_prior\n\n    #     return -loss\n\n    def mixed_loss(self, x: Tensor, out_dict: dict[str, Tensor]) -&gt; tuple[Tensor, Tensor]:\n        b = x.shape[0]\n        device = x.device\n        t, pt = self.sample_time(b, device, \"uniform\")\n\n        x_num = x[:, : self.num_numerical_features]\n        x_cat = x[:, self.num_numerical_features :]\n\n        x_num_t = x_num\n        log_x_cat_t = x_cat\n        if x_num.shape[1] &gt; 0:\n            noise = torch.randn_like(x_num)\n            x_num_t = self.gaussian_q_sample(x_num, t, noise=noise)\n        if x_cat.shape[1] &gt; 0:\n            log_x_cat = index_to_log_onehot(x_cat.long(), torch.from_numpy(self.num_classes))\n            log_x_cat_t = self.q_sample(log_x_start=log_x_cat, t=t)\n\n        x_in = torch.cat([x_num_t, log_x_cat_t], dim=1)\n\n        model_out = self._denoise_fn(x_in, t, **out_dict)\n\n        model_out_num = model_out[:, : self.num_numerical_features]\n        model_out_cat = model_out[:, self.num_numerical_features :]\n\n        loss_multi = torch.zeros((1,)).float()\n        loss_gauss = torch.zeros((1,)).float()\n        if x_cat.shape[1] &gt; 0:\n            loss_multi = self._multinomial_loss(model_out_cat, log_x_cat, log_x_cat_t, t, pt, out_dict) / len(\n                self.num_classes\n            )\n\n        if x_num.shape[1] &gt; 0:\n            loss_gauss = self._gaussian_loss(model_out_num, x_num, x_num_t, t, noise)\n\n        # loss_multi = torch.where(out_dict['y'] == 1, loss_multi, 2 * loss_multi)\n        # loss_gauss = torch.where(out_dict['y'] == 1, loss_gauss, 2 * loss_gauss)\n\n        return loss_multi.mean(), loss_gauss.mean()\n\n    @torch.no_grad()\n    def mixed_elbo(self, x0, out_dict):\n        b = x0.size(0)\n        device = x0.device\n\n        x_num = x0[:, : self.num_numerical_features]\n        x_cat = x0[:, self.num_numerical_features :]\n        has_cat = x_cat.shape[1] &gt; 0\n        if has_cat:\n            log_x_cat = index_to_log_onehot(x_cat.long(), torch.from_numpy(self.num_classes)).to(device)\n\n        gaussian_loss = []\n        xstart_mse = []\n        mse = []\n        # mu_mse = []\n        out_mean = []\n        true_mean = []\n        multinomial_loss = []\n        for t in range(self.num_timesteps):\n            t_array = (torch.ones(b, device=device) * t).long()\n            noise = torch.randn_like(x_num)\n\n            x_num_t = self.gaussian_q_sample(x_start=x_num, t=t_array, noise=noise)\n            log_x_cat_t = self.q_sample(log_x_start=log_x_cat, t=t_array) if has_cat else x_cat\n\n            model_out = self._denoise_fn(torch.cat([x_num_t, log_x_cat_t], dim=1), t_array, **out_dict)\n\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n\n            kl = torch.tensor([0.0])\n            if has_cat:\n                kl = self.compute_Lt(\n                    model_out=model_out_cat,\n                    log_x_start=log_x_cat,\n                    log_x_t=log_x_cat_t,\n                    t=t_array,\n                    out_dict=out_dict,\n                )\n\n            out = self._vb_terms_bpd(\n                model_out_num,\n                x_start=x_num,\n                x_t=x_num_t,\n                t=t_array,\n                clip_denoised=False,\n            )\n\n            multinomial_loss.append(kl)\n            gaussian_loss.append(out[\"output\"])\n            xstart_mse.append(mean_flat((out[\"pred_xstart\"] - x_num) ** 2))\n            # mu_mse.append(mean_flat(out[\"mean_mse\"]))\n            out_mean.append(mean_flat(out[\"out_mean\"]))\n            true_mean.append(mean_flat(out[\"true_mean\"]))\n\n            eps = self._predict_eps_from_xstart(x_num_t, t_array, out[\"pred_xstart\"])\n            mse.append(mean_flat((eps - noise) ** 2))\n\n        gaussian_loss_tensor = torch.stack(gaussian_loss, dim=1)\n        multinomial_loss_tensor = torch.stack(multinomial_loss, dim=1)\n        xstart_mse_tensor = torch.stack(xstart_mse, dim=1)\n        mse_tensor = torch.stack(mse, dim=1)\n        # mu_mse = torch.stack(mu_mse, dim=1)\n        out_mean_tensor = torch.stack(out_mean, dim=1)\n        true_mean_tensor = torch.stack(true_mean, dim=1)\n\n        prior_gauss = self._prior_gaussian(x_num)\n\n        prior_multin = torch.tensor([0.0])\n        if has_cat:\n            prior_multin = self.kl_prior(log_x_cat)\n\n        total_gauss = gaussian_loss_tensor.sum(dim=1) + prior_gauss\n        total_multin = multinomial_loss_tensor.sum(dim=1) + prior_multin\n        return {\n            \"total_gaussian\": total_gauss,\n            \"total_multinomial\": total_multin,\n            \"losses_gaussian\": gaussian_loss_tensor,\n            \"losses_multinimial\": multinomial_loss_tensor,\n            \"xstart_mse\": xstart_mse_tensor,\n            \"mse\": mse_tensor,\n            # \"mu_mse\": mu_mse\n            \"out_mean\": out_mean_tensor,\n            \"true_mean\": true_mean_tensor,\n        }\n\n    @torch.no_grad()\n    def gaussian_ddim_step(\n        self,\n        model_out_num: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        denoised_fn: Callable | None = None,\n        eta: float = 0.0,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; Tensor:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        out = self.gaussian_p_mean_variance(\n            model_out_num,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=None,\n        )\n\n        if cond_fn is not None:\n            out = self.condition_score(cond_fn, out, x, t, model_kwargs=model_kwargs)\n\n        eps = self._predict_eps_from_xstart(x, t, out[\"pred_xstart\"])\n\n        alpha_bar = extract(self.alphas_cumprod, t, x.shape)\n        alpha_bar_prev = extract(self.alphas_cumprod_prev, t, x.shape)\n        sigma = eta * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar)) * torch.sqrt(1 - alpha_bar / alpha_bar_prev)\n\n        noise = torch.randn_like(x)\n        mean_pred = out[\"pred_xstart\"] * torch.sqrt(alpha_bar_prev) + torch.sqrt(1 - alpha_bar_prev - sigma**2) * eps\n        nonzero_mask = (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))  # no noise when t == 0\n        return mean_pred + nonzero_mask * sigma * noise\n\n    @torch.no_grad()\n    def gaussian_ddim_sample(self, noise, T, out_dict, eta=0.0, model_kwargs=None, cond_fn=None):\n        # ruff: noqa: D102, N803\n        x = noise\n        b = x.shape[0]\n        device = x.device\n        for t in reversed(range(T)):\n            print(f\"Sample timestep {t:4d}\", end=\"\\r\")\n            t_array = (torch.ones(b, device=device) * t).long()\n            out_num = self._denoise_fn(x, t_array, **out_dict)\n            x = self.gaussian_ddim_step(out_num, x, t_array, model_kwargs=model_kwargs, cond_fn=cond_fn)\n        print()\n        return x\n\n    @torch.no_grad()\n    def gaussian_ddim_reverse_step(\n        self,\n        model_out_num: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        eta: float = 0.0,\n    ) -&gt; Tensor:\n        # ruff: noqa: D102\n        assert eta == 0.0, \"Eta must be zero.\"\n        out = self.gaussian_p_mean_variance(\n            model_out_num,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=None,\n            model_kwargs=None,\n        )\n\n        eps = (extract(self.sqrt_recip_alphas_cumprod, t, x.shape) * x - out[\"pred_xstart\"]) / extract(\n            self.sqrt_recipm1_alphas_cumprod, t, x.shape\n        )\n        alpha_bar_next = extract(self.alphas_cumprod_next, t, x.shape)\n\n        return out[\"pred_xstart\"] * torch.sqrt(alpha_bar_next) + torch.sqrt(1 - alpha_bar_next) * eps\n\n    @torch.no_grad()\n    def gaussian_ddim_reverse_sample(\n        self,\n        x,\n        T,\n        # ruff: noqa: N803\n        out_dict,\n    ):\n        # ruff: noqa: D102\n        b = x.shape[0]\n        device = x.device\n        for t in range(T):\n            print(f\"Reverse timestep {t:4d}\", end=\"\\r\")\n            t_array = (torch.ones(b, device=device) * t).long()\n            out_num = self._denoise_fn(x, t_array, **out_dict)\n            x = self.gaussian_ddim_reverse_step(out_num, x, t_array, eta=0.0)\n        print()\n\n        return x\n\n    @torch.no_grad()\n    def multinomial_ddim_step(\n        self,\n        model_out_cat: Tensor,\n        log_x_t: Tensor,\n        t: Tensor,\n        out_dict: dict[str, Tensor],\n        eta: float = 0.0,\n    ) -&gt; Tensor:\n        # ruff: noqa: D102\n        # not ddim, essentially\n        log_x0 = self.predict_start(model_out_cat, log_x_t=log_x_t, t=t, out_dict=out_dict)\n\n        alpha_bar = extract(self.alphas_cumprod, t, log_x_t.shape)\n        alpha_bar_prev = extract(self.alphas_cumprod_prev, t, log_x_t.shape)\n        sigma = eta * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar)) * torch.sqrt(1 - alpha_bar / alpha_bar_prev)\n\n        coef1 = sigma\n        coef2 = alpha_bar_prev - sigma * alpha_bar\n        coef3 = 1 - coef1 - coef2\n\n        log_ps = torch.stack(\n            [\n                torch.log(coef1) + log_x_t,\n                torch.log(coef2) + log_x0,\n                torch.log(coef3) - torch.log(self.num_classes_expanded),\n            ],\n            dim=2,\n        )\n\n        log_prob = torch.logsumexp(log_ps, dim=2)\n\n        return self.log_sample_categorical(log_prob)\n\n    @torch.no_grad()\n    def sample_ddim(\n        self,\n        num_samples: int,\n        y_dist: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; tuple[Tensor, dict[str, Tensor]]:\n        # ruff: noqa: D102\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        b = num_samples\n        z_norm = torch.randn((b, self.num_numerical_features), device=self.device)\n\n        assert self.num_classes is not None\n        has_cat = self.num_classes[0] != 0\n        log_z = torch.zeros((b, 0), device=self.device).float()\n        if has_cat:\n            uniform_logits = torch.zeros((b, len(self.num_classes_expanded)), device=self.device)\n            log_z = self.log_sample_categorical(uniform_logits)\n\n        y = torch.multinomial(y_dist, num_samples=b, replacement=True)\n        out_dict = {\"y\": y.long().to(self.device)}\n        for i in reversed(range(0, self.num_timesteps)):\n            print(f\"Sample timestep {i:4d}\", end=\"\\r\")\n            t = torch.full((b,), i, device=self.device, dtype=torch.long)\n            model_out = self._denoise_fn(torch.cat([z_norm, log_z], dim=1).float(), t, **out_dict)\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n            z_norm = self.gaussian_ddim_step(\n                model_out_num,\n                z_norm,\n                t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n                cond_fn=cond_fn,\n            )\n            if has_cat:\n                log_z = self.multinomial_ddim_step(model_out_cat, log_z, t, out_dict)\n\n        print()\n        z_ohe = torch.exp(log_z).round()\n        z_cat = log_z\n        if has_cat:\n            z_cat = ohe_to_categories(z_ohe, torch.from_numpy(self.num_classes))\n        sample = torch.cat([z_norm, z_cat], dim=1).cpu()\n        return sample, out_dict\n\n    @torch.no_grad()\n    def conditional_sample(\n        self,\n        ys: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; tuple[Tensor, dict[str, Tensor]]:\n        # ruff: noqa: D102\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        b = len(ys)\n        z_norm = torch.randn((b, self.num_numerical_features), device=self.device)\n        assert self.num_classes is not None\n        has_cat = self.num_classes[0] != 0\n        log_z = torch.zeros((b, 0), device=self.device).float()\n\n        out_dict = {\"y\": ys.long().to(self.device)}\n        for i in reversed(range(0, self.num_timesteps)):\n            print(f\"Sample timestep {i:4d}\", end=\"\\r\")\n            t = torch.full((b,), i, device=self.device, dtype=torch.long)\n            model_out = self._denoise_fn(torch.cat([z_norm, log_z], dim=1).float(), t, **out_dict)\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n            z_norm = self.gaussian_p_sample(\n                model_out_num,\n                z_norm,\n                t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n                cond_fn=cond_fn,\n            )[\"sample\"]\n            if has_cat:\n                log_z = self.p_sample(model_out_cat, log_z, t, out_dict)\n\n        print()\n        z_ohe = torch.exp(log_z).round()\n        z_cat = log_z\n        if has_cat:\n            z_cat = ohe_to_categories(z_ohe, torch.from_numpy(self.num_classes))\n        sample = torch.cat([z_norm, z_cat], dim=1).cpu()\n        return sample, out_dict\n\n    @torch.no_grad()\n    def sample(\n        self,\n        num_samples: int,\n        y_dist: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; tuple[Tensor, dict[str, Tensor]]:\n        # ruff: noqa: D102\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        b = num_samples\n        z_norm = torch.randn((b, self.num_numerical_features), device=self.device)\n\n        assert self.num_classes is not None\n        has_cat = self.num_classes[0] != 0\n        log_z = torch.zeros((b, 0), device=self.device).float()\n        if has_cat:\n            uniform_logits = torch.zeros((b, len(self.num_classes_expanded)), device=self.device)\n            log_z = self.log_sample_categorical(uniform_logits)\n\n        y = torch.multinomial(y_dist, num_samples=b, replacement=True)\n        out_dict = {\"y\": y.long().to(self.device)}\n        for i in reversed(range(0, self.num_timesteps)):\n            print(f\"Sample timestep {i:4d}\", end=\"\\r\")\n            t = torch.full((b,), i, device=self.device, dtype=torch.long)\n            model_out = self._denoise_fn(torch.cat([z_norm, log_z], dim=1).float(), t, **out_dict)\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n            z_norm = self.gaussian_p_sample(\n                model_out_num,\n                z_norm,\n                t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n                cond_fn=cond_fn,\n            )[\"sample\"]\n            if has_cat:\n                log_z = self.p_sample(model_out_cat, log_z, t, out_dict)\n\n        print()\n        z_ohe = torch.exp(log_z).round()\n        z_cat = log_z\n        if has_cat:\n            z_cat = ohe_to_categories(z_ohe, torch.from_numpy(self.num_classes))\n        sample = torch.cat([z_norm, z_cat], dim=1).cpu()\n        return sample, out_dict\n\n    def sample_all(\n        self,\n        num_samples,\n        batch_size,\n        y_dist,\n        ddim=False,\n        model_kwargs=None,\n        cond_fn=None,\n    ):\n        # ruff: noqa: D102\n        if ddim:\n            print(\"Sample using DDIM.\")\n            sample_fn = self.sample_ddim\n        else:\n            sample_fn = self.sample\n\n        b = batch_size\n\n        all_y = []\n        all_samples = []\n        num_generated = 0\n        while num_generated &lt; num_samples:\n            sample, out_dict = sample_fn(b, y_dist, model_kwargs=model_kwargs, cond_fn=cond_fn)\n            mask_nan = torch.any(sample.isnan(), dim=1)\n            sample = sample[~mask_nan]\n            out_dict[\"y\"] = out_dict[\"y\"][~mask_nan]\n\n            all_samples.append(sample)\n            all_y.append(out_dict[\"y\"].cpu())\n            if sample.shape[0] != b:\n                raise FoundNANsError\n            num_generated += sample.shape[0]\n\n        x_gen = torch.cat(all_samples, dim=0)[:num_samples]\n        y_gen = torch.cat(all_y, dim=0)[:num_samples]\n\n        return x_gen, y_gen\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.GaussianMultinomialDiffusion.condition_mean","title":"condition_mean","text":"<pre><code>condition_mean(\n    cond_fn, p_mean_var, x, t, model_kwargs=None\n)\n</code></pre> <p>Compute the mean for the previous step, given a function cond_fn that computes the gradient of a conditional log probability with respect to x. In particular, cond_fn computes grad(log(p(y|x))), and we want to condition on y.</p> <p>This uses the conditioning strategy from Sohl-Dickstein et al. (2015).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def condition_mean(\n    self,\n    cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n    p_mean_var: dict[str, Tensor],\n    x: Tensor,\n    t: Tensor,\n    model_kwargs: dict[str, Any] | None = None,\n) -&gt; Tensor:\n    \"\"\"\n    Compute the mean for the previous step, given a function cond_fn that\n    computes the gradient of a conditional log probability with respect to\n    x. In particular, cond_fn computes grad(log(p(y|x))), and we want to\n    condition on y.\n\n    This uses the conditioning strategy from Sohl-Dickstein et al. (2015).\n    \"\"\"\n    if model_kwargs is None:\n        model_kwargs = {}\n\n    gradient = cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n    return p_mean_var[\"mean\"].float() + p_mean_var[\"variance\"] * gradient.float()\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.GaussianMultinomialDiffusion.condition_score","title":"condition_score","text":"<pre><code>condition_score(\n    cond_fn, p_mean_var, x, t, model_kwargs=None\n)\n</code></pre> <p>Compute what the p_mean_variance output would have been, should the model's score function be conditioned by cond_fn.</p> <p>See condition_mean() for details on cond_fn.</p> <p>Unlike condition_mean(), this instead uses the conditioning strategy from Song et al (2020).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def condition_score(\n    self,\n    cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n    p_mean_var: dict[str, Tensor],\n    x: Tensor,\n    t: Tensor,\n    model_kwargs: dict[str, Any] | None = None,\n) -&gt; dict[str, Tensor]:\n    \"\"\"\n    Compute what the p_mean_variance output would have been, should the\n    model's score function be conditioned by cond_fn.\n\n    See condition_mean() for details on cond_fn.\n\n    Unlike condition_mean(), this instead uses the conditioning strategy\n    from Song et al (2020).\n    \"\"\"\n    if model_kwargs is None:\n        model_kwargs = {}\n\n    alpha_bar = extract(self.alphas_cumprod, t, x.shape)\n\n    eps = self._predict_eps_from_xstart(x, t, p_mean_var[\"pred_xstart\"])\n    eps = eps - (1 - alpha_bar).sqrt() * cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n\n    out = p_mean_var.copy()\n    out[\"pred_xstart\"] = self._predict_xstart_from_eps(x, t, eps)\n    out[\"mean\"], _, _ = self.gaussian_q_posterior_mean_variance(x_start=out[\"pred_xstart\"], x_t=x, t=t)\n    return out\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.get_named_beta_schedule","title":"get_named_beta_schedule","text":"<pre><code>get_named_beta_schedule(\n    schedule_name, num_diffusion_timesteps\n)\n</code></pre> <p>Get a pre-defined beta schedule for the given name. The beta schedule library consists of beta schedules which remain similar in the limit of num_diffusion_timesteps. Beta schedules may be added, but should not be removed or changed once they are committed to maintain backwards compatibility.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def get_named_beta_schedule(schedule_name: str, num_diffusion_timesteps: int) -&gt; np.ndarray:\n    \"\"\"\n    Get a pre-defined beta schedule for the given name.\n    The beta schedule library consists of beta schedules which remain similar\n    in the limit of num_diffusion_timesteps.\n    Beta schedules may be added, but should not be removed or changed once\n    they are committed to maintain backwards compatibility.\n    \"\"\"\n    if schedule_name == \"linear\":\n        # Linear schedule from Ho et al, extended to work for any number of\n        # diffusion steps.\n        scale = 1000 / num_diffusion_timesteps\n        beta_start = scale * 0.0001\n        beta_end = scale * 0.02\n        return np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64)\n    if schedule_name == \"cosine\":\n        return betas_for_alpha_bar(\n            num_diffusion_timesteps,\n            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n        )\n    raise NotImplementedError(f\"unknown beta schedule: {schedule_name}\")\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.betas_for_alpha_bar","title":"betas_for_alpha_bar","text":"<pre><code>betas_for_alpha_bar(\n    num_diffusion_timesteps, alpha_bar, max_beta=0.999\n)\n</code></pre> <p>Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of (1-beta) over time from t = [0,1]. :param num_diffusion_timesteps: the number of betas to produce. :param alpha_bar: a lambda that takes an argument t from 0 to 1 and                   produces the cumulative product of (1-beta) up to that                   part of the diffusion process. :param max_beta: the maximum beta to use; use values lower than 1 to                  prevent singularities.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def betas_for_alpha_bar(num_diffusion_timesteps: int, alpha_bar: Callable, max_beta: float = 0.999) -&gt; np.ndarray:\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function,\n    which defines the cumulative product of (1-beta) over time from t = [0,1].\n    :param num_diffusion_timesteps: the number of betas to produce.\n    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n                      produces the cumulative product of (1-beta) up to that\n                      part of the diffusion process.\n    :param max_beta: the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n    \"\"\"\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return np.array(betas)\n</code></pre>"},{"location":"api/#clavaddpm-model-module","title":"ClavaDDPM Model Module","text":""},{"location":"api/#midst_toolkit.models.clavaddpm.model","title":"midst_toolkit.models.clavaddpm.model","text":""},{"location":"api/#midst_toolkit.models.clavaddpm.model.ScheduleSampler","title":"ScheduleSampler","text":"<p>               Bases: <code>ABC</code></p> <p>A distribution over timesteps in the diffusion process, intended to reduce variance of the objective.</p> <p>By default, samplers perform unbiased importance sampling, in which the objective's mean is unchanged. However, subclasses may override sample() to change how the resampled terms are reweighted, allowing for actual changes in the objective.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class ScheduleSampler(ABC):\n    \"\"\"\n    A distribution over timesteps in the diffusion process, intended to reduce\n    variance of the objective.\n\n    By default, samplers perform unbiased importance sampling, in which the\n    objective's mean is unchanged.\n    However, subclasses may override sample() to change how the resampled\n    terms are reweighted, allowing for actual changes in the objective.\n    \"\"\"\n\n    @abstractmethod\n    def weights(self) -&gt; Tensor:\n        \"\"\"\n        Get a numpy array of weights, one per diffusion step.\n\n        The weights needn't be normalized, but must be positive.\n        \"\"\"\n\n    def sample(self, batch_size: int, device: str) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"\n        Importance-sample timesteps for a batch.\n\n        :param batch_size: the number of timesteps.\n        :param device: the torch device to save to.\n        :return: a tuple (timesteps, weights):\n                 - timesteps: a tensor of timestep indices.\n                 - weights: a tensor of weights to scale the resulting losses.\n        \"\"\"\n        w = self.weights().cpu().numpy()\n        p = w / np.sum(w)\n        indices_np = np.random.choice(len(p), size=(batch_size,), p=p)\n        indices = torch.from_numpy(indices_np).long().to(device)\n        weights_np = 1 / (len(p) * p[indices_np])\n        weights = torch.from_numpy(weights_np).float().to(device)\n        return indices, weights\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ScheduleSampler.weights","title":"weights  <code>abstractmethod</code>","text":"<pre><code>weights()\n</code></pre> <p>Get a numpy array of weights, one per diffusion step.</p> <p>The weights needn't be normalized, but must be positive.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>@abstractmethod\ndef weights(self) -&gt; Tensor:\n    \"\"\"\n    Get a numpy array of weights, one per diffusion step.\n\n    The weights needn't be normalized, but must be positive.\n    \"\"\"\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ScheduleSampler.sample","title":"sample","text":"<pre><code>sample(batch_size, device)\n</code></pre> <p>Importance-sample timesteps for a batch.</p> <p>:param batch_size: the number of timesteps. :param device: the torch device to save to. :return: a tuple (timesteps, weights):          - timesteps: a tensor of timestep indices.          - weights: a tensor of weights to scale the resulting losses.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def sample(self, batch_size: int, device: str) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"\n    Importance-sample timesteps for a batch.\n\n    :param batch_size: the number of timesteps.\n    :param device: the torch device to save to.\n    :return: a tuple (timesteps, weights):\n             - timesteps: a tensor of timestep indices.\n             - weights: a tensor of weights to scale the resulting losses.\n    \"\"\"\n    w = self.weights().cpu().numpy()\n    p = w / np.sum(w)\n    indices_np = np.random.choice(len(p), size=(batch_size,), p=p)\n    indices = torch.from_numpy(indices_np).long().to(device)\n    weights_np = 1 / (len(p) * p[indices_np])\n    weights = torch.from_numpy(weights_np).float().to(device)\n    return indices, weights\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.LossAwareSampler","title":"LossAwareSampler","text":"<p>               Bases: <code>ScheduleSampler</code></p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class LossAwareSampler(ScheduleSampler):\n    def update_with_local_losses(self, local_ts: Tensor, local_losses: Tensor) -&gt; None:\n        \"\"\"\n        Update the reweighting using losses from a model.\n\n        Call this method from each rank with a batch of timesteps and the\n        corresponding losses for each of those timesteps.\n        This method will perform synchronization to make sure all of the ranks\n        maintain the exact same reweighting.\n\n        :param local_ts: an integer Tensor of timesteps.\n        :param local_losses: a 1D Tensor of losses.\n        \"\"\"\n        batch_sizes = [\n            torch.tensor([0], dtype=torch.int32, device=local_ts.device)\n            for _ in range(torch.distributed.get_world_size())\n        ]\n        torch.distributed.all_gather(\n            batch_sizes,\n            torch.tensor([len(local_ts)], dtype=torch.int32, device=local_ts.device),\n        )\n\n        # Pad all_gather batches to be the maximum batch size.\n        max_bs = max([int(x.item()) for x in batch_sizes])\n\n        timestep_batches = [torch.zeros(max_bs).to(local_ts) for bs in batch_sizes]\n        loss_batches = [torch.zeros(max_bs).to(local_losses) for bs in batch_sizes]\n        torch.distributed.all_gather(timestep_batches, local_ts)\n        torch.distributed.all_gather(loss_batches, local_losses)\n        timesteps = [x.item() for y, bs in zip(timestep_batches, batch_sizes) for x in y[:bs]]\n        losses = [x.item() for y, bs in zip(loss_batches, batch_sizes) for x in y[:bs]]\n        self.update_with_all_losses(timesteps, losses)\n\n    @abstractmethod\n    def update_with_all_losses(self, ts: list[int], losses: list[float]) -&gt; None:\n        \"\"\"\n        Update the reweighting using losses from a model.\n\n        Sub-classes should override this method to update the reweighting\n        using losses from the model.\n\n        This method directly updates the reweighting without synchronizing\n        between workers. It is called by update_with_local_losses from all\n        ranks with identical arguments. Thus, it should have deterministic\n        behavior to maintain state across workers.\n\n        :param ts: a list of int timesteps.\n        :param losses: a list of float losses, one per timestep.\n        \"\"\"\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.LossAwareSampler.update_with_local_losses","title":"update_with_local_losses","text":"<pre><code>update_with_local_losses(local_ts, local_losses)\n</code></pre> <p>Update the reweighting using losses from a model.</p> <p>Call this method from each rank with a batch of timesteps and the corresponding losses for each of those timesteps. This method will perform synchronization to make sure all of the ranks maintain the exact same reweighting.</p> <p>:param local_ts: an integer Tensor of timesteps. :param local_losses: a 1D Tensor of losses.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def update_with_local_losses(self, local_ts: Tensor, local_losses: Tensor) -&gt; None:\n    \"\"\"\n    Update the reweighting using losses from a model.\n\n    Call this method from each rank with a batch of timesteps and the\n    corresponding losses for each of those timesteps.\n    This method will perform synchronization to make sure all of the ranks\n    maintain the exact same reweighting.\n\n    :param local_ts: an integer Tensor of timesteps.\n    :param local_losses: a 1D Tensor of losses.\n    \"\"\"\n    batch_sizes = [\n        torch.tensor([0], dtype=torch.int32, device=local_ts.device)\n        for _ in range(torch.distributed.get_world_size())\n    ]\n    torch.distributed.all_gather(\n        batch_sizes,\n        torch.tensor([len(local_ts)], dtype=torch.int32, device=local_ts.device),\n    )\n\n    # Pad all_gather batches to be the maximum batch size.\n    max_bs = max([int(x.item()) for x in batch_sizes])\n\n    timestep_batches = [torch.zeros(max_bs).to(local_ts) for bs in batch_sizes]\n    loss_batches = [torch.zeros(max_bs).to(local_losses) for bs in batch_sizes]\n    torch.distributed.all_gather(timestep_batches, local_ts)\n    torch.distributed.all_gather(loss_batches, local_losses)\n    timesteps = [x.item() for y, bs in zip(timestep_batches, batch_sizes) for x in y[:bs]]\n    losses = [x.item() for y, bs in zip(loss_batches, batch_sizes) for x in y[:bs]]\n    self.update_with_all_losses(timesteps, losses)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.LossAwareSampler.update_with_all_losses","title":"update_with_all_losses  <code>abstractmethod</code>","text":"<pre><code>update_with_all_losses(ts, losses)\n</code></pre> <p>Update the reweighting using losses from a model.</p> <p>Sub-classes should override this method to update the reweighting using losses from the model.</p> <p>This method directly updates the reweighting without synchronizing between workers. It is called by update_with_local_losses from all ranks with identical arguments. Thus, it should have deterministic behavior to maintain state across workers.</p> <p>:param ts: a list of int timesteps. :param losses: a list of float losses, one per timestep.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>@abstractmethod\ndef update_with_all_losses(self, ts: list[int], losses: list[float]) -&gt; None:\n    \"\"\"\n    Update the reweighting using losses from a model.\n\n    Sub-classes should override this method to update the reweighting\n    using losses from the model.\n\n    This method directly updates the reweighting without synchronizing\n    between workers. It is called by update_with_local_losses from all\n    ranks with identical arguments. Thus, it should have deterministic\n    behavior to maintain state across workers.\n\n    :param ts: a list of int timesteps.\n    :param losses: a list of float losses, one per timestep.\n    \"\"\"\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.FastTensorDataLoader","title":"FastTensorDataLoader","text":"<p>Defines a faster dataloader for PyTorch tensors.</p> <p>A DataLoader-like object for a set of tensors that can be much faster than TensorDataset + DataLoader because dataloader grabs individual indices of the dataset and calls cat (slow). Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class FastTensorDataLoader:\n    \"\"\"\n    Defines a faster dataloader for PyTorch tensors.\n\n    A DataLoader-like object for a set of tensors that can be much faster than\n    TensorDataset + DataLoader because dataloader grabs individual indices of\n    the dataset and calls cat (slow).\n    Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6\n    \"\"\"\n\n    def __init__(self, *tensors: Tensor, batch_size: int = 32, shuffle: bool = False):\n        \"\"\"\n        Initialize a FastTensorDataLoader.\n        :param *tensors: tensors to store. Must have the same length @ dim 0.\n        :param batch_size: batch size to load.\n        :param shuffle: if True, shuffle the data *in-place* whenever an\n            iterator is created out of this object.\n        :returns: A FastTensorDataLoader.\n        \"\"\"\n        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n        self.tensors = tensors\n\n        self.dataset_len = self.tensors[0].shape[0]\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n\n        # Calculate # batches\n        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n        if remainder &gt; 0:\n            n_batches += 1\n        self.n_batches = n_batches\n\n    def __iter__(self):\n        # ruff: noqa: D105\n        if self.shuffle:\n            r = torch.randperm(self.dataset_len)\n            self.tensors = [t[r] for t in self.tensors]  # type: ignore[assignment]\n        self.i = 0\n        return self\n\n    def __next__(self):\n        # ruff: noqa: D105\n        if self.i &gt;= self.dataset_len:\n            raise StopIteration\n        batch = tuple(t[self.i : self.i + self.batch_size] for t in self.tensors)\n        self.i += self.batch_size\n        return batch\n\n    def __len__(self):\n        # ruff: noqa: D105\n        return self.n_batches\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.FastTensorDataLoader.__init__","title":"__init__","text":"<pre><code>__init__(*tensors, batch_size=32, shuffle=False)\n</code></pre> <p>Initialize a FastTensorDataLoader. :param tensors: tensors to store. Must have the same length @ dim 0. :param batch_size: batch size to load. :param shuffle: if True, shuffle the data in-place* whenever an     iterator is created out of this object. :returns: A FastTensorDataLoader.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def __init__(self, *tensors: Tensor, batch_size: int = 32, shuffle: bool = False):\n    \"\"\"\n    Initialize a FastTensorDataLoader.\n    :param *tensors: tensors to store. Must have the same length @ dim 0.\n    :param batch_size: batch size to load.\n    :param shuffle: if True, shuffle the data *in-place* whenever an\n        iterator is created out of this object.\n    :returns: A FastTensorDataLoader.\n    \"\"\"\n    assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n    self.tensors = tensors\n\n    self.dataset_len = self.tensors[0].shape[0]\n    self.batch_size = batch_size\n    self.shuffle = shuffle\n\n    # Calculate # batches\n    n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n    if remainder &gt; 0:\n        n_batches += 1\n    self.n_batches = n_batches\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.MLP","title":"MLP","text":"<p>               Bases: <code>Module</code></p> <p>The MLP model used in [gorishniy2021revisiting].</p> <p>The following scheme describes the architecture:</p> <p>.. code-block:: text</p> <pre><code>  MLP: (in) -&gt; Block -&gt; ... -&gt; Block -&gt; Linear -&gt; (out)\nBlock: (in) -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; (out)\n</code></pre> <p>Examples:</p> <p>.. testcode::</p> <pre><code>x = torch.randn(4, 2)\nmodule = MLP.make_baseline(x.shape[1], [3, 5], 0.1, 1)\nassert module(x).shape == (len(x), 1)\n</code></pre> References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class MLP(nn.Module):\n    \"\"\"The MLP model used in [gorishniy2021revisiting].\n\n    The following scheme describes the architecture:\n\n    .. code-block:: text\n\n          MLP: (in) -&gt; Block -&gt; ... -&gt; Block -&gt; Linear -&gt; (out)\n        Block: (in) -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; (out)\n\n    Examples:\n        .. testcode::\n\n            x = torch.randn(4, 2)\n            module = MLP.make_baseline(x.shape[1], [3, 5], 0.1, 1)\n            assert module(x).shape == (len(x), 1)\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n\n    class Block(nn.Module):\n        \"\"\"The main building block of `MLP`.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            d_in: int,\n            d_out: int,\n            bias: bool,\n            activation: ModuleType,\n            dropout: float,\n        ) -&gt; None:\n            super().__init__()\n            self.linear = nn.Linear(d_in, d_out, bias)\n            self.activation = _make_nn_module(activation)\n            self.dropout = nn.Dropout(dropout)\n\n        def forward(self, x: Tensor) -&gt; Tensor:\n            return self.dropout(self.activation(self.linear(x)))\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        d_layers: list[int],\n        dropouts: float | list[float],\n        activation: str | Callable[[], nn.Module],\n        d_out: int,\n    ) -&gt; None:\n        \"\"\"\n        Note:\n            `make_baseline` is the recommended constructor.\n        \"\"\"\n        super().__init__()\n        if isinstance(dropouts, float):\n            dropouts = [dropouts] * len(d_layers)\n        assert len(d_layers) == len(dropouts)\n        assert activation not in [\"ReGLU\", \"GEGLU\"]\n\n        self.blocks = nn.ModuleList(\n            [\n                MLP.Block(\n                    d_in=d_layers[i - 1] if i else d_in,\n                    d_out=d,\n                    bias=True,\n                    activation=activation,\n                    dropout=dropout,\n                )\n                for i, (d, dropout) in enumerate(zip(d_layers, dropouts))\n            ]\n        )\n        self.head = nn.Linear(d_layers[-1] if d_layers else d_in, d_out)\n\n    @classmethod\n    def make_baseline(\n        cls,\n        d_in: int,\n        d_layers: list[int],\n        dropout: float,\n        d_out: int,\n    ) -&gt; Self:\n        \"\"\"Create a \"baseline\" `MLP`.\n\n        This variation of MLP was used in [gorishniy2021revisiting]. Features:\n\n        * all linear layers except for the first one and the last one are of the same dimension\n        * the dropout rate is the same for all dropout layers\n\n        Args:\n            d_in: the input size\n            d_layers: the dimensions of the linear layers. If there are more than two\n                layers, then all of them except for the first and the last ones must\n                have the same dimension.\n            dropout: the dropout rate for all hidden layers\n            d_out: the output size\n        Returns:\n            MLP\n\n        References:\n            * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n            Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n        \"\"\"\n        assert isinstance(dropout, float)\n        if len(d_layers) &gt; 2:\n            assert len(set(d_layers[1:-1])) == 1, (\n                \"if d_layers contains more than two elements, then\"\n                \" all elements except for the first and the last ones must be equal.\"\n            )\n        return cls(\n            d_in=d_in,\n            d_layers=d_layers,\n            dropouts=dropout,\n            activation=\"ReLU\",\n            d_out=d_out,\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = x.float()\n        for block in self.blocks:\n            x = block(x)\n        return self.head(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.MLP.Block","title":"Block","text":"<p>               Bases: <code>Module</code></p> <p>The main building block of <code>MLP</code>.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class Block(nn.Module):\n    \"\"\"The main building block of `MLP`.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        d_out: int,\n        bias: bool,\n        activation: ModuleType,\n        dropout: float,\n    ) -&gt; None:\n        super().__init__()\n        self.linear = nn.Linear(d_in, d_out, bias)\n        self.activation = _make_nn_module(activation)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        return self.dropout(self.activation(self.linear(x)))\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.MLP.__init__","title":"__init__","text":"<pre><code>__init__(*, d_in, d_layers, dropouts, activation, d_out)\n</code></pre> Note <p><code>make_baseline</code> is the recommended constructor.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def __init__(\n    self,\n    *,\n    d_in: int,\n    d_layers: list[int],\n    dropouts: float | list[float],\n    activation: str | Callable[[], nn.Module],\n    d_out: int,\n) -&gt; None:\n    \"\"\"\n    Note:\n        `make_baseline` is the recommended constructor.\n    \"\"\"\n    super().__init__()\n    if isinstance(dropouts, float):\n        dropouts = [dropouts] * len(d_layers)\n    assert len(d_layers) == len(dropouts)\n    assert activation not in [\"ReGLU\", \"GEGLU\"]\n\n    self.blocks = nn.ModuleList(\n        [\n            MLP.Block(\n                d_in=d_layers[i - 1] if i else d_in,\n                d_out=d,\n                bias=True,\n                activation=activation,\n                dropout=dropout,\n            )\n            for i, (d, dropout) in enumerate(zip(d_layers, dropouts))\n        ]\n    )\n    self.head = nn.Linear(d_layers[-1] if d_layers else d_in, d_out)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.MLP.make_baseline","title":"make_baseline  <code>classmethod</code>","text":"<pre><code>make_baseline(d_in, d_layers, dropout, d_out)\n</code></pre> <p>Create a \"baseline\" <code>MLP</code>.</p> <p>This variation of MLP was used in [gorishniy2021revisiting]. Features:</p> <ul> <li>all linear layers except for the first one and the last one are of the same dimension</li> <li>the dropout rate is the same for all dropout layers</li> </ul> <p>Parameters:</p> Name Type Description Default <code>d_in</code> <code>int</code> <p>the input size</p> required <code>d_layers</code> <code>list[int]</code> <p>the dimensions of the linear layers. If there are more than two layers, then all of them except for the first and the last ones must have the same dimension.</p> required <code>dropout</code> <code>float</code> <p>the dropout rate for all hidden layers</p> required <code>d_out</code> <code>int</code> <p>the output size</p> required <p>Returns:     MLP</p> References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>@classmethod\ndef make_baseline(\n    cls,\n    d_in: int,\n    d_layers: list[int],\n    dropout: float,\n    d_out: int,\n) -&gt; Self:\n    \"\"\"Create a \"baseline\" `MLP`.\n\n    This variation of MLP was used in [gorishniy2021revisiting]. Features:\n\n    * all linear layers except for the first one and the last one are of the same dimension\n    * the dropout rate is the same for all dropout layers\n\n    Args:\n        d_in: the input size\n        d_layers: the dimensions of the linear layers. If there are more than two\n            layers, then all of them except for the first and the last ones must\n            have the same dimension.\n        dropout: the dropout rate for all hidden layers\n        d_out: the output size\n    Returns:\n        MLP\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n    assert isinstance(dropout, float)\n    if len(d_layers) &gt; 2:\n        assert len(set(d_layers[1:-1])) == 1, (\n            \"if d_layers contains more than two elements, then\"\n            \" all elements except for the first and the last ones must be equal.\"\n        )\n    return cls(\n        d_in=d_in,\n        d_layers=d_layers,\n        dropouts=dropout,\n        activation=\"ReLU\",\n        d_out=d_out,\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ResNet","title":"ResNet","text":"<p>               Bases: <code>Module</code></p> <p>The ResNet model used in [gorishniy2021revisiting].</p> <p>The following scheme describes the architecture: .. code-block:: text     ResNet: (in) -&gt; Linear -&gt; Block -&gt; ... -&gt; Block -&gt; Head -&gt; (out)              |-&gt; Norm -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; Linear -&gt; Dropout -&gt;|              |                                                                  |      Block: (in) ------------------------------------------------------------&gt; Add -&gt; (out)       Head: (in) -&gt; Norm -&gt; Activation -&gt; Linear -&gt; (out)</p> <p>Examples:</p> <p>.. testcode::     x = torch.randn(4, 2)     module = ResNet.make_baseline(         d_in=x.shape[1],         n_blocks=2,         d_main=3,         d_hidden=4,         dropout_first=0.25,         dropout_second=0.0,         d_out=1     )     assert module(x).shape == (len(x), 1)</p> References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class ResNet(nn.Module):\n    \"\"\"\n    The ResNet model used in [gorishniy2021revisiting].\n\n    The following scheme describes the architecture:\n    .. code-block:: text\n        ResNet: (in) -&gt; Linear -&gt; Block -&gt; ... -&gt; Block -&gt; Head -&gt; (out)\n                 |-&gt; Norm -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; Linear -&gt; Dropout -&gt;|\n                 |                                                                  |\n         Block: (in) ------------------------------------------------------------&gt; Add -&gt; (out)\n          Head: (in) -&gt; Norm -&gt; Activation -&gt; Linear -&gt; (out)\n\n    Examples:\n        .. testcode::\n            x = torch.randn(4, 2)\n            module = ResNet.make_baseline(\n                d_in=x.shape[1],\n                n_blocks=2,\n                d_main=3,\n                d_hidden=4,\n                dropout_first=0.25,\n                dropout_second=0.0,\n                d_out=1\n            )\n            assert module(x).shape == (len(x), 1)\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n\n    class Block(nn.Module):\n        \"\"\"The main building block of `ResNet`.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            d_main: int,\n            d_hidden: int,\n            bias_first: bool,\n            bias_second: bool,\n            dropout_first: float,\n            dropout_second: float,\n            normalization: ModuleType,\n            activation: ModuleType,\n            skip_connection: bool,\n        ) -&gt; None:\n            super().__init__()\n            self.normalization = _make_nn_module(normalization, d_main)\n            self.linear_first = nn.Linear(d_main, d_hidden, bias_first)\n            self.activation = _make_nn_module(activation)\n            self.dropout_first = nn.Dropout(dropout_first)\n            self.linear_second = nn.Linear(d_hidden, d_main, bias_second)\n            self.dropout_second = nn.Dropout(dropout_second)\n            self.skip_connection = skip_connection\n\n        def forward(self, x: Tensor) -&gt; Tensor:\n            x_input = x\n            x = self.normalization(x)\n            x = self.linear_first(x)\n            x = self.activation(x)\n            x = self.dropout_first(x)\n            x = self.linear_second(x)\n            x = self.dropout_second(x)\n            if self.skip_connection:\n                x = x_input + x\n            return x\n\n    class Head(nn.Module):\n        \"\"\"The final module of `ResNet`.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            d_in: int,\n            d_out: int,\n            bias: bool,\n            normalization: ModuleType,\n            activation: ModuleType,\n        ) -&gt; None:\n            super().__init__()\n            self.normalization = _make_nn_module(normalization, d_in)\n            self.activation = _make_nn_module(activation)\n            self.linear = nn.Linear(d_in, d_out, bias)\n\n        def forward(self, x: Tensor) -&gt; Tensor:\n            if self.normalization is not None:\n                x = self.normalization(x)\n            x = self.activation(x)\n            return self.linear(x)\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        n_blocks: int,\n        d_main: int,\n        d_hidden: int,\n        dropout_first: float,\n        dropout_second: float,\n        normalization: ModuleType,\n        activation: ModuleType,\n        d_out: int,\n    ) -&gt; None:\n        \"\"\"\n        Note:\n            `make_baseline` is the recommended constructor.\n        \"\"\"\n        super().__init__()\n\n        self.first_layer = nn.Linear(d_in, d_main)\n        if d_main is None:\n            d_main = d_in\n        self.blocks = nn.Sequential(\n            *[\n                ResNet.Block(\n                    d_main=d_main,\n                    d_hidden=d_hidden,\n                    bias_first=True,\n                    bias_second=True,\n                    dropout_first=dropout_first,\n                    dropout_second=dropout_second,\n                    normalization=normalization,\n                    activation=activation,\n                    skip_connection=True,\n                )\n                for _ in range(n_blocks)\n            ]\n        )\n        self.head = ResNet.Head(\n            d_in=d_main,\n            d_out=d_out,\n            bias=True,\n            normalization=normalization,\n            activation=activation,\n        )\n\n    @classmethod\n    def make_baseline(\n        cls,\n        *,\n        d_in: int,\n        n_blocks: int,\n        d_main: int,\n        d_hidden: int,\n        dropout_first: float,\n        dropout_second: float,\n        d_out: int,\n    ) -&gt; Self:\n        \"\"\"\n        Create a \"baseline\" `ResNet`. This variation of ResNet was used in [gorishniy2021revisiting].\n\n        Args:\n            d_in: the input size\n            n_blocks: the number of Blocks\n            d_main: the input size (or, equivalently, the output size) of each Block\n            d_hidden: the output size of the first linear layer in each Block\n            dropout_first: the dropout rate of the first dropout layer in each Block.\n            dropout_second: the dropout rate of the second dropout layer in each Block.\n            d_out: Output dimension.\n\n        References:\n            * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n            Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n        \"\"\"\n        return cls(\n            d_in=d_in,\n            n_blocks=n_blocks,\n            d_main=d_main,\n            d_hidden=d_hidden,\n            dropout_first=dropout_first,\n            dropout_second=dropout_second,\n            normalization=\"BatchNorm1d\",\n            activation=\"ReLU\",\n            d_out=d_out,\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = x.float()\n        x = self.first_layer(x)\n        x = self.blocks(x)\n        return self.head(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ResNet.Block","title":"Block","text":"<p>               Bases: <code>Module</code></p> <p>The main building block of <code>ResNet</code>.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class Block(nn.Module):\n    \"\"\"The main building block of `ResNet`.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        d_main: int,\n        d_hidden: int,\n        bias_first: bool,\n        bias_second: bool,\n        dropout_first: float,\n        dropout_second: float,\n        normalization: ModuleType,\n        activation: ModuleType,\n        skip_connection: bool,\n    ) -&gt; None:\n        super().__init__()\n        self.normalization = _make_nn_module(normalization, d_main)\n        self.linear_first = nn.Linear(d_main, d_hidden, bias_first)\n        self.activation = _make_nn_module(activation)\n        self.dropout_first = nn.Dropout(dropout_first)\n        self.linear_second = nn.Linear(d_hidden, d_main, bias_second)\n        self.dropout_second = nn.Dropout(dropout_second)\n        self.skip_connection = skip_connection\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x_input = x\n        x = self.normalization(x)\n        x = self.linear_first(x)\n        x = self.activation(x)\n        x = self.dropout_first(x)\n        x = self.linear_second(x)\n        x = self.dropout_second(x)\n        if self.skip_connection:\n            x = x_input + x\n        return x\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ResNet.Head","title":"Head","text":"<p>               Bases: <code>Module</code></p> <p>The final module of <code>ResNet</code>.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class Head(nn.Module):\n    \"\"\"The final module of `ResNet`.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        d_out: int,\n        bias: bool,\n        normalization: ModuleType,\n        activation: ModuleType,\n    ) -&gt; None:\n        super().__init__()\n        self.normalization = _make_nn_module(normalization, d_in)\n        self.activation = _make_nn_module(activation)\n        self.linear = nn.Linear(d_in, d_out, bias)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        if self.normalization is not None:\n            x = self.normalization(x)\n        x = self.activation(x)\n        return self.linear(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ResNet.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    d_in,\n    n_blocks,\n    d_main,\n    d_hidden,\n    dropout_first,\n    dropout_second,\n    normalization,\n    activation,\n    d_out,\n)\n</code></pre> Note <p><code>make_baseline</code> is the recommended constructor.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def __init__(\n    self,\n    *,\n    d_in: int,\n    n_blocks: int,\n    d_main: int,\n    d_hidden: int,\n    dropout_first: float,\n    dropout_second: float,\n    normalization: ModuleType,\n    activation: ModuleType,\n    d_out: int,\n) -&gt; None:\n    \"\"\"\n    Note:\n        `make_baseline` is the recommended constructor.\n    \"\"\"\n    super().__init__()\n\n    self.first_layer = nn.Linear(d_in, d_main)\n    if d_main is None:\n        d_main = d_in\n    self.blocks = nn.Sequential(\n        *[\n            ResNet.Block(\n                d_main=d_main,\n                d_hidden=d_hidden,\n                bias_first=True,\n                bias_second=True,\n                dropout_first=dropout_first,\n                dropout_second=dropout_second,\n                normalization=normalization,\n                activation=activation,\n                skip_connection=True,\n            )\n            for _ in range(n_blocks)\n        ]\n    )\n    self.head = ResNet.Head(\n        d_in=d_main,\n        d_out=d_out,\n        bias=True,\n        normalization=normalization,\n        activation=activation,\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ResNet.make_baseline","title":"make_baseline  <code>classmethod</code>","text":"<pre><code>make_baseline(\n    *,\n    d_in,\n    n_blocks,\n    d_main,\n    d_hidden,\n    dropout_first,\n    dropout_second,\n    d_out,\n)\n</code></pre> <p>Create a \"baseline\" <code>ResNet</code>. This variation of ResNet was used in [gorishniy2021revisiting].</p> <p>Parameters:</p> Name Type Description Default <code>d_in</code> <code>int</code> <p>the input size</p> required <code>n_blocks</code> <code>int</code> <p>the number of Blocks</p> required <code>d_main</code> <code>int</code> <p>the input size (or, equivalently, the output size) of each Block</p> required <code>d_hidden</code> <code>int</code> <p>the output size of the first linear layer in each Block</p> required <code>dropout_first</code> <code>float</code> <p>the dropout rate of the first dropout layer in each Block.</p> required <code>dropout_second</code> <code>float</code> <p>the dropout rate of the second dropout layer in each Block.</p> required <code>d_out</code> <code>int</code> <p>Output dimension.</p> required References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>@classmethod\ndef make_baseline(\n    cls,\n    *,\n    d_in: int,\n    n_blocks: int,\n    d_main: int,\n    d_hidden: int,\n    dropout_first: float,\n    dropout_second: float,\n    d_out: int,\n) -&gt; Self:\n    \"\"\"\n    Create a \"baseline\" `ResNet`. This variation of ResNet was used in [gorishniy2021revisiting].\n\n    Args:\n        d_in: the input size\n        n_blocks: the number of Blocks\n        d_main: the input size (or, equivalently, the output size) of each Block\n        d_hidden: the output size of the first linear layer in each Block\n        dropout_first: the dropout rate of the first dropout layer in each Block.\n        dropout_second: the dropout rate of the second dropout layer in each Block.\n        d_out: Output dimension.\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n    return cls(\n        d_in=d_in,\n        n_blocks=n_blocks,\n        d_main=d_main,\n        d_hidden=d_hidden,\n        dropout_first=dropout_first,\n        dropout_second=dropout_second,\n        normalization=\"BatchNorm1d\",\n        activation=\"ReLU\",\n        d_out=d_out,\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ReGLU","title":"ReGLU","text":"<p>               Bases: <code>Module</code></p> <p>The ReGLU activation function from [shazeer2020glu].</p> <p>Examples:</p> <p>.. testcode::</p> <pre><code>module = ReGLU()\nx = torch.randn(3, 4)\nassert module(x).shape == (3, 2)\n</code></pre> References <ul> <li>[shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class ReGLU(nn.Module):\n    \"\"\"The ReGLU activation function from [shazeer2020glu].\n\n    Examples:\n        .. testcode::\n\n            module = ReGLU()\n            x = torch.randn(3, 4)\n            assert module(x).shape == (3, 2)\n\n    References:\n        * [shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        # ruff: noqa: D102\n        return reglu(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.GEGLU","title":"GEGLU","text":"<p>               Bases: <code>Module</code></p> <p>The GEGLU activation function from [shazeer2020glu].</p> <p>Examples:</p> <p>.. testcode::</p> <pre><code>module = GEGLU()\nx = torch.randn(3, 4)\nassert module(x).shape == (3, 2)\n</code></pre> References <ul> <li>[shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class GEGLU(nn.Module):\n    \"\"\"The GEGLU activation function from [shazeer2020glu].\n\n    Examples:\n        .. testcode::\n\n            module = GEGLU()\n            x = torch.randn(3, 4)\n            assert module(x).shape == (3, 2)\n\n    References:\n        * [shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        # ruff: noqa: D102\n        return geglu(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.make_dataset_from_df","title":"make_dataset_from_df","text":"<pre><code>make_dataset_from_df(\n    df, T, is_y_cond, df_info, ratios=None, std=0\n)\n</code></pre> <p>The order of the generated dataset: (y, X_num, X_cat).</p> is_y_cond <p>concat: y is concatenated to X, the model learn a joint distribution of (y, X) embedding: y is not concatenated to X. During computations, y is embedded     and added to the latent vector of X none: y column is completely ignored</p> <p>How does is_y_cond affect the generation of y? is_y_cond:     concat: the model synthesizes (y, X) directly, so y is just the first column     embedding: y is first sampled using empirical distribution of y. The model only         synthesizes X. When returning the generated data, we return the generated X         and the sampled y. (y is sampled from empirical distribution, instead of being         generated by the model)         Note that in this way, y is still not independent of X, because the model has been         adding the embedding of y to the latent vector of X during computations.     none:         y is synthesized using y's empirical distribution. X is generated by the model.         In this case, y is completely independent of X.</p> <p>Note: For now, n_classes has to be set to 0. This is because our matrix is the concatenation of (X_num, X_cat). In this case, if we have is_y_cond == 'concat', we can guarantee that y is the first column of the matrix. However, if we have n_classes &gt; 0, then y is not the first column of the matrix.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def make_dataset_from_df(\n    # ruff: noqa: PLR0915, PLR0912\n    df: pd.DataFrame,\n    T: Transformations,\n    is_y_cond: str,\n    df_info: pd.DataFrame,\n    ratios: list[float] | None = None,\n    std: float = 0,\n) -&gt; tuple[Dataset, dict[int, LabelEncoder], list[int]]:\n    \"\"\"\n    The order of the generated dataset: (y, X_num, X_cat).\n\n    is_y_cond:\n        concat: y is concatenated to X, the model learn a joint distribution of (y, X)\n        embedding: y is not concatenated to X. During computations, y is embedded\n            and added to the latent vector of X\n        none: y column is completely ignored\n\n    How does is_y_cond affect the generation of y?\n    is_y_cond:\n        concat: the model synthesizes (y, X) directly, so y is just the first column\n        embedding: y is first sampled using empirical distribution of y. The model only\n            synthesizes X. When returning the generated data, we return the generated X\n            and the sampled y. (y is sampled from empirical distribution, instead of being\n            generated by the model)\n            Note that in this way, y is still not independent of X, because the model has been\n            adding the embedding of y to the latent vector of X during computations.\n        none:\n            y is synthesized using y's empirical distribution. X is generated by the model.\n            In this case, y is completely independent of X.\n\n    Note: For now, n_classes has to be set to 0. This is because our matrix is the concatenation\n    of (X_num, X_cat). In this case, if we have is_y_cond == 'concat', we can guarantee that y\n    is the first column of the matrix.\n    However, if we have n_classes &gt; 0, then y is not the first column of the matrix.\n    \"\"\"\n    if ratios is None:\n        ratios = [0.7, 0.2, 0.1]\n\n    train_val_df, test_df = train_test_split(df, test_size=ratios[2], random_state=42)\n    train_df, val_df = train_test_split(train_val_df, test_size=ratios[1] / (ratios[0] + ratios[1]), random_state=42)\n\n    cat_column_orders = []\n    num_column_orders = []\n    index_to_column = list(df.columns)\n    column_to_index = {col: i for i, col in enumerate(index_to_column)}\n\n    if df_info[\"n_classes\"] &gt; 0:\n        X_cat: dict[str, np.ndarray] | None = {} if df_info[\"cat_cols\"] is not None or is_y_cond == \"concat\" else None\n        X_num: dict[str, np.ndarray] | None = {} if df_info[\"num_cols\"] is not None else None\n        y = {}\n\n        cat_cols_with_y = []\n        if df_info[\"cat_cols\"] is not None:\n            cat_cols_with_y += df_info[\"cat_cols\"]\n        if is_y_cond == \"concat\":\n            cat_cols_with_y = [df_info[\"y_col\"]] + cat_cols_with_y\n\n        if len(cat_cols_with_y) &gt; 0:\n            X_cat[\"train\"] = train_df[cat_cols_with_y].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"val\"] = val_df[cat_cols_with_y].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"test\"] = test_df[cat_cols_with_y].to_numpy(dtype=np.str_)  # type: ignore[index]\n\n        y[\"train\"] = train_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"val\"] = val_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"test\"] = test_df[df_info[\"y_col\"]].values.astype(np.float32)\n\n        if df_info[\"num_cols\"] is not None:\n            X_num[\"train\"] = train_df[df_info[\"num_cols\"]].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"val\"] = val_df[df_info[\"num_cols\"]].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"test\"] = test_df[df_info[\"num_cols\"]].values.astype(np.float32)  # type: ignore[index]\n\n        cat_column_orders = [column_to_index[col] for col in cat_cols_with_y]\n        num_column_orders = [column_to_index[col] for col in df_info[\"num_cols\"]]\n\n    else:\n        X_cat = {} if df_info[\"cat_cols\"] is not None else None\n        X_num = {} if df_info[\"num_cols\"] is not None or is_y_cond == \"concat\" else None\n        y = {}\n\n        num_cols_with_y = []\n        if df_info[\"num_cols\"] is not None:\n            num_cols_with_y += df_info[\"num_cols\"]\n        if is_y_cond == \"concat\":\n            num_cols_with_y = [df_info[\"y_col\"]] + num_cols_with_y\n\n        if len(num_cols_with_y) &gt; 0:\n            X_num[\"train\"] = train_df[num_cols_with_y].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"val\"] = val_df[num_cols_with_y].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"test\"] = test_df[num_cols_with_y].values.astype(np.float32)  # type: ignore[index]\n\n        y[\"train\"] = train_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"val\"] = val_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"test\"] = test_df[df_info[\"y_col\"]].values.astype(np.float32)\n\n        if df_info[\"cat_cols\"] is not None:\n            X_cat[\"train\"] = train_df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"val\"] = val_df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"test\"] = test_df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)  # type: ignore[index]\n\n        cat_column_orders = [column_to_index[col] for col in df_info[\"cat_cols\"]]\n        num_column_orders = [column_to_index[col] for col in num_cols_with_y]\n\n    column_orders = num_column_orders + cat_column_orders\n    column_orders = [index_to_column[index] for index in column_orders]\n\n    label_encoders = {}\n    if X_cat is not None and len(df_info[\"cat_cols\"]) &gt; 0:\n        X_cat_all = np.vstack((X_cat[\"train\"], X_cat[\"val\"], X_cat[\"test\"]))\n        X_cat_converted = []\n        for col_index in range(X_cat_all.shape[1]):\n            label_encoder = LabelEncoder()\n            X_cat_converted.append(label_encoder.fit_transform(X_cat_all[:, col_index]).astype(float))\n            if std &gt; 0:\n                # add noise\n                X_cat_converted[-1] += np.random.normal(0, std, X_cat_converted[-1].shape)\n            label_encoders[col_index] = label_encoder\n\n        X_cat_converted = np.vstack(X_cat_converted).T  # type: ignore[assignment]\n\n        train_num = X_cat[\"train\"].shape[0]\n        val_num = X_cat[\"val\"].shape[0]\n        # test_num = X_cat[\"test\"].shape[0]\n\n        X_cat[\"train\"] = X_cat_converted[:train_num, :]  # type: ignore[call-overload]\n        X_cat[\"val\"] = X_cat_converted[train_num : train_num + val_num, :]  # type: ignore[call-overload]\n        X_cat[\"test\"] = X_cat_converted[train_num + val_num :, :]  # type: ignore[call-overload]\n\n        if X_num and len(X_num) &gt; 0:\n            X_num[\"train\"] = np.concatenate((X_num[\"train\"], X_cat[\"train\"]), axis=1)\n            X_num[\"val\"] = np.concatenate((X_num[\"val\"], X_cat[\"val\"]), axis=1)\n            X_num[\"test\"] = np.concatenate((X_num[\"test\"], X_cat[\"test\"]), axis=1)\n        else:\n            X_num = X_cat\n            X_cat = None\n\n    D = Dataset(\n        # ruff: noqa: N806\n        X_num,\n        None,\n        y,\n        y_info={},\n        task_type=TaskType(df_info[\"task_type\"]),\n        n_classes=df_info[\"n_classes\"],\n    )\n\n    return transform_dataset(D, T, None), label_encoders, column_orders\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.update_ema","title":"update_ema","text":"<pre><code>update_ema(target_params, source_params, rate=0.999)\n</code></pre> <p>Update target parameters to be closer to those of source parameters using an exponential moving average. :param target_params: the target parameter sequence. :param source_params: the source parameter sequence. :param rate: the EMA rate (closer to 1 means slower).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def update_ema(\n    target_params: Iterator[nn.Parameter],\n    source_params: Iterator[nn.Parameter],\n    rate: float = 0.999,\n) -&gt; None:\n    \"\"\"\n    Update target parameters to be closer to those of source parameters using\n    an exponential moving average.\n    :param target_params: the target parameter sequence.\n    :param source_params: the source parameter sequence.\n    :param rate: the EMA rate (closer to 1 means slower).\n    \"\"\"\n    for targ, src in zip(target_params, source_params):\n        targ.detach().mul_(rate).add_(src.detach(), alpha=1 - rate)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.create_named_schedule_sampler","title":"create_named_schedule_sampler","text":"<pre><code>create_named_schedule_sampler(name, diffusion)\n</code></pre> <p>Create a ScheduleSampler from a library of pre-defined samplers.</p> <p>:param name: the name of the sampler. :param diffusion: the diffusion object to sample for.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def create_named_schedule_sampler(name: str, diffusion: GaussianMultinomialDiffusion) -&gt; ScheduleSampler:\n    \"\"\"\n    Create a ScheduleSampler from a library of pre-defined samplers.\n\n    :param name: the name of the sampler.\n    :param diffusion: the diffusion object to sample for.\n    \"\"\"\n    if name == \"uniform\":\n        return UniformSampler(diffusion)\n    if name == \"loss-second-moment\":\n        return LossSecondMomentResampler(diffusion)\n    raise NotImplementedError(f\"unknown schedule sampler: {name}\")\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.timestep_embedding","title":"timestep_embedding","text":"<pre><code>timestep_embedding(timesteps, dim, max_period=10000)\n</code></pre> <p>Create sinusoidal timestep embeddings.</p> <p>:param timesteps: a 1-D Tensor of N indices, one per batch element.                   These may be fractional. :param dim: the dimension of the output. :param max_period: controls the minimum frequency of the embeddings. :return: an [N x dim] Tensor of positional embeddings.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def timestep_embedding(timesteps: Tensor, dim: int, max_period: int = 10000) -&gt; Tensor:\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2\n    freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(\n        device=timesteps.device\n    )\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.reglu","title":"reglu","text":"<pre><code>reglu(x)\n</code></pre> <p>The ReGLU activation function from [1].</p> References <p>[1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def reglu(x: Tensor) -&gt; Tensor:\n    \"\"\"The ReGLU activation function from [1].\n\n    References:\n        [1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n    assert x.shape[-1] % 2 == 0\n    a, b = x.chunk(2, dim=-1)\n    return a * F.relu(b)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.geglu","title":"geglu","text":"<pre><code>geglu(x)\n</code></pre> <p>The GEGLU activation function from [1].</p> References <p>[1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def geglu(x: Tensor) -&gt; Tensor:\n    \"\"\"The GEGLU activation function from [1].\n\n    References:\n        [1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n    assert x.shape[-1] % 2 == 0\n    a, b = x.chunk(2, dim=-1)\n    return a * F.gelu(b)\n</code></pre>"},{"location":"user_guide/","title":"User Guide","text":""},{"location":"user_guide/#pyprojecttoml-file-and-dependency-management","title":"pyproject.toml file and dependency management","text":"<p>If your project doesn't have a pyproject.toml file, simply copy the one from the template and update file according to your project.</p> <p>For managing dependencies, this template makes use of uv, which according to some benchmarks is faster than alternative like Poetry (which our original AI Engineering Template makes use of).</p> <p>Hence, be sure to install uv in order to to setup the development virtual environment. Instructions for installing uv can be found here. Note that uv supports optional dependency groups which helps to manage dependencies for different parts of development such as <code>documentation</code>, <code>testing</code>, etc. The core dependencies are installed using the command:</p> <pre><code>uv sync\n</code></pre> <p>Additional dependency groups can be installed using the <code>--group</code> flag followed by the group name. For example:</p> <pre><code>uv sync --all-extras --group docs --group test\n</code></pre> <p>mypy configuration options</p> <p>By default, the <code>mypy</code> configuration in the <code>pyproject.toml</code> disallows subclassing the <code>Any</code> type - <code>allow_subclassing_any = false</code>. In cases where the type checker is not able to determine the types of objects in some external library (e.g. <code>PyTorch</code>), it will treat them as <code>Any</code> and raise errors. If your codebase has many of such cases, you can set <code>allow_subclassing_any = true</code> in the <code>mypy</code> configuration or remove it entirely to use the default value (which is <code>true</code>). For example, in a <code>PyTorch</code> project, subclassing <code>nn.Module</code> will raise errors if <code>allow_subclassing_any</code> is set to <code>false</code>.</p>"},{"location":"user_guide/#pre-commit","title":"pre-commit","text":"<p>You can use pre-commit to run pre-commit hooks (code checks, liniting, etc.) when you run <code>git commit</code> and commit your code. Simply copy the <code>.pre-commit-config.yaml</code> file to the root of the repository and install the test dependencies which installs pre-commit. Then run:</p> <pre><code>pre-commit install\n</code></pre> <p>If you prefer to not enforce using pre-commit every time you run <code>git commit</code>, you will have to run <code>pre-commit run --all-files</code> from the command line before you commit your code.</p> <p>hook configuration</p> <p>Some of the pre-commit hooks use supported hooks from the web.</p> <p>For some others, they are locally installed and hence use the python virtual environment locally. If <code>language</code> is set to <code>python</code>, each time the hook is installed, a separate python virtual environment is created and you can specify dependencies needed using <code>additional_dependencies</code>.</p> <p>If <code>language</code> is set to <code>system</code>, the activated python virtual environment is used and and hence you have to ensure that the required dependencies and their versions are correctly installed.</p> <pre><code>  - repo: local\n    hooks:\n    - id: pytest\n      name: pytest\n      entry: python3 -m pytest -m \"not integration_test\"\n      language: python/system # set according to your project needs\n</code></pre> <p>typos</p> <p>The typos pre-commit hook is used to check for common spelling mistakes in the codebase. While useful, it may require some configuration to ignore certain words or phrases that are not typos. You can configure the typos hook in the <code>pyproject.toml</code> file. In a large codebase, it may be useful to disable the typos hook and only run it occasionally on the entire codebase.</p>"},{"location":"user_guide/#pre-commit-ci","title":"pre-commit ci","text":"<p>Instead of fixing pre-commit errors manually, a CI to fix them as well as update pre-commit hooks periodically can be enabled for your repository. Please check pre-commit.ci and add your repository. The configuration for <code>pre-commit.ci</code> can be added to the <code>.pre-commit-config.yaml</code> file.</p>"},{"location":"user_guide/#documentation","title":"documentation","text":"<p>If your project doesn't have documentation, copy the directory named <code>docs</code> to the root directory of your repository. This template uses MkDocs with the Material for MkDocs theme.</p> <p>In order to build the documentation, install the documentation dependencies as mentioned in the previous section, then run the command:</p> <pre><code>mkdocs build\n</code></pre> <p>If you're making changes to the docs, and want to serve them locally on your machine, then you can use this command instead:</p> <pre><code>mkdocs serve\n</code></pre> <p>The above will launch the docs locally on <code>http://127.0.0.1:8000</code>, which you can enter into your browser of choice. Conveniently, this process also watches for any changes you make to the docs and will update them as they occur.</p> <p>You can configure the documentation by updating the <code>mkdocs.yml</code> file at the root of your repository. The markdown files in the <code>docs</code> directory can be updated to reflect the project's documentation.</p>"},{"location":"user_guide/#github-actions","title":"github actions","text":"<p>The template consists of some github action continuous integration workflows that you can add to your repository.</p> <p>The available workflows are:</p> <ul> <li>code checks: Static code analysis, code formatting and unit tests</li> <li>documentation: Project documentation including example API reference</li> <li>integration tests: Integration tests</li> <li>publish: Publishing python package to PyPI. Create a <code>PYPI_API_TOKEN</code> and add it to the repository's actions secret variables in order to publish PyPI packages when new software releases are created on Github.</li> </ul>"}]}