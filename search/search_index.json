{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MIDST Toolkit Repository","text":""},{"location":"#midst-toolkit","title":"MIDST Toolkit","text":"<p>A toolkit for facilitating MIA resiliency testing on diffusion-model-based synthetic tabular data. Many of the attacks included in this toolkit are based on the most success ones used in the 2025 SaTML MIDST Competition.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#top-level-module","title":"Top Level Module","text":""},{"location":"api/#midst_toolkit","title":"midst_toolkit","text":"<p>PLACEHOLDER.</p>"},{"location":"api/#midst_toolkit.common","title":"common","text":""},{"location":"api/#midst_toolkit.common.enumerations","title":"enumerations","text":""},{"location":"api/#midst_toolkit.common.enumerations.TaskType","title":"TaskType","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>src/midst_toolkit/common/enumerations.py</code> <pre><code>class TaskType(Enum):\n    BINCLASS = \"binclass\"\n    MULTICLASS = \"multiclass\"\n    REGRESSION = \"regression\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string value of the enum.\"\"\"\n        return self.value\n</code></pre>"},{"location":"api/#midst_toolkit.common.enumerations.TaskType.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Return the string value of the enum.</p> Source code in <code>src/midst_toolkit/common/enumerations.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string value of the enum.\"\"\"\n    return self.value\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger","title":"logger","text":"<p>MIDST Toolkit Logger. Borrowed heavily from the Flower Labs logger.</p>"},{"location":"api/#midst_toolkit.common.logger.ConsoleHandler","title":"ConsoleHandler","text":"<p>               Bases: <code>StreamHandler</code></p> Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>class ConsoleHandler(StreamHandler):\n    def __init__(\n        self,\n        timestamps: bool = False,\n        json: bool = False,\n        colored: bool = True,\n        stream: TextIO | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Console handler that allows configurable formatting.\n\n        Args:\n            timestamps: Whether or not to include timestamps. Defaults to False.\n            json: Whether or not to accept json. Defaults to False.\n            colored: Whether or not to apply color to the logs. Defaults to True.\n            stream: To initialize the underlying StreamHandler. Defaults to None.\n        \"\"\"\n        super().__init__(stream)\n        self.timestamps = timestamps\n        self.json = json\n        self.colored = colored\n\n    def emit(self, record: LogRecord) -&gt; None:\n        \"\"\"\n        Console handler that emits the provided record.\n\n        Args:\n            record: Record to emit\n        \"\"\"\n        if self.json:\n            record.message = record.getMessage().replace(\"\\t\", \"\").strip()\n\n            # Check if the message is empty\n            if not record.message:\n                return\n\n        super().emit(record)\n\n    def format(self, record: LogRecord) -&gt; str:\n        \"\"\"\n        Format function that adds colors to log level.\n\n        Args:\n            record: Record to have color added\n\n        Returns:\n            String with color formatting corresponding to the log.\n        \"\"\"\n        seperator = \" \" * (8 - len(record.levelname))\n        if self.json:\n            log_fmt = \"{lvl='%(levelname)s', time='%(asctime)s', msg='%(message)s'}\"\n        else:\n            log_fmt = (\n                f\"{LOG_COLORS[record.levelname] if self.colored else ''}\"\n                f\"%(levelname)s {'%(asctime)s' if self.timestamps else ''}\"\n                f\"{LOG_COLORS['RESET'] if self.colored else ''}\"\n                f\": {seperator} %(message)s\"\n            )\n        formatter = logging.Formatter(log_fmt)\n        return formatter.format(record)\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.ConsoleHandler.__init__","title":"__init__","text":"<pre><code>__init__(\n    timestamps=False, json=False, colored=True, stream=None\n)\n</code></pre> <p>Console handler that allows configurable formatting.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>bool</code> <p>Whether or not to include timestamps. Defaults to False.</p> <code>False</code> <code>json</code> <code>bool</code> <p>Whether or not to accept json. Defaults to False.</p> <code>False</code> <code>colored</code> <code>bool</code> <p>Whether or not to apply color to the logs. Defaults to True.</p> <code>True</code> <code>stream</code> <code>TextIO | None</code> <p>To initialize the underlying StreamHandler. Defaults to None.</p> <code>None</code> Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def __init__(\n    self,\n    timestamps: bool = False,\n    json: bool = False,\n    colored: bool = True,\n    stream: TextIO | None = None,\n) -&gt; None:\n    \"\"\"\n    Console handler that allows configurable formatting.\n\n    Args:\n        timestamps: Whether or not to include timestamps. Defaults to False.\n        json: Whether or not to accept json. Defaults to False.\n        colored: Whether or not to apply color to the logs. Defaults to True.\n        stream: To initialize the underlying StreamHandler. Defaults to None.\n    \"\"\"\n    super().__init__(stream)\n    self.timestamps = timestamps\n    self.json = json\n    self.colored = colored\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.ConsoleHandler.emit","title":"emit","text":"<pre><code>emit(record)\n</code></pre> <p>Console handler that emits the provided record.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>LogRecord</code> <p>Record to emit</p> required Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def emit(self, record: LogRecord) -&gt; None:\n    \"\"\"\n    Console handler that emits the provided record.\n\n    Args:\n        record: Record to emit\n    \"\"\"\n    if self.json:\n        record.message = record.getMessage().replace(\"\\t\", \"\").strip()\n\n        # Check if the message is empty\n        if not record.message:\n            return\n\n    super().emit(record)\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.ConsoleHandler.format","title":"format","text":"<pre><code>format(record)\n</code></pre> <p>Format function that adds colors to log level.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>LogRecord</code> <p>Record to have color added</p> required <p>Returns:</p> Type Description <code>str</code> <p>String with color formatting corresponding to the log.</p> Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def format(self, record: LogRecord) -&gt; str:\n    \"\"\"\n    Format function that adds colors to log level.\n\n    Args:\n        record: Record to have color added\n\n    Returns:\n        String with color formatting corresponding to the log.\n    \"\"\"\n    seperator = \" \" * (8 - len(record.levelname))\n    if self.json:\n        log_fmt = \"{lvl='%(levelname)s', time='%(asctime)s', msg='%(message)s'}\"\n    else:\n        log_fmt = (\n            f\"{LOG_COLORS[record.levelname] if self.colored else ''}\"\n            f\"%(levelname)s {'%(asctime)s' if self.timestamps else ''}\"\n            f\"{LOG_COLORS['RESET'] if self.colored else ''}\"\n            f\": {seperator} %(message)s\"\n        )\n    formatter = logging.Formatter(log_fmt)\n    return formatter.format(record)\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.update_console_handler","title":"update_console_handler","text":"<pre><code>update_console_handler(\n    level=None, timestamps=None, colored=None\n)\n</code></pre> <p>Helper function for setting the proper logging.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int | str | None</code> <p>Level of the logger. Defaults to None.</p> <code>None</code> <code>timestamps</code> <code>bool | None</code> <p>Whether to include timestamps. Defaults to None.</p> <code>None</code> <code>colored</code> <code>bool | None</code> <p>Whether to apply color formatting. Defaults to None.</p> <code>None</code> Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def update_console_handler(\n    level: int | str | None = None,\n    timestamps: bool | None = None,\n    colored: bool | None = None,\n) -&gt; None:\n    \"\"\"\n    Helper function for setting the proper logging.\n\n    Args:\n        level: Level of the logger. Defaults to None.\n        timestamps: Whether to include timestamps. Defaults to None.\n        colored: Whether to apply color formatting. Defaults to None.\n    \"\"\"\n    for handler in logging.getLogger(LOGGER_NAME).handlers:\n        if isinstance(handler, ConsoleHandler):\n            if level is not None:\n                handler.setLevel(level)\n            if timestamps is not None:\n                handler.timestamps = timestamps\n            if colored is not None:\n                handler.colored = colored\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.configure","title":"configure","text":"<pre><code>configure(identifier, filename=None)\n</code></pre> <p>Configure logging to file.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>Identifier to front the logged string</p> required <code>filename</code> <code>str | None</code> <p>Name of the file producing the log, if desired. Defaults to None.</p> <code>None</code> Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def configure(identifier: str, filename: str | None = None) -&gt; None:\n    \"\"\"\n    Configure logging to file.\n\n    Args:\n        identifier: Identifier to front the logged string\n        filename: Name of the file producing the log, if desired. Defaults to None.\n    \"\"\"\n    # Create formatter\n    string_to_input = f\"{identifier} | %(levelname)s %(name)s %(asctime)s \"\n    string_to_input += \"| %(filename)s:%(lineno)d | %(message)s\"\n    formatter = logging.Formatter(string_to_input)\n\n    file_path = Path(filename) if filename else None\n\n    if file_path:\n        assert file_path.parent.exists(), \"Folder into which the logging file is to be inserted does not exist.\"\n        # Create file handler and log to disk\n        file_handler = logging.FileHandler(file_path)\n        file_handler.setLevel(logging.DEBUG)\n        file_handler.setFormatter(formatter)\n        TOOLKIT_LOGGER.addHandler(file_handler)\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.set_logger_propagation","title":"set_logger_propagation","text":"<pre><code>set_logger_propagation(child_logger, value=True)\n</code></pre> <p>Set the logger propagation attribute.</p> <p>Parameters:</p> Name Type Description Default <code>child_logger</code> <code>Logger</code> <p>Child logger object</p> required <code>value</code> <code>bool</code> <p>Boolean setting for propagation. If True, both parent and child logger display messages. Otherwise, only the child logger displays a message. This False setting prevents duplicate logs in Colab notebooks. Reference: https://stackoverflow.com/a/19561320. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Logger</code> <p>Child logger object with updated propagation setting</p> Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def set_logger_propagation(child_logger: logging.Logger, value: bool = True) -&gt; logging.Logger:\n    \"\"\"\n    Set the logger propagation attribute.\n\n    Args:\n        child_logger: Child logger object\n        value: Boolean setting for propagation. If True, both parent and child logger display messages. Otherwise,\n            only the child logger displays a message. This False setting prevents duplicate logs in Colab notebooks.\n            Reference: https://stackoverflow.com/a/19561320. Defaults to True.\n\n    Returns:\n        Child logger object with updated propagation setting\n    \"\"\"\n    child_logger.propagate = value\n    if not child_logger.propagate:\n        child_logger.log(logging.DEBUG, \"Logger propagate set to False\")\n    return child_logger\n</code></pre>"},{"location":"api/#midst_toolkit.common.logger.redirect_output","title":"redirect_output","text":"<pre><code>redirect_output(output_buffer)\n</code></pre> <p>Redirect stdout and stderr to text I/O buffer.</p> <p>Parameters:</p> Name Type Description Default <code>output_buffer</code> <code>StringIO</code> <p>output buffer to be directed to the I/O buffer</p> required Source code in <code>src/midst_toolkit/common/logger.py</code> <pre><code>def redirect_output(output_buffer: StringIO) -&gt; None:\n    \"\"\"\n    Redirect stdout and stderr to text I/O buffer.\n\n    Args:\n        output_buffer: output buffer to be directed to the I/O buffer\n    \"\"\"\n    sys.stdout = output_buffer\n    sys.stderr = output_buffer\n    console_handler.stream = sys.stdout\n</code></pre>"},{"location":"api/#midst_toolkit.core","title":"core","text":"<p>PLACEHOLDER.</p>"},{"location":"api/#midst_toolkit.core.logger","title":"logger","text":"<p>Logger copied from OpenAI baselines to avoid extra RL-based dependencies.</p> <p>https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/logger.py</p>"},{"location":"api/#midst_toolkit.core.logger.TensorBoardOutputFormat","title":"TensorBoardOutputFormat","text":"<p>               Bases: <code>KVWriter</code></p> <p>Dumps key/value pairs into TensorBoard's numeric format.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>class TensorBoardOutputFormat(KVWriter):\n    \"\"\"Dumps key/value pairs into TensorBoard's numeric format.\"\"\"\n\n    def __init__(self, dir: str):\n        os.makedirs(dir, exist_ok=True)\n        self.dir = dir\n        self.step = 1\n        prefix = \"events\"\n        path = osp.join(osp.abspath(dir), prefix)\n        import tensorflow as tf\n        from tensorflow.core.util import event_pb2\n        from tensorflow.python import pywrap_tensorflow\n        from tensorflow.python.util import compat\n\n        self.tf = tf\n        self.event_pb2 = event_pb2\n        self.pywrap_tensorflow = pywrap_tensorflow\n        self.writer = pywrap_tensorflow.EventsWriter(compat.as_bytes(path))\n\n    def writekvs(self, kvs: dict[str, Any]) -&gt; None:\n        def summary_val(k: str, v: Any) -&gt; Any:\n            kwargs = {\"tag\": k, \"simple_value\": float(v)}\n            return self.tf.Summary.Value(**kwargs)\n\n        summary = self.tf.Summary(value=[summary_val(k, v) for k, v in kvs.items()])\n        event = self.event_pb2.Event(wall_time=time.time(), summary=summary)\n        event.step = self.step  # is there any reason why you'd want to specify the step?\n        self.writer.WriteEvent(event)\n        self.writer.Flush()\n        self.step += 1\n\n    def close(self) -&gt; None:\n        if self.writer:\n            self.writer.Close()\n            self.writer = None\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.logkv","title":"logkv","text":"<pre><code>logkv(key, val)\n</code></pre> <p>Log a value of some diagnostic.</p> <p>Call this once for each diagnostic quantity, each iteration If called many times, last value will be used.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def logkv(key: str, val: Any) -&gt; None:\n    \"\"\"\n    Log a value of some diagnostic.\n\n    Call this once for each diagnostic quantity, each iteration\n    If called many times, last value will be used.\n    \"\"\"\n    get_current().logkv(key, val)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.logkv_mean","title":"logkv_mean","text":"<pre><code>logkv_mean(key, val)\n</code></pre> <p>The same as logkv(), but if called many times, values averaged.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def logkv_mean(key: str, val: Any) -&gt; None:\n    \"\"\"The same as logkv(), but if called many times, values averaged.\"\"\"\n    get_current().logkv_mean(key, val)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.logkvs","title":"logkvs","text":"<pre><code>logkvs(d)\n</code></pre> <p>Log a dictionary of key-value pairs.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def logkvs(d: dict[str, Any]) -&gt; None:\n    \"\"\"Log a dictionary of key-value pairs.\"\"\"\n    for k, v in d.items():\n        logkv(k, v)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.dumpkvs","title":"dumpkvs","text":"<pre><code>dumpkvs()\n</code></pre> <p>Write all of the diagnostics from the current iteration.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def dumpkvs() -&gt; dict[str, Any]:\n    \"\"\"Write all of the diagnostics from the current iteration.\"\"\"\n    return get_current().dumpkvs()\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.log","title":"log","text":"<pre><code>log(*args, level=INFO)\n</code></pre> <p>Logs the args in the desired level.</p> <p>Write the sequence of args, with no separators, to the console and output files (if you've configured an output file).</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def log(*args: Iterable[Any], level: int = INFO) -&gt; None:\n    \"\"\"\n    Logs the args in the desired level.\n\n    Write the sequence of args, with no separators, to the console and output\n    files (if you've configured an output file).\n    \"\"\"\n    get_current().log(*args, level=level)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.set_level","title":"set_level","text":"<pre><code>set_level(level)\n</code></pre> <p>Set logging threshold on current logger.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def set_level(level: int) -&gt; None:\n    \"\"\"Set logging threshold on current logger.\"\"\"\n    get_current().set_level(level)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.get_dir","title":"get_dir","text":"<pre><code>get_dir()\n</code></pre> <p>Get directory that log files are being written to.</p> <p>will be None if there is no output directory (i.e., if you didn't call start)</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def get_dir() -&gt; str:\n    \"\"\"\n    Get directory that log files are being written to.\n\n    will be None if there is no output directory (i.e., if you didn't call start)\n    \"\"\"\n    return get_current().get_dir()\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.profile","title":"profile","text":"<pre><code>profile(n)\n</code></pre> <p>Usage.</p> <p>@profile(\"my_func\") def my_func(): code</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def profile(n):\n    \"\"\"\n    Usage.\n\n    @profile(\"my_func\")\n    def my_func(): code\n    \"\"\"\n\n    def decorator_with_name(func):\n        def func_wrapper(*args, **kwargs):\n            with profile_kv(n):\n                return func(*args, **kwargs)\n\n        return func_wrapper\n\n    return decorator_with_name\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.mpi_weighted_mean","title":"mpi_weighted_mean","text":"<pre><code>mpi_weighted_mean(comm, local_name2valcount)\n</code></pre> <p>Copied from below.</p> <p>https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/common/mpi_util.py#L110 Perform a weighted average over dicts that are each on a different node Input: local_name2valcount: dict mapping key -&gt; (value, count) Returns: key -&gt; mean</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def mpi_weighted_mean(comm: Any, local_name2valcount: dict[str, tuple[float, float]]) -&gt; dict[str, float]:\n    \"\"\"\n    Copied from below.\n\n    https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/common/mpi_util.py#L110\n    Perform a weighted average over dicts that are each on a different node\n    Input: local_name2valcount: dict mapping key -&gt; (value, count)\n    Returns: key -&gt; mean\n    \"\"\"\n    all_name2valcount = comm.gather(local_name2valcount)\n    if comm.rank == 0:\n        name2sum: defaultdict[str, float] = defaultdict(float)\n        name2count: defaultdict[str, float] = defaultdict(float)\n        for n2vc in all_name2valcount:\n            for name, (val, count) in n2vc.items():\n                try:\n                    val = float(val)\n                except ValueError:\n                    if comm.rank == 0:\n                        warnings.warn(\"WARNING: tried to compute mean on non-float {}={}\".format(name, val))\n                        # ruff: noqa: B028\n                else:\n                    name2sum[name] += val * count\n                    name2count[name] += count\n        return {name: name2sum[name] / name2count[name] for name in name2sum}\n    return {}\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.configure","title":"configure","text":"<pre><code>configure(\n    dir=None, format_strs=None, comm=None, log_suffix=\"\"\n)\n</code></pre> <p>If comm is provided, average all numerical stats across that comm.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def configure(\n    dir: str | None = None,\n    format_strs: list[str] | None = None,\n    comm: Any | None = None,\n    log_suffix: str = \"\",\n) -&gt; None:\n    \"\"\"If comm is provided, average all numerical stats across that comm.\"\"\"\n    if dir is None:\n        dir = os.getenv(\"OPENAI_LOGDIR\")\n    if dir is None:\n        dir = osp.join(\n            tempfile.gettempdir(),\n            datetime.datetime.now().strftime(\"openai-%Y-%m-%d-%H-%M-%S-%f\"),\n        )\n    assert isinstance(dir, str)\n    dir = os.path.expanduser(dir)\n    os.makedirs(os.path.expanduser(dir), exist_ok=True)\n\n    rank = get_rank_without_mpi_import()\n    if rank &gt; 0:\n        log_suffix = log_suffix + \"-rank%03i\" % rank\n\n    if format_strs is None:\n        if rank == 0:\n            format_strs = os.getenv(\"OPENAI_LOG_FORMAT\", \"stdout,log,csv\").split(\",\")\n        else:\n            format_strs = os.getenv(\"OPENAI_LOG_FORMAT_MPI\", \"log\").split(\",\")\n    format_strs_filter = filter(None, format_strs)\n    output_formats = [make_output_format(f, dir, log_suffix) for f in format_strs_filter]\n\n    Logger.CURRENT = Logger(dir=dir, output_formats=output_formats, comm=comm)  # type: ignore[assignment]\n    if output_formats:\n        log(\"Logging to %s\" % dir)\n</code></pre>"},{"location":"api/#midst_toolkit.data_processing","title":"data_processing","text":""},{"location":"api/#midst_toolkit.data_processing.midst_data_processing","title":"midst_data_processing","text":""},{"location":"api/#midst_toolkit.data_processing.midst_data_processing.process_midst_data_for_quality_evaluation","title":"process_midst_data_for_quality_evaluation","text":"<pre><code>process_midst_data_for_quality_evaluation(\n    numerical_real_data,\n    categorical_real_data,\n    numerical_synthetic_data,\n    categorical_synthetic_data,\n    dataset_name,\n    model,\n)\n</code></pre> <p>This function handles data preprocessing customized to some of the models and datasets used in the MIDST competition. The processing is drawn from https://github.com/VectorInstitute/MIDSTModels/blob/main/midst_models/single_table_TabDDPM/eval/eval_quality.py.</p> <p>It has special considerations for how the provided dataframes are processed into numpy arrays depending on the dataname and model provided in the arguments.</p> <p>Parameters:</p> Name Type Description Default <code>numerical_real_data</code> <code>DataFrame</code> <p>Real data with numerical values</p> required <code>categorical_real_data</code> <code>DataFrame</code> <p>Real data with categorical values</p> required <code>numerical_synthetic_data</code> <code>DataFrame</code> <p>Synthetically generated data with numerical values</p> required <code>categorical_synthetic_data</code> <code>DataFrame</code> <p>Synthetically generated data with numerical values</p> required <code>dataset_name</code> <code>str</code> <p>Name of the dataset to which the real data belongs. The way that the data is processed will depend on whether special treatment is required for the specified name.</p> required <code>model</code> <code>str</code> <p>Model that was used to generate the synthetic data. Specific model names require special postprocessing in order for quality evaluation</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A tuple of four Numpy arrays, one for each of the numerical and categorical collections of real and synthetic</p> <code>ndarray</code> <p>data after processing. The order is numerical and categorical data for the real data, followed by the same</p> <code>ndarray</code> <p>for the synthetic data.</p> Source code in <code>src/midst_toolkit/data_processing/midst_data_processing.py</code> <pre><code>def process_midst_data_for_quality_evaluation(\n    numerical_real_data: pd.DataFrame,\n    categorical_real_data: pd.DataFrame,\n    numerical_synthetic_data: pd.DataFrame,\n    categorical_synthetic_data: pd.DataFrame,\n    dataset_name: str,\n    model: str,\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    This function handles data preprocessing customized to some of the models and datasets used in the MIDST\n    competition. The processing is drawn from\n    https://github.com/VectorInstitute/MIDSTModels/blob/main/midst_models/single_table_TabDDPM/eval/eval_quality.py.\n\n    It has special considerations for how the provided dataframes are processed into numpy arrays depending on the\n    dataname and model provided in the arguments.\n\n    Args:\n        numerical_real_data: Real data with numerical values\n        categorical_real_data: Real data with categorical values\n        numerical_synthetic_data: Synthetically generated data with numerical values\n        categorical_synthetic_data: Synthetically generated data with numerical values\n        dataset_name: Name of the dataset to which the real data belongs. The way that the data is processed will\n            depend on whether special treatment is required for the specified name.\n        model: Model that was used to generate the synthetic data. Specific model names require special postprocessing\n            in order for quality evaluation\n\n    Returns:\n        A tuple of four Numpy arrays, one for each of the numerical and categorical collections of real and synthetic\n        data after processing. The order is numerical and categorical data for the real data, followed by the same\n        for the synthetic data.\n    \"\"\"\n    categorical_synthetic_numpy = categorical_synthetic_data.to_numpy().astype(\"str\")\n\n    # Perform some special data post-processing for specific datasets and models as specified in the script\n    # arguments\n\n    if dataset_name in CONVERSION_DATASETS and model.startswith(CONVERSION_MODEL_PREFIX):\n        # If using the default or news dataset and a model postfixed with \"codi,\" need to perform an int cast\n        categorical_synthetic_numpy = categorical_synthetic_data.astype(\"int\").to_numpy().astype(\"str\")\n    elif model.startswith(CLIPPING_MODEL_PREFIX):\n        if dataset_name in MAX_CLIPPING_DATASETS:\n            # Column reassignment\n            categorical_synthetic_numpy[:, 1] = categorical_synthetic_data[11].astype(\"int\").to_numpy().astype(\"str\")\n            categorical_synthetic_numpy[:, 2] = categorical_synthetic_data[12].astype(\"int\").to_numpy().astype(\"str\")\n            categorical_synthetic_numpy[:, 3] = categorical_synthetic_data[13].astype(\"int\").to_numpy().astype(\"str\")\n\n            # Clip the maximum value to reflect that of the real data\n            max_data = categorical_real_data[14].max()\n            categorical_synthetic_data.loc[categorical_synthetic_data[14] &gt; max_data, 14] = max_data\n\n            # Perform column reassignment\n            categorical_synthetic_numpy[:, 4] = categorical_synthetic_data[14].astype(\"int\").to_numpy().astype(\"str\")\n            categorical_synthetic_numpy[:, 4] = categorical_synthetic_data[14].astype(\"int\").to_numpy().astype(\"str\")\n\n        elif dataset_name in MIN_MAX_CLIPPING_DATASETS:\n            # Note that columns here are not contiguous, so we enumerate\n            columns = categorical_real_data.columns\n            for i, col in enumerate(columns):\n                if categorical_real_data[col].dtype == \"int\":\n                    max_data = categorical_real_data[col].max()\n                    min_data = categorical_real_data[col].min()\n\n                    # Perform clipping based on the real data on both sides (min and max)\n                    categorical_synthetic_data.loc[categorical_synthetic_data[col] &gt; max_data, col] = max_data\n                    categorical_synthetic_data.loc[categorical_synthetic_data[col] &lt; min_data, col] = min_data\n\n                    categorical_synthetic_numpy[:, i] = (\n                        categorical_synthetic_data[col].astype(\"int\").to_numpy().astype(\"str\")\n                    )\n    return (\n        numerical_real_data.to_numpy(),\n        categorical_real_data.to_numpy().astype(\"str\"),\n        numerical_synthetic_data.to_numpy(),\n        categorical_synthetic_numpy,\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.data_processing.midst_data_processing.load_midst_data","title":"load_midst_data","text":"<pre><code>load_midst_data(\n    real_data_path, synthetic_data_path, meta_info_path\n)\n</code></pre> <p>Helper function for loading data at the specified paths. These paths are constructed either by the user or with a particular set of defaults that were used in the original MIDST competition (see, for example, https://github.com/VectorInstitute/MIDSTModels/blob/main/midst_models/single_table_TabDDPM/eval/eval_quality.py).</p> <p>Parameters:</p> Name Type Description Default <code>real_data_path</code> <code>Path</code> <p>Path from which to load the real data to which the synthetic data will be compared. This should be a CSV file.</p> required <code>synthetic_data_path</code> <code>Path</code> <p>Path from which to load the synthetic data to which the real data will be compared. This should be a CSV file.</p> required <code>meta_info_path</code> <code>Path</code> <p>This should be a JSON file containing meta information about the data generation process. Specifically, it should contain information about which columns of the real and synthetic data should actually be compared. It must contain keys: 'num_col_idx', 'cat_col_idx', 'target_col_idx', and 'task_type'.</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame, dict[str, Any]]</code> <p>The loaded real data, synthetic data, and meta information json for further processing.</p> Source code in <code>src/midst_toolkit/data_processing/midst_data_processing.py</code> <pre><code>def load_midst_data(\n    real_data_path: Path, synthetic_data_path: Path, meta_info_path: Path\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, dict[str, Any]]:\n    \"\"\"\n    Helper function for loading data at the specified paths. These paths are constructed either by the user or with a\n    particular set of defaults that were used in the original MIDST competition (see, for example,\n    https://github.com/VectorInstitute/MIDSTModels/blob/main/midst_models/single_table_TabDDPM/eval/eval_quality.py).\n\n    Args:\n        real_data_path: Path from which to load the real data to which the synthetic data will be compared. This\n            should be a CSV file.\n        synthetic_data_path: Path from which to load the synthetic data to which the real data will be compared. This\n            should be a CSV file.\n        meta_info_path: This should be a JSON file containing meta information about the data generation process.\n            Specifically, it should contain information about which columns of the real and synthetic data should\n            actually be compared. It must contain keys: 'num_col_idx', 'cat_col_idx', 'target_col_idx', and\n            'task_type'.\n\n    Returns:\n        The loaded real data, synthetic data, and meta information json for further processing.\n    \"\"\"\n    real_data = pd.read_csv(real_data_path)\n    synthetic_data = pd.read_csv(synthetic_data_path)\n\n    with open(meta_info_path, \"r\") as f:\n        meta_info = json.load(f)\n\n    return real_data, synthetic_data, meta_info\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation","title":"evaluation","text":""},{"location":"api/#midst_toolkit.evaluation.generation_quality","title":"generation_quality","text":""},{"location":"api/#midst_toolkit.evaluation.generation_quality.alpha_precision","title":"alpha_precision","text":""},{"location":"api/#midst_toolkit.evaluation.generation_quality.alpha_precision.synthcity_alpha_precision_metrics","title":"synthcity_alpha_precision_metrics","text":"<pre><code>synthcity_alpha_precision_metrics(\n    real_data, synthetic_data, naive_only=True\n)\n</code></pre> <p>Computes a number of quality metrics comparing the synthetic data to ground truth data using the Synthcity library. This function uses the AlphaPrecision class in that library, which computes the alpha-precision, beta-recall, and authenticity scores between the two datasets. If the <code>naive_only</code> boolean is True, then only the \"naive\" metrics are reported, i.e. metrics with \"naive\" in their name.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>DataFrame</code> <p>Real data that the synthetic data is meant to mimic/replace.</p> required <code>synthetic_data</code> <code>DataFrame</code> <p>Synthetic data to be compared against the provided real data.</p> required <code>naive_only</code> <code>bool</code> <p>If True, then only the \"naive\" metrics are reported. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the computed scores using the AlphaPrecision class in the Synthcity library.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/alpha_precision.py</code> <pre><code>def synthcity_alpha_precision_metrics(\n    real_data: pd.DataFrame, synthetic_data: pd.DataFrame, naive_only: bool = True\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Computes a number of quality metrics comparing the synthetic data to ground truth data using the Synthcity library.\n    This function uses the AlphaPrecision class in that library, which computes the alpha-precision, beta-recall, and\n    authenticity scores between the two datasets. If the ``naive_only`` boolean is True, then only the \"naive\" metrics\n    are reported, i.e. metrics with \"naive\" in their name.\n\n    Args:\n        real_data: Real data that the synthetic data is meant to mimic/replace.\n        synthetic_data: Synthetic data to be compared against the provided real data.\n        naive_only: If True, then only the \"naive\" metrics are reported. Defaults to True.\n\n    Returns:\n        A dictionary containing the computed scores using the AlphaPrecision class in the Synthcity library.\n    \"\"\"\n    # Wrap the dataframes in a Synthcity compatible dataloader\n    real_data_loader = GenericDataLoader(real_data)\n    synthetic_data_loader = GenericDataLoader(synthetic_data)\n\n    quality_evaluator = eval_statistical.AlphaPrecision()\n    quality_results = quality_evaluator.evaluate(real_data_loader, synthetic_data_loader)\n\n    # Log results and filter to naive keys if requested\n    for metric_key, metric_value in quality_results.items():\n        log(INFO, f\"{metric_key}: {metric_value}\")\n        if naive_only and (NAIVE_METRIC_SUFFIX not in metric_key):\n            del quality_results[metric_key]\n\n    return quality_results\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.generation_quality.utils","title":"utils","text":""},{"location":"api/#midst_toolkit.evaluation.generation_quality.utils.create_quality_metrics_directory","title":"create_quality_metrics_directory","text":"<pre><code>create_quality_metrics_directory(save_directory)\n</code></pre> <p>Helper function for creating a directory at the specified path to whole metrics results. If the directory already exists, this function will log a warning and no-op.</p> <p>Parameters:</p> Name Type Description Default <code>save_directory</code> <code>Path</code> <p>Path of the directory to create.</p> required Source code in <code>src/midst_toolkit/evaluation/generation_quality/utils.py</code> <pre><code>def create_quality_metrics_directory(save_directory: Path) -&gt; None:\n    \"\"\"\n    Helper function for creating a directory at the specified path to whole metrics results. If the directory already\n    exists, this function will log a warning and no-op.\n\n    Args:\n        save_directory: Path of the directory to create.\n    \"\"\"\n    if not os.path.exists(save_directory):\n        os.makedirs(save_directory)\n    else:\n        log(WARNING, f\"Path: {save_directory} already exists. Make sure this is intended.\")\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.generation_quality.utils.dump_metrics_dict","title":"dump_metrics_dict","text":"<pre><code>dump_metrics_dict(metrics_dict, file_path)\n</code></pre> <p>Write the provided metrics dictionary to the provided <code>file_path</code> argument. The metrics dictionary is written in a specific format.</p> <p>Parameters:</p> Name Type Description Default <code>metrics_dict</code> <code>dict[str, float]</code> <p>Dictionary of metrics with string key values and associated floats representing metrics calculations</p> required <code>file_path</code> <code>Path</code> <p>Path to which the metrics are written. The file will be created or overwritten if it exists</p> required Source code in <code>src/midst_toolkit/evaluation/generation_quality/utils.py</code> <pre><code>def dump_metrics_dict(metrics_dict: dict[str, float], file_path: Path) -&gt; None:\n    \"\"\"\n    Write the provided metrics dictionary to the provided ``file_path`` argument. The metrics dictionary is written\n    in a specific format.\n\n    Args:\n        metrics_dict: Dictionary of metrics with string key values and associated floats representing metrics\n            calculations\n        file_path: Path to which the metrics are written. The file will be created or overwritten if it exists\n    \"\"\"\n    if os.path.exists(file_path):\n        log(WARNING, f\"File at path {file_path} already exists.\")\n    with open(file_path, \"w\") as f:\n        for metric_key, metric_value in metrics_dict.items():\n            f.write(f\"Metric Name: {metric_key}\\t Metric Value: {metric_value}\\n\")\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.generation_quality.utils.extract_columns_based_on_meta_info","title":"extract_columns_based_on_meta_info","text":"<pre><code>extract_columns_based_on_meta_info(data, meta_info)\n</code></pre> <p>Given a set of meta information, which should be in JSON format with keys 'num_col_idx', 'cat_col_idx', 'target_col_idx', and 'task_type', the provided dataframe is filtered to the correct set of columns for evaluation using the meta information.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe to be filtered using the meta information</p> required <code>meta_info</code> <code>dict[str, Any]</code> <p>JSON with meta information about the columns and their corresponding types that should be considered. At minimum, it should have the keys keys 'num_col_idx', 'cat_col_idx', 'target_col_idx', and 'task_type'</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Filtered dataframes. The first dataframe is the filtered set of columns associated with numerical data. The</p> <code>DataFrame</code> <p>second is the filtered set of columns associated with categorical data.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/utils.py</code> <pre><code>def extract_columns_based_on_meta_info(\n    data: pd.DataFrame, meta_info: dict[str, Any]\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Given a set of meta information, which should be in JSON format with keys 'num_col_idx', 'cat_col_idx',\n    'target_col_idx', and 'task_type', the provided dataframe is filtered to the correct set of columns for evaluation\n    using the meta information.\n\n    Args:\n        data: Dataframe to be filtered using the meta information\n        meta_info: JSON with meta information about the columns and their corresponding types that should be\n            considered. At minimum, it should have the keys keys 'num_col_idx', 'cat_col_idx', 'target_col_idx',\n            and 'task_type'\n\n    Returns:\n        Filtered dataframes. The first dataframe is the filtered set of columns associated with numerical data. The\n        second is the filtered set of columns associated with categorical data.\n    \"\"\"\n    # TODO: Consider creating a meta_info class that formalizes the structure of the meta_info produced when\n    # Training the diffusion generators.\n\n    # Enumerate columns and replace column name with index\n    data.columns = range(len(data.columns))\n\n    # Get numerical and categorical column indices from meta info\n    # NOTE: numerical and categorical columns are the only admissible/generate-able types\"\n    numerical_column_idx = meta_info[\"num_col_idx\"]\n    categorical_column_idx = meta_info[\"cat_col_idx\"]\n\n    # Target columns are also part of the generation, just need to add it to the right \"category\"\n    target_col_idx = meta_info[\"target_col_idx\"]\n    task_type = TaskType(meta_info[\"task_type\"])\n    if task_type == TaskType.REGRESSION:\n        numerical_column_idx = numerical_column_idx + target_col_idx\n    else:\n        categorical_column_idx = categorical_column_idx + target_col_idx\n\n    numerical_data = data[numerical_column_idx]\n    categorical_data = data[categorical_column_idx]\n\n    return numerical_data, categorical_data\n</code></pre>"},{"location":"api/#midst_toolkit.evaluation.generation_quality.utils.one_hot_encode_categoricals_and_merge_with_numerical","title":"one_hot_encode_categoricals_and_merge_with_numerical","text":"<pre><code>one_hot_encode_categoricals_and_merge_with_numerical(\n    real_categorical_data,\n    synthetic_categorical_data,\n    real_numerical_data,\n    synthetic_numerical_data,\n)\n</code></pre> <p>Performs one-hot encoding on the real and synthetic data contained in numpy arrays. The <code>real_categorical_data</code> is used to fit the one-hot encoder, which is then applied to the data in <code>synthetic_categorical_data</code>. The resulting, one-hot encoded, numpy arrays are then concatenated together numerical then one-hots for both the synthetic and real data.</p> <p>Parameters:</p> Name Type Description Default <code>real_categorical_data</code> <code>ndarray</code> <p>Categorical data from the real dataset.</p> required <code>synthetic_categorical_data</code> <code>ndarray</code> <p>Categorical data from the synthetically generated dataset.</p> required <code>real_numerical_data</code> <code>ndarray</code> <p>Numerical data from the real dataset.</p> required <code>synthetic_numerical_data</code> <code>ndarray</code> <p>Numerical data from the synthetically generated dataset.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Two pandas dataframes representing the numerical and categorical data concatenated together. First dataframe</p> <code>DataFrame</code> <p>is the real data, second is the synthetic data.</p> Source code in <code>src/midst_toolkit/evaluation/generation_quality/utils.py</code> <pre><code>def one_hot_encode_categoricals_and_merge_with_numerical(\n    real_categorical_data: np.ndarray,\n    synthetic_categorical_data: np.ndarray,\n    real_numerical_data: np.ndarray,\n    synthetic_numerical_data: np.ndarray,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Performs one-hot encoding on the real and synthetic data contained in numpy arrays. The ``real_categorical_data``\n    is used to fit the one-hot encoder, which is then applied to the data in ``synthetic_categorical_data``. The\n    resulting, one-hot encoded, numpy arrays are then concatenated together numerical then one-hots for both the\n    synthetic and real data.\n\n    Args:\n        real_categorical_data: Categorical data from the real dataset.\n        synthetic_categorical_data: Categorical data from the synthetically generated dataset.\n        real_numerical_data: Numerical data from the real dataset.\n        synthetic_numerical_data: Numerical data from the synthetically generated dataset.\n\n    Returns:\n        Two pandas dataframes representing the numerical and categorical data concatenated together. First dataframe\n        is the real data, second is the synthetic data.\n    \"\"\"\n    encoder = OneHotEncoder()\n    one_hot_real_data = encoder.fit_transform(real_categorical_data).toarray()\n    one_hot_synthetic_data = encoder.transform(synthetic_categorical_data).toarray()\n\n    real_dataframe = pd.DataFrame(np.concatenate((real_numerical_data, one_hot_real_data), axis=1)).astype(float)\n\n    synthetic_dataframe = pd.DataFrame(\n        np.concatenate((synthetic_numerical_data, one_hot_synthetic_data), axis=1)\n    ).astype(float)\n\n    return real_dataframe, synthetic_dataframe\n</code></pre>"},{"location":"api/#midst_toolkit.models","title":"models","text":"<p>PLACEHOLDER.</p>"},{"location":"api/#midst_toolkit.models.clavaddpm","title":"clavaddpm","text":"<p>PLACEHOLDER.</p>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils","title":"diffusion_utils","text":"<p>PLACEHOLDER.</p>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.FoundNANsError","title":"FoundNANsError","text":"<p>               Bases: <code>BaseException</code></p> <p>Found NANs during sampling.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>class FoundNANsError(BaseException):\n    \"\"\"Found NANs during sampling.\"\"\"\n\n    def __init__(self, message=\"Found NANs during sampling.\"):\n        # ruff: noqa: D107\n        super(FoundNANsError, self).__init__(message)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.normal_kl","title":"normal_kl","text":"<pre><code>normal_kl(mean1, logvar1, mean2, logvar2)\n</code></pre> <p>Compute the KL divergence between two gaussians.</p> <p>Shapes are automatically broadcasted, so batches can be compared to scalars, among other use cases.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def normal_kl(\n    mean1: Tensor | float,\n    logvar1: Tensor | float,\n    mean2: Tensor | float,\n    logvar2: Tensor | float,\n) -&gt; Tensor:\n    \"\"\"\n    Compute the KL divergence between two gaussians.\n\n    Shapes are automatically broadcasted, so batches can be compared to\n    scalars, among other use cases.\n    \"\"\"\n    tensor = None\n    for obj in (mean1, logvar1, mean2, logvar2):\n        if isinstance(obj, torch.Tensor):\n            tensor = obj\n            break\n    assert tensor is not None, \"at least one argument must be a Tensor\"\n\n    # Force variances to be Tensors. Broadcasting helps convert scalars to\n    # Tensors, but it does not work for torch.exp().\n    logvar1, logvar2 = [x if isinstance(x, torch.Tensor) else torch.tensor(x).to(tensor) for x in (logvar1, logvar2)]\n\n    return 0.5 * (\n        -1.0 + logvar2 - logvar1 + torch.exp(logvar1 - logvar2) + ((mean1 - mean2) ** 2) * torch.exp(-logvar2)\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.approx_standard_normal_cdf","title":"approx_standard_normal_cdf","text":"<pre><code>approx_standard_normal_cdf(x)\n</code></pre> <p>A fast approximation of the cumulative distribution function of the standard normal.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def approx_standard_normal_cdf(x: Tensor) -&gt; Tensor:\n    \"\"\"\n    A fast approximation of the cumulative distribution function of the\n    standard normal.\n    \"\"\"\n    return 0.5 * (1.0 + torch.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * torch.pow(x, 3))))\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.discretized_gaussian_log_likelihood","title":"discretized_gaussian_log_likelihood","text":"<pre><code>discretized_gaussian_log_likelihood(\n    x, *, means, log_scales\n)\n</code></pre> <p>Compute the log-likelihood of a Gaussian distribution discretizing to a given image.</p> <p>:param x: the target images. It is assumed that this was uint8 values,           rescaled to the range [-1, 1]. :param means: the Gaussian mean Tensor. :param log_scales: the Gaussian log stddev Tensor. :return: a tensor like x of log probabilities (in nats).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def discretized_gaussian_log_likelihood(x: Tensor, *, means: Tensor, log_scales: Tensor) -&gt; Tensor:\n    \"\"\"\n    Compute the log-likelihood of a Gaussian distribution discretizing to a\n    given image.\n\n    :param x: the target images. It is assumed that this was uint8 values,\n              rescaled to the range [-1, 1].\n    :param means: the Gaussian mean Tensor.\n    :param log_scales: the Gaussian log stddev Tensor.\n    :return: a tensor like x of log probabilities (in nats).\n    \"\"\"\n    assert x.shape == means.shape == log_scales.shape\n    centered_x = x - means\n    inv_stdv = torch.exp(-log_scales)\n    plus_in = inv_stdv * (centered_x + 1.0 / 255.0)\n    cdf_plus = approx_standard_normal_cdf(plus_in)\n    min_in = inv_stdv * (centered_x - 1.0 / 255.0)\n    cdf_min = approx_standard_normal_cdf(min_in)\n    log_cdf_plus = torch.log(cdf_plus.clamp(min=1e-12))\n    log_one_minus_cdf_min = torch.log((1.0 - cdf_min).clamp(min=1e-12))\n    cdf_delta = cdf_plus - cdf_min\n    log_probs = torch.where(\n        x &lt; -0.999,\n        log_cdf_plus,\n        torch.where(x &gt; 0.999, log_one_minus_cdf_min, torch.log(cdf_delta.clamp(min=1e-12))),\n    )\n    assert log_probs.shape == x.shape\n    return log_probs\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.sum_except_batch","title":"sum_except_batch","text":"<pre><code>sum_except_batch(x, num_dims=1)\n</code></pre> <p>Sums all dimensions except the first.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Tensor, shape (batch_size, ...)</p> required <code>num_dims</code> <code>int</code> <p>int, number of batch dims (default=1)</p> <code>1</code> <p>Returns:</p> Name Type Description <code>x_sum</code> <code>Tensor</code> <p>Tensor, shape (batch_size,)</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def sum_except_batch(x: Tensor, num_dims: int = 1) -&gt; Tensor:\n    \"\"\"\n    Sums all dimensions except the first.\n\n    Args:\n        x: Tensor, shape (batch_size, ...)\n        num_dims: int, number of batch dims (default=1)\n\n    Returns:\n        x_sum: Tensor, shape (batch_size,)\n    \"\"\"\n    return x.reshape(*x.shape[:num_dims], -1).sum(-1)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.mean_flat","title":"mean_flat","text":"<pre><code>mean_flat(tensor)\n</code></pre> <p>Take the mean over all non-batch dimensions.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def mean_flat(tensor: Tensor) -&gt; Tensor:\n    \"\"\"Take the mean over all non-batch dimensions.\"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion","title":"gaussian_multinomial_diffusion","text":"<p>Based on the code below.</p> <p>https://github.com/openai/guided-diffusion/blob/main/guided_diffusion https://github.com/ehoogeboom/multinomial_diffusion</p>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.GaussianMultinomialDiffusion","title":"GaussianMultinomialDiffusion","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>class GaussianMultinomialDiffusion(torch.nn.Module):\n    def __init__(\n        # ruff: noqa: PLR0915\n        self,\n        num_classes: np.ndarray,\n        num_numerical_features: int,\n        denoise_fn: torch.nn.Module,\n        num_timesteps: int = 1000,\n        gaussian_loss_type: str = \"mse\",\n        gaussian_parametrization: str = \"eps\",\n        multinomial_loss_type: str = \"vb_stochastic\",\n        parametrization: str = \"x0\",\n        scheduler: str = \"cosine\",\n        device: torch.device | None = None,\n    ):\n        # ruff: noqa: D107\n        if device is None:\n            device = torch.device(\"cpu\")\n\n        super(GaussianMultinomialDiffusion, self).__init__()\n        assert multinomial_loss_type in (\"vb_stochastic\", \"vb_all\")\n        assert parametrization in (\"x0\", \"direct\")\n\n        if multinomial_loss_type == \"vb_all\":\n            print(\n                \"Computing the loss using the bound on _all_ timesteps.\"\n                \" This is expensive both in terms of memory and computation.\"\n            )\n\n        self.num_numerical_features = num_numerical_features\n        self.num_classes = num_classes  # it as a vector [K1, K2, ..., Km]\n        self.num_classes_expanded = torch.from_numpy(\n            np.concatenate([num_classes[i].repeat(num_classes[i]) for i in range(len(num_classes))])\n        ).to(device)\n\n        self.slices_for_classes = [np.arange(self.num_classes[0])]\n        offsets: np.ndarray = np.cumsum(self.num_classes)\n        for i in range(1, len(offsets)):\n            self.slices_for_classes.append(np.arange(offsets[i - 1], offsets[i]))\n        self.offsets = torch.from_numpy(np.append([0], offsets)).to(device)\n\n        self._denoise_fn = denoise_fn\n        self.gaussian_loss_type = gaussian_loss_type\n        self.gaussian_parametrization = gaussian_parametrization\n        self.multinomial_loss_type = multinomial_loss_type\n        self.num_timesteps = num_timesteps\n        self.parametrization = parametrization\n        self.scheduler = scheduler\n        self.device = device\n        self.alphas: Tensor\n        self.alphas_cumprod: Tensor\n        self.alphas_cumprod_next: Tensor\n        self.alphas_cumprod_prev: Tensor\n        self.sqrt_alphas_cumprod: Tensor\n        self.sqrt_one_minus_alphas_cumprod: Tensor\n        self.log_cumprod_alpha: Tensor\n        self.log_alpha: Tensor\n        self.log_1_min_alpha: Tensor\n        self.log_1_min_cumprod_alpha: Tensor\n        self.sqrt_recipm1_alphas_cumprod: Tensor\n        self.sqrt_recip_alphas_cumprod: Tensor\n        self.Lt_history: Tensor\n        self.Lt_count: Tensor\n\n        a = 1.0 - get_named_beta_schedule(scheduler, num_timesteps)\n        alphas = torch.tensor(a.astype(\"float64\"))\n        betas = 1.0 - alphas\n\n        log_alpha: Tensor = np.log(alphas)  # type: ignore[assignment]\n        log_cumprod_alpha: Tensor = np.cumsum(log_alpha)  # type: ignore[assignment]\n\n        log_1_min_alpha: Tensor = log_1_min_a(log_alpha)\n        log_1_min_cumprod_alpha: Tensor = log_1_min_a(log_cumprod_alpha)\n\n        alphas_cumprod: Tensor = np.cumprod(alphas, axis=0)  # type: ignore[assignment]\n        alphas_cumprod_prev = torch.tensor(np.append(1.0, alphas_cumprod[:-1]))\n        alphas_cumprod_next = torch.tensor(np.append(alphas_cumprod[1:], 0.0))\n        sqrt_alphas_cumprod: Tensor = np.sqrt(alphas_cumprod)  # type: ignore[assignment]\n        sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - alphas_cumprod)\n        sqrt_recip_alphas_cumprod: Tensor = np.sqrt(1.0 / alphas_cumprod)\n        sqrt_recipm1_alphas_cumprod: Tensor = np.sqrt(1.0 / alphas_cumprod - 1)\n\n        # Gaussian diffusion\n\n        self.posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n        self.posterior_log_variance_clipped = (\n            torch.from_numpy(np.log(np.append(self.posterior_variance[1], self.posterior_variance[1:])))\n            .float()\n            .to(device)\n        )\n        self.posterior_mean_coef1 = (betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)).float().to(device)\n        self.posterior_mean_coef2 = (\n            ((1.0 - alphas_cumprod_prev) * np.sqrt(alphas.numpy()) / (1.0 - alphas_cumprod)).float().to(device)\n        )\n\n        assert log_add_exp(log_alpha, log_1_min_alpha).abs().sum().item() &lt; 1.0e-5\n        assert log_add_exp(log_cumprod_alpha, log_1_min_cumprod_alpha).abs().sum().item() &lt; 1e-5\n        diff: Tensor = cast(Tensor, np.cumsum(log_alpha) - log_cumprod_alpha)\n        assert diff.abs().sum().item() &lt; 1.0e-5\n\n        # Convert to float32 and register buffers.\n        self.register_buffer(\"alphas\", alphas.float().to(device))\n        self.register_buffer(\"log_alpha\", log_alpha.float().to(device))\n        self.register_buffer(\"log_1_min_alpha\", log_1_min_alpha.float().to(device))\n        self.register_buffer(\"log_1_min_cumprod_alpha\", log_1_min_cumprod_alpha.float().to(device))\n        self.register_buffer(\"log_cumprod_alpha\", log_cumprod_alpha.float().to(device))\n        self.register_buffer(\"alphas_cumprod\", alphas_cumprod.float().to(device))\n        self.register_buffer(\"alphas_cumprod_prev\", alphas_cumprod_prev.float().to(device))\n        self.register_buffer(\"alphas_cumprod_next\", alphas_cumprod_next.float().to(device))\n        self.register_buffer(\"sqrt_alphas_cumprod\", sqrt_alphas_cumprod.float().to(device))\n        self.register_buffer(\n            \"sqrt_one_minus_alphas_cumprod\",\n            sqrt_one_minus_alphas_cumprod.float().to(device),\n        )\n        self.register_buffer(\"sqrt_recip_alphas_cumprod\", sqrt_recip_alphas_cumprod.float().to(device))\n        self.register_buffer(\n            \"sqrt_recipm1_alphas_cumprod\",\n            sqrt_recipm1_alphas_cumprod.float().to(device),\n        )\n\n        self.register_buffer(\"Lt_history\", torch.zeros(num_timesteps))\n        self.register_buffer(\"Lt_count\", torch.zeros(num_timesteps))\n\n    # Gaussian part\n    def gaussian_q_mean_variance(self, x_start: Tensor, t: Tensor) -&gt; tuple[Tensor, Tensor, Tensor]:\n        mean = extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n        variance = extract(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract(self.log_1_min_cumprod_alpha, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def gaussian_q_sample(self, x_start: Tensor, t: Tensor, noise: Tensor | None = None) -&gt; Tensor:\n        if noise is None:\n            noise = torch.randn_like(x_start)\n        assert noise.shape == x_start.shape\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n            + extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n        )\n\n    def gaussian_q_posterior_mean_variance(\n        self,\n        x_start: Tensor,\n        x_t: Tensor,\n        t: Tensor,\n    ) -&gt; tuple[Tensor, Tensor, Tensor]:\n        assert x_start.shape == x_t.shape\n        posterior_mean = (\n            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start\n            + extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n        assert (\n            posterior_mean.shape[0]\n            == posterior_variance.shape[0]\n            == posterior_log_variance_clipped.shape[0]\n            == x_start.shape[0]\n        )\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def gaussian_p_mean_variance(\n        self,\n        model_output: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        denoised_fn: Callable | None = None,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Tensor]:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        B, C = x.shape[:2]\n        assert t.shape == (B,)\n\n        model_variance = torch.cat(\n            [\n                self.posterior_variance[1].unsqueeze(0).to(x.device),\n                (1.0 - self.alphas)[1:],\n            ],\n            dim=0,\n        )\n        # model_variance = self.posterior_variance.to(x.device)\n        model_log_variance = torch.log(model_variance)\n\n        model_variance = extract(model_variance, t, x.shape)\n        model_log_variance = extract(model_log_variance, t, x.shape)\n\n        if self.gaussian_parametrization == \"eps\":\n            pred_xstart = self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n        elif self.gaussian_parametrization == \"x0\":\n            pred_xstart = model_output\n        else:\n            raise NotImplementedError\n\n        model_mean, _, _ = self.gaussian_q_posterior_mean_variance(x_start=pred_xstart, x_t=x, t=t)\n\n        assert model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape, (\n            f\"{model_mean.shape}, {model_log_variance.shape}, {pred_xstart.shape}, {x.shape}\"\n        )\n\n        return {\n            \"mean\": model_mean,\n            \"variance\": model_variance,\n            \"log_variance\": model_log_variance,\n            \"pred_xstart\": pred_xstart,\n        }\n\n    def _vb_terms_bpd(\n        self,\n        model_output: Tensor,\n        x_start: Tensor,\n        x_t: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Tensor]:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        (\n            true_mean,\n            _,\n            true_log_variance_clipped,\n        ) = self.gaussian_q_posterior_mean_variance(x_start=x_start, x_t=x_t, t=t)\n        out = self.gaussian_p_mean_variance(\n            model_output, x_t, t, clip_denoised=clip_denoised, model_kwargs=model_kwargs\n        )\n        kl = normal_kl(true_mean, true_log_variance_clipped, out[\"mean\"], out[\"log_variance\"])\n        kl = mean_flat(kl) / np.log(2.0)\n\n        decoder_nll = -discretized_gaussian_log_likelihood(\n            x_start, means=out[\"mean\"], log_scales=0.5 * out[\"log_variance\"]\n        )\n        assert decoder_nll.shape == x_start.shape\n        decoder_nll = mean_flat(decoder_nll) / np.log(2.0)\n\n        # At the first timestep return the decoder NLL,\n        # otherwise return KL(q(x_{t-1}|x_t,x_0) || p(x_{t-1}|x_t))\n        output = torch.where((t == 0), decoder_nll, kl)\n        return {\n            \"output\": output,\n            \"pred_xstart\": out[\"pred_xstart\"],\n            \"out_mean\": out[\"mean\"],\n            \"true_mean\": true_mean,\n        }\n\n    def _prior_gaussian(self, x_start: Tensor) -&gt; Tensor:\n        \"\"\"\n        Get the prior KL term for the variational lower-bound, measured in\n        bits-per-dim.\n\n        This term can't be optimized, as it only depends on the encoder.\n\n        :param x_start: the [N x C x ...] tensor of inputs.\n        :return: a batch of [N] KL values (in bits), one per batch element.\n        \"\"\"\n        batch_size = x_start.shape[0]\n        t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n        qt_mean, _, qt_log_variance = self.gaussian_q_mean_variance(x_start, t)\n        kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0)\n        return mean_flat(kl_prior) / np.log(2.0)\n\n    def _gaussian_loss(\n        self,\n        model_out: Tensor,\n        x_start: Tensor,\n        x_t: Tensor,\n        t: Tensor,\n        noise: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; Tensor:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        terms = {}\n        if self.gaussian_loss_type == \"mse\":\n            terms[\"loss\"] = mean_flat((noise - model_out) ** 2)\n        elif self.gaussian_loss_type == \"kl\":\n            terms[\"loss\"] = self._vb_terms_bpd(\n                model_output=model_out,\n                x_start=x_start,\n                x_t=x_t,\n                t=t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n            )[\"output\"]\n\n        return terms[\"loss\"]\n\n    def _predict_xstart_from_eps(self, x_t: Tensor, t: Tensor, eps: Tensor) -&gt; Tensor:\n        assert x_t.shape == eps.shape\n        return (\n            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n            - extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * eps\n        )\n\n    def _predict_eps_from_xstart(self, x_t: Tensor, t: Tensor, pred_xstart: Tensor) -&gt; Tensor:\n        return (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / extract(\n            self.sqrt_recipm1_alphas_cumprod, t, x_t.shape\n        )\n\n    def condition_mean(\n        self,\n        cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n        p_mean_var: dict[str, Tensor],\n        x: Tensor,\n        t: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; Tensor:\n        \"\"\"\n        Compute the mean for the previous step, given a function cond_fn that\n        computes the gradient of a conditional log probability with respect to\n        x. In particular, cond_fn computes grad(log(p(y|x))), and we want to\n        condition on y.\n\n        This uses the conditioning strategy from Sohl-Dickstein et al. (2015).\n        \"\"\"\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        gradient = cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n        return p_mean_var[\"mean\"].float() + p_mean_var[\"variance\"] * gradient.float()\n\n    def condition_score(\n        self,\n        cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n        p_mean_var: dict[str, Tensor],\n        x: Tensor,\n        t: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Tensor]:\n        \"\"\"\n        Compute what the p_mean_variance output would have been, should the\n        model's score function be conditioned by cond_fn.\n\n        See condition_mean() for details on cond_fn.\n\n        Unlike condition_mean(), this instead uses the conditioning strategy\n        from Song et al (2020).\n        \"\"\"\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        alpha_bar = extract(self.alphas_cumprod, t, x.shape)\n\n        eps = self._predict_eps_from_xstart(x, t, p_mean_var[\"pred_xstart\"])\n        eps = eps - (1 - alpha_bar).sqrt() * cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n\n        out = p_mean_var.copy()\n        out[\"pred_xstart\"] = self._predict_xstart_from_eps(x, t, eps)\n        out[\"mean\"], _, _ = self.q_posterior_mean_variance(x_start=out[\"pred_xstart\"], x_t=x, t=t)\n        return out\n\n    def gaussian_p_sample(\n        self,\n        model_out: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        denoised_fn: Callable | None = None,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; dict[str, Tensor]:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        out = self.gaussian_p_mean_variance(\n            model_out,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n        )\n        noise = torch.randn_like(x)\n        nonzero_mask = (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))  # no noise when t == 0\n\n        if cond_fn is not None:\n            out[\"mean\"] = self.condition_mean(cond_fn, out, x, t, model_kwargs=model_kwargs)\n\n        sample = out[\"mean\"] + nonzero_mask * torch.exp(0.5 * out[\"log_variance\"]) * noise\n        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    # Multinomial part\n\n    def multinomial_kl(self, log_prob1: Tensor, log_prob2: Tensor) -&gt; Tensor:\n        return (log_prob1.exp() * (log_prob1 - log_prob2)).sum(dim=1)\n\n    def q_pred_one_timestep(self, log_x_t: Tensor, t: Tensor) -&gt; Tensor:\n        log_alpha_t = extract(self.log_alpha, t, log_x_t.shape)\n        log_1_min_alpha_t = extract(self.log_1_min_alpha, t, log_x_t.shape)\n\n        # alpha_t * E[xt] + (1 - alpha_t) 1 / K\n        return log_add_exp(\n            log_x_t + log_alpha_t,\n            log_1_min_alpha_t - torch.log(self.num_classes_expanded),\n        )\n\n    def q_pred(self, log_x_start: Tensor, t: Tensor) -&gt; Tensor:\n        log_cumprod_alpha_t = extract(self.log_cumprod_alpha, t, log_x_start.shape)\n        log_1_min_cumprod_alpha = extract(self.log_1_min_cumprod_alpha, t, log_x_start.shape)\n\n        return log_add_exp(\n            log_x_start + log_cumprod_alpha_t,\n            log_1_min_cumprod_alpha - torch.log(self.num_classes_expanded),\n        )\n\n    def predict_start(self, model_out: Tensor, log_x_t: Tensor, t: Tensor, out_dict: dict[str, Tensor]) -&gt; Tensor:\n        # model_out = self._denoise_fn(x_t, t.to(x_t.device), **out_dict)\n\n        assert model_out.size(0) == log_x_t.size(0)\n        assert self.num_classes is not None\n        assert model_out.size(1) == self.num_classes.sum(), f\"{model_out.size()}\"\n\n        log_pred = torch.empty_like(model_out)\n        for ix in self.slices_for_classes:\n            log_pred[:, ix] = F.log_softmax(model_out[:, ix], dim=1)\n        return log_pred\n\n    def q_posterior(self, log_x_start: Tensor, log_x_t: Tensor, t: Tensor) -&gt; Tensor:\n        # q(xt-1 | xt, x0) = q(xt | xt-1, x0) * q(xt-1 | x0) / q(xt | x0)\n        # where q(xt | xt-1, x0) = q(xt | xt-1).\n\n        # EV_log_qxt_x0 = self.q_pred(log_x_start, t)\n\n        # print('sum exp', EV_log_qxt_x0.exp().sum(1).mean())\n        # assert False\n\n        # log_qxt_x0 = (log_x_t.exp() * EV_log_qxt_x0).sum(dim=1)\n        t_minus_1 = t - 1\n        # Remove negative values, will not be used anyway for final decoder\n        t_minus_1 = torch.where(t_minus_1 &lt; 0, torch.zeros_like(t_minus_1), t_minus_1)\n        log_EV_qxtmin_x0 = self.q_pred(log_x_start, t_minus_1)\n\n        num_axes = (1,) * (len(log_x_start.size()) - 1)\n        t_broadcast = t.to(log_x_start.device).view(-1, *num_axes) * torch.ones_like(log_x_start)\n        log_EV_qxtmin_x0 = torch.where(t_broadcast == 0, log_x_start, log_EV_qxtmin_x0.to(torch.float32))\n\n        # unnormed_logprobs = log_EV_qxtmin_x0 +\n        #                     log q_pred_one_timestep(x_t, t)\n        # Note: _NOT_ x_tmin1, which is how the formula is typically used!!!\n        # Not very easy to see why this is true. But it is :)\n        unnormed_logprobs = log_EV_qxtmin_x0 + self.q_pred_one_timestep(log_x_t, t)\n\n        return unnormed_logprobs - sliced_logsumexp(unnormed_logprobs, self.offsets)\n\n    def p_pred(self, model_out: Tensor, log_x: Tensor, t: Tensor, out_dict: dict[str, Tensor]) -&gt; Tensor:\n        if self.parametrization == \"x0\":\n            log_x_recon = self.predict_start(model_out, log_x, t=t, out_dict=out_dict)\n            log_model_pred = self.q_posterior(log_x_start=log_x_recon, log_x_t=log_x, t=t)\n        elif self.parametrization == \"direct\":\n            log_model_pred = self.predict_start(model_out, log_x, t=t, out_dict=out_dict)\n        else:\n            raise ValueError\n        return log_model_pred\n\n    @torch.no_grad()\n    def p_sample(self, model_out: Tensor, log_x: Tensor, t: Tensor, out_dict: dict[str, Tensor]) -&gt; Tensor:\n        model_log_prob = self.p_pred(model_out, log_x=log_x, t=t, out_dict=out_dict)\n        return self.log_sample_categorical(model_log_prob)\n\n    # Dead code\n    # @torch.no_grad()\n    # def p_sample_loop(self, shape, out_dict):\n    #     b = shape[0]\n    #     # start with random normal image.\n    #     img = torch.randn(shape, device=device)\n\n    #     for i in reversed(range(1, self.num_timesteps)):\n    #         img = self.p_sample(img, torch.full((b,), i, device=self.device, dtype=torch.long), out_dict)\n    #     return img\n\n    # @torch.no_grad()\n    # def _sample(self, image_size, out_dict, batch_size=16):\n    #     return self.p_sample_loop((batch_size, 3, image_size, image_size), out_dict)\n\n    # Dead code\n    # @torch.no_grad()\n    # def interpolate(self, x1: Tensor, x2: Tensor, t: Tensor | None = None, lam: float = 0.5) -&gt; Tensor:\n    #     b, *_, device = *x1.shape, x1.device\n    #     t = default(t, self.num_timesteps - 1)\n\n    #     assert x1.shape == x2.shape\n\n    #     t_batched = torch.stack([torch.tensor(t, device=device)] * b)\n    #     xt1, xt2 = map(lambda x: self.q_sample(x, t=t_batched), (x1, x2))\n\n    #     img = (1 - lam) * xt1 + lam * xt2\n    #     for i in reversed(range(0, t)):\n    #         img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long))\n\n    #     return img\n\n    def log_sample_categorical(self, logits: Tensor) -&gt; Tensor:\n        full_sample = []\n        for i in range(len(self.num_classes)):\n            one_class_logits = logits[:, self.slices_for_classes[i]]\n            uniform = torch.rand_like(one_class_logits)\n            gumbel_noise = -torch.log(-torch.log(uniform + 1e-30) + 1e-30)\n            sample = (gumbel_noise + one_class_logits).argmax(dim=1)\n            full_sample.append(sample.unsqueeze(1))\n        full_sample_tensor = torch.cat(full_sample, dim=1)\n        return index_to_log_onehot(full_sample_tensor, torch.from_numpy(self.num_classes))\n\n    def q_sample(self, log_x_start: Tensor, t: Tensor) -&gt; Tensor:\n        log_EV_qxt_x0 = self.q_pred(log_x_start, t)\n        # ruff: noqa: N806\n        return self.log_sample_categorical(log_EV_qxt_x0)\n\n    # Dead code\n    # def nll(self, log_x_start, out_dict):\n    #     b = log_x_start.size(0)\n    #     device = log_x_start.device\n    #     loss = 0\n    #     for t in range(0, self.num_timesteps):\n    #         t_array = (torch.ones(b, device=device) * t).long()\n\n    #         kl = self.compute_Lt(\n    #             log_x_start=log_x_start,\n    #             log_x_t=self.q_sample(log_x_start=log_x_start, t=t_array),\n    #             t=t_array,\n    #             out_dict=out_dict,\n    #         )\n\n    #         loss += kl\n\n    #     loss += self.kl_prior(log_x_start)\n\n    #     return loss\n\n    def kl_prior(self, log_x_start: Tensor) -&gt; Tensor:\n        b = log_x_start.size(0)\n        device = log_x_start.device\n        ones = torch.ones(b, device=device).long()\n\n        log_qxT_prob = self.q_pred(log_x_start, t=(self.num_timesteps - 1) * ones)\n        # ruff: noqa: N806\n        log_half_prob = -torch.log(self.num_classes_expanded * torch.ones_like(log_qxT_prob))\n\n        kl_prior = self.multinomial_kl(log_qxT_prob, log_half_prob)\n        return sum_except_batch(kl_prior)\n\n    def compute_Lt(\n        # ruff: noqa: N802\n        self,\n        model_out: Tensor,\n        log_x_start: Tensor,\n        log_x_t: Tensor,\n        t: Tensor,\n        out_dict: dict[str, Tensor],\n        detach_mean: bool = False,\n    ) -&gt; Tensor:\n        log_true_prob = self.q_posterior(log_x_start=log_x_start, log_x_t=log_x_t, t=t)\n        log_model_prob = self.p_pred(model_out, log_x=log_x_t, t=t, out_dict=out_dict)\n\n        if detach_mean:\n            log_model_prob = log_model_prob.detach()\n\n        kl = self.multinomial_kl(log_true_prob, log_model_prob)\n        kl = sum_except_batch(kl)\n\n        decoder_nll = -log_categorical(log_x_start, log_model_prob)\n        decoder_nll = sum_except_batch(decoder_nll)\n\n        mask = (t == torch.zeros_like(t)).float()\n        return mask * decoder_nll + (1.0 - mask) * kl\n\n    def sample_time(self, b: int, device: torch.device, method: str = \"uniform\") -&gt; tuple[Tensor, Tensor]:\n        if method == \"importance\":\n            if not (self.Lt_count &gt; 10).all():\n                return self.sample_time(b, device, method=\"uniform\")\n\n            Lt_sqrt = torch.sqrt(self.Lt_history + 1e-10) + 0.0001\n            # ruff: noqa: N806\n            Lt_sqrt[0] = Lt_sqrt[1]  # Overwrite decoder term with L1.\n            pt_all = (Lt_sqrt / Lt_sqrt.sum()).to(device)\n\n            t = torch.multinomial(pt_all, num_samples=b, replacement=True).to(device)\n\n            pt = pt_all.gather(dim=0, index=t)\n\n            return t, pt\n\n        if method == \"uniform\":\n            t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n\n            pt = torch.ones_like(t).float() / self.num_timesteps\n            return t, pt\n        raise ValueError\n\n    def _multinomial_loss(\n        self,\n        model_out: Tensor,\n        log_x_start: Tensor,\n        log_x_t: Tensor,\n        t: Tensor,\n        pt: Tensor,\n        out_dict: dict[str, Tensor],\n    ) -&gt; Tensor:\n        if self.multinomial_loss_type == \"vb_stochastic\":\n            kl = self.compute_Lt(model_out, log_x_start, log_x_t, t, out_dict)\n            kl_prior = self.kl_prior(log_x_start)\n            # Upweigh loss term of the kl\n            return kl / pt + kl_prior\n\n        if self.multinomial_loss_type == \"vb_all\":\n            # Expensive, dont do it ;).\n            # DEPRECATED\n            # return -self.nll(log_x_start)\n            raise ValueError(\"multinomial_loss_type == 'vb_all' is deprecated.\")\n        raise ValueError\n\n    # Dead code\n    # def log_prob(self, x, out_dict):\n    #     b, device = x.size(0), x.device\n    #     if self.training:\n    #         return self._multinomial_loss(x, out_dict)\n\n    #     log_x_start = index_to_log_onehot(x, self.num_classes)\n\n    #     t, pt = self.sample_time(b, device, \"importance\")\n\n    #     kl = self.compute_Lt(log_x_start, self.q_sample(log_x_start=log_x_start, t=t), t, out_dict)\n\n    #     kl_prior = self.kl_prior(log_x_start)\n\n    #     # Upweigh loss term of the kl\n    #     loss = kl / pt + kl_prior\n\n    #     return -loss\n\n    def mixed_loss(self, x: Tensor, out_dict: dict[str, Tensor]) -&gt; tuple[Tensor, Tensor]:\n        b = x.shape[0]\n        device = x.device\n        t, pt = self.sample_time(b, device, \"uniform\")\n\n        x_num = x[:, : self.num_numerical_features]\n        x_cat = x[:, self.num_numerical_features :]\n\n        x_num_t = x_num\n        log_x_cat_t = x_cat\n        if x_num.shape[1] &gt; 0:\n            noise = torch.randn_like(x_num)\n            x_num_t = self.gaussian_q_sample(x_num, t, noise=noise)\n        if x_cat.shape[1] &gt; 0:\n            log_x_cat = index_to_log_onehot(x_cat.long(), torch.from_numpy(self.num_classes))\n            log_x_cat_t = self.q_sample(log_x_start=log_x_cat, t=t)\n\n        x_in = torch.cat([x_num_t, log_x_cat_t], dim=1)\n\n        model_out = self._denoise_fn(x_in, t, **out_dict)\n\n        model_out_num = model_out[:, : self.num_numerical_features]\n        model_out_cat = model_out[:, self.num_numerical_features :]\n\n        loss_multi = torch.zeros((1,)).float()\n        loss_gauss = torch.zeros((1,)).float()\n        if x_cat.shape[1] &gt; 0:\n            loss_multi = self._multinomial_loss(model_out_cat, log_x_cat, log_x_cat_t, t, pt, out_dict) / len(\n                self.num_classes\n            )\n\n        if x_num.shape[1] &gt; 0:\n            loss_gauss = self._gaussian_loss(model_out_num, x_num, x_num_t, t, noise)\n\n        # loss_multi = torch.where(out_dict['y'] == 1, loss_multi, 2 * loss_multi)\n        # loss_gauss = torch.where(out_dict['y'] == 1, loss_gauss, 2 * loss_gauss)\n\n        return loss_multi.mean(), loss_gauss.mean()\n\n    @torch.no_grad()\n    def mixed_elbo(self, x0, out_dict):\n        b = x0.size(0)\n        device = x0.device\n\n        x_num = x0[:, : self.num_numerical_features]\n        x_cat = x0[:, self.num_numerical_features :]\n        has_cat = x_cat.shape[1] &gt; 0\n        if has_cat:\n            log_x_cat = index_to_log_onehot(x_cat.long(), torch.from_numpy(self.num_classes)).to(device)\n\n        gaussian_loss = []\n        xstart_mse = []\n        mse = []\n        # mu_mse = []\n        out_mean = []\n        true_mean = []\n        multinomial_loss = []\n        for t in range(self.num_timesteps):\n            t_array = (torch.ones(b, device=device) * t).long()\n            noise = torch.randn_like(x_num)\n\n            x_num_t = self.gaussian_q_sample(x_start=x_num, t=t_array, noise=noise)\n            log_x_cat_t = self.q_sample(log_x_start=log_x_cat, t=t_array) if has_cat else x_cat\n\n            model_out = self._denoise_fn(torch.cat([x_num_t, log_x_cat_t], dim=1), t_array, **out_dict)\n\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n\n            kl = torch.tensor([0.0])\n            if has_cat:\n                kl = self.compute_Lt(\n                    model_out=model_out_cat,\n                    log_x_start=log_x_cat,\n                    log_x_t=log_x_cat_t,\n                    t=t_array,\n                    out_dict=out_dict,\n                )\n\n            out = self._vb_terms_bpd(\n                model_out_num,\n                x_start=x_num,\n                x_t=x_num_t,\n                t=t_array,\n                clip_denoised=False,\n            )\n\n            multinomial_loss.append(kl)\n            gaussian_loss.append(out[\"output\"])\n            xstart_mse.append(mean_flat((out[\"pred_xstart\"] - x_num) ** 2))\n            # mu_mse.append(mean_flat(out[\"mean_mse\"]))\n            out_mean.append(mean_flat(out[\"out_mean\"]))\n            true_mean.append(mean_flat(out[\"true_mean\"]))\n\n            eps = self._predict_eps_from_xstart(x_num_t, t_array, out[\"pred_xstart\"])\n            mse.append(mean_flat((eps - noise) ** 2))\n\n        gaussian_loss_tensor = torch.stack(gaussian_loss, dim=1)\n        multinomial_loss_tensor = torch.stack(multinomial_loss, dim=1)\n        xstart_mse_tensor = torch.stack(xstart_mse, dim=1)\n        mse_tensor = torch.stack(mse, dim=1)\n        # mu_mse = torch.stack(mu_mse, dim=1)\n        out_mean_tensor = torch.stack(out_mean, dim=1)\n        true_mean_tensor = torch.stack(true_mean, dim=1)\n\n        prior_gauss = self._prior_gaussian(x_num)\n\n        prior_multin = torch.tensor([0.0])\n        if has_cat:\n            prior_multin = self.kl_prior(log_x_cat)\n\n        total_gauss = gaussian_loss_tensor.sum(dim=1) + prior_gauss\n        total_multin = multinomial_loss_tensor.sum(dim=1) + prior_multin\n        return {\n            \"total_gaussian\": total_gauss,\n            \"total_multinomial\": total_multin,\n            \"losses_gaussian\": gaussian_loss_tensor,\n            \"losses_multinimial\": multinomial_loss_tensor,\n            \"xstart_mse\": xstart_mse_tensor,\n            \"mse\": mse_tensor,\n            # \"mu_mse\": mu_mse\n            \"out_mean\": out_mean_tensor,\n            \"true_mean\": true_mean_tensor,\n        }\n\n    @torch.no_grad()\n    def gaussian_ddim_step(\n        self,\n        model_out_num: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        denoised_fn: Callable | None = None,\n        eta: float = 0.0,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; Tensor:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        out = self.gaussian_p_mean_variance(\n            model_out_num,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=None,\n        )\n\n        if cond_fn is not None:\n            out = self.condition_score(cond_fn, out, x, t, model_kwargs=model_kwargs)\n\n        eps = self._predict_eps_from_xstart(x, t, out[\"pred_xstart\"])\n\n        alpha_bar = extract(self.alphas_cumprod, t, x.shape)\n        alpha_bar_prev = extract(self.alphas_cumprod_prev, t, x.shape)\n        sigma = eta * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar)) * torch.sqrt(1 - alpha_bar / alpha_bar_prev)\n\n        noise = torch.randn_like(x)\n        mean_pred = out[\"pred_xstart\"] * torch.sqrt(alpha_bar_prev) + torch.sqrt(1 - alpha_bar_prev - sigma**2) * eps\n        nonzero_mask = (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))  # no noise when t == 0\n        return mean_pred + nonzero_mask * sigma * noise\n\n    @torch.no_grad()\n    def gaussian_ddim_sample(self, noise, T, out_dict, eta=0.0, model_kwargs=None, cond_fn=None):\n        # ruff: noqa: D102, N803\n        x = noise\n        b = x.shape[0]\n        device = x.device\n        for t in reversed(range(T)):\n            print(f\"Sample timestep {t:4d}\", end=\"\\r\")\n            t_array = (torch.ones(b, device=device) * t).long()\n            out_num = self._denoise_fn(x, t_array, **out_dict)\n            x = self.gaussian_ddim_step(out_num, x, t_array, model_kwargs=model_kwargs, cond_fn=cond_fn)\n        print()\n        return x\n\n    @torch.no_grad()\n    def gaussian_ddim_reverse_step(\n        self,\n        model_out_num: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        eta: float = 0.0,\n    ) -&gt; Tensor:\n        # ruff: noqa: D102\n        assert eta == 0.0, \"Eta must be zero.\"\n        out = self.gaussian_p_mean_variance(\n            model_out_num,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=None,\n            model_kwargs=None,\n        )\n\n        eps = (extract(self.sqrt_recip_alphas_cumprod, t, x.shape) * x - out[\"pred_xstart\"]) / extract(\n            self.sqrt_recipm1_alphas_cumprod, t, x.shape\n        )\n        alpha_bar_next = extract(self.alphas_cumprod_next, t, x.shape)\n\n        return out[\"pred_xstart\"] * torch.sqrt(alpha_bar_next) + torch.sqrt(1 - alpha_bar_next) * eps\n\n    @torch.no_grad()\n    def gaussian_ddim_reverse_sample(\n        self,\n        x,\n        T,\n        # ruff: noqa: N803\n        out_dict,\n    ):\n        # ruff: noqa: D102\n        b = x.shape[0]\n        device = x.device\n        for t in range(T):\n            print(f\"Reverse timestep {t:4d}\", end=\"\\r\")\n            t_array = (torch.ones(b, device=device) * t).long()\n            out_num = self._denoise_fn(x, t_array, **out_dict)\n            x = self.gaussian_ddim_reverse_step(out_num, x, t_array, eta=0.0)\n        print()\n\n        return x\n\n    @torch.no_grad()\n    def multinomial_ddim_step(\n        self,\n        model_out_cat: Tensor,\n        log_x_t: Tensor,\n        t: Tensor,\n        out_dict: dict[str, Tensor],\n        eta: float = 0.0,\n    ) -&gt; Tensor:\n        # ruff: noqa: D102\n        # not ddim, essentially\n        log_x0 = self.predict_start(model_out_cat, log_x_t=log_x_t, t=t, out_dict=out_dict)\n\n        alpha_bar = extract(self.alphas_cumprod, t, log_x_t.shape)\n        alpha_bar_prev = extract(self.alphas_cumprod_prev, t, log_x_t.shape)\n        sigma = eta * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar)) * torch.sqrt(1 - alpha_bar / alpha_bar_prev)\n\n        coef1 = sigma\n        coef2 = alpha_bar_prev - sigma * alpha_bar\n        coef3 = 1 - coef1 - coef2\n\n        log_ps = torch.stack(\n            [\n                torch.log(coef1) + log_x_t,\n                torch.log(coef2) + log_x0,\n                torch.log(coef3) - torch.log(self.num_classes_expanded),\n            ],\n            dim=2,\n        )\n\n        log_prob = torch.logsumexp(log_ps, dim=2)\n\n        return self.log_sample_categorical(log_prob)\n\n    @torch.no_grad()\n    def sample_ddim(\n        self,\n        num_samples: int,\n        y_dist: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; tuple[Tensor, dict[str, Tensor]]:\n        # ruff: noqa: D102\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        b = num_samples\n        z_norm = torch.randn((b, self.num_numerical_features), device=self.device)\n\n        assert self.num_classes is not None\n        has_cat = self.num_classes[0] != 0\n        log_z = torch.zeros((b, 0), device=self.device).float()\n        if has_cat:\n            uniform_logits = torch.zeros((b, len(self.num_classes_expanded)), device=self.device)\n            log_z = self.log_sample_categorical(uniform_logits)\n\n        y = torch.multinomial(y_dist, num_samples=b, replacement=True)\n        out_dict = {\"y\": y.long().to(self.device)}\n        for i in reversed(range(0, self.num_timesteps)):\n            print(f\"Sample timestep {i:4d}\", end=\"\\r\")\n            t = torch.full((b,), i, device=self.device, dtype=torch.long)\n            model_out = self._denoise_fn(torch.cat([z_norm, log_z], dim=1).float(), t, **out_dict)\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n            z_norm = self.gaussian_ddim_step(\n                model_out_num,\n                z_norm,\n                t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n                cond_fn=cond_fn,\n            )\n            if has_cat:\n                log_z = self.multinomial_ddim_step(model_out_cat, log_z, t, out_dict)\n\n        print()\n        z_ohe = torch.exp(log_z).round()\n        z_cat = log_z\n        if has_cat:\n            z_cat = ohe_to_categories(z_ohe, torch.from_numpy(self.num_classes))\n        sample = torch.cat([z_norm, z_cat], dim=1).cpu()\n        return sample, out_dict\n\n    @torch.no_grad()\n    def conditional_sample(\n        self,\n        ys: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; tuple[Tensor, dict[str, Tensor]]:\n        # ruff: noqa: D102\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        b = len(ys)\n        z_norm = torch.randn((b, self.num_numerical_features), device=self.device)\n        assert self.num_classes is not None\n        has_cat = self.num_classes[0] != 0\n        log_z = torch.zeros((b, 0), device=self.device).float()\n\n        out_dict = {\"y\": ys.long().to(self.device)}\n        for i in reversed(range(0, self.num_timesteps)):\n            print(f\"Sample timestep {i:4d}\", end=\"\\r\")\n            t = torch.full((b,), i, device=self.device, dtype=torch.long)\n            model_out = self._denoise_fn(torch.cat([z_norm, log_z], dim=1).float(), t, **out_dict)\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n            z_norm = self.gaussian_p_sample(\n                model_out_num,\n                z_norm,\n                t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n                cond_fn=cond_fn,\n            )[\"sample\"]\n            if has_cat:\n                log_z = self.p_sample(model_out_cat, log_z, t, out_dict)\n\n        print()\n        z_ohe = torch.exp(log_z).round()\n        z_cat = log_z\n        if has_cat:\n            z_cat = ohe_to_categories(z_ohe, torch.from_numpy(self.num_classes))\n        sample = torch.cat([z_norm, z_cat], dim=1).cpu()\n        return sample, out_dict\n\n    @torch.no_grad()\n    def sample(\n        self,\n        num_samples: int,\n        y_dist: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; tuple[Tensor, dict[str, Tensor]]:\n        # ruff: noqa: D102\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        b = num_samples\n        z_norm = torch.randn((b, self.num_numerical_features), device=self.device)\n\n        assert self.num_classes is not None\n        has_cat = self.num_classes[0] != 0\n        log_z = torch.zeros((b, 0), device=self.device).float()\n        if has_cat:\n            uniform_logits = torch.zeros((b, len(self.num_classes_expanded)), device=self.device)\n            log_z = self.log_sample_categorical(uniform_logits)\n\n        y = torch.multinomial(y_dist, num_samples=b, replacement=True)\n        out_dict = {\"y\": y.long().to(self.device)}\n        for i in reversed(range(0, self.num_timesteps)):\n            print(f\"Sample timestep {i:4d}\", end=\"\\r\")\n            t = torch.full((b,), i, device=self.device, dtype=torch.long)\n            model_out = self._denoise_fn(torch.cat([z_norm, log_z], dim=1).float(), t, **out_dict)\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n            z_norm = self.gaussian_p_sample(\n                model_out_num,\n                z_norm,\n                t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n                cond_fn=cond_fn,\n            )[\"sample\"]\n            if has_cat:\n                log_z = self.p_sample(model_out_cat, log_z, t, out_dict)\n\n        print()\n        z_ohe = torch.exp(log_z).round()\n        z_cat = log_z\n        if has_cat:\n            z_cat = ohe_to_categories(z_ohe, torch.from_numpy(self.num_classes))\n        sample = torch.cat([z_norm, z_cat], dim=1).cpu()\n        return sample, out_dict\n\n    def sample_all(\n        self,\n        num_samples,\n        batch_size,\n        y_dist,\n        ddim=False,\n        model_kwargs=None,\n        cond_fn=None,\n    ):\n        # ruff: noqa: D102\n        if ddim:\n            print(\"Sample using DDIM.\")\n            sample_fn = self.sample_ddim\n        else:\n            sample_fn = self.sample\n\n        b = batch_size\n\n        all_y = []\n        all_samples = []\n        num_generated = 0\n        while num_generated &lt; num_samples:\n            sample, out_dict = sample_fn(b, y_dist, model_kwargs=model_kwargs, cond_fn=cond_fn)\n            mask_nan = torch.any(sample.isnan(), dim=1)\n            sample = sample[~mask_nan]\n            out_dict[\"y\"] = out_dict[\"y\"][~mask_nan]\n\n            all_samples.append(sample)\n            all_y.append(out_dict[\"y\"].cpu())\n            if sample.shape[0] != b:\n                raise FoundNANsError\n            num_generated += sample.shape[0]\n\n        x_gen = torch.cat(all_samples, dim=0)[:num_samples]\n        y_gen = torch.cat(all_y, dim=0)[:num_samples]\n\n        return x_gen, y_gen\n</code></pre> <code></code> condition_mean \u00b6 <pre><code>condition_mean(\n    cond_fn, p_mean_var, x, t, model_kwargs=None\n)\n</code></pre> <p>Compute the mean for the previous step, given a function cond_fn that computes the gradient of a conditional log probability with respect to x. In particular, cond_fn computes grad(log(p(y|x))), and we want to condition on y.</p> <p>This uses the conditioning strategy from Sohl-Dickstein et al. (2015).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def condition_mean(\n    self,\n    cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n    p_mean_var: dict[str, Tensor],\n    x: Tensor,\n    t: Tensor,\n    model_kwargs: dict[str, Any] | None = None,\n) -&gt; Tensor:\n    \"\"\"\n    Compute the mean for the previous step, given a function cond_fn that\n    computes the gradient of a conditional log probability with respect to\n    x. In particular, cond_fn computes grad(log(p(y|x))), and we want to\n    condition on y.\n\n    This uses the conditioning strategy from Sohl-Dickstein et al. (2015).\n    \"\"\"\n    if model_kwargs is None:\n        model_kwargs = {}\n\n    gradient = cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n    return p_mean_var[\"mean\"].float() + p_mean_var[\"variance\"] * gradient.float()\n</code></pre> <code></code> condition_score \u00b6 <pre><code>condition_score(\n    cond_fn, p_mean_var, x, t, model_kwargs=None\n)\n</code></pre> <p>Compute what the p_mean_variance output would have been, should the model's score function be conditioned by cond_fn.</p> <p>See condition_mean() for details on cond_fn.</p> <p>Unlike condition_mean(), this instead uses the conditioning strategy from Song et al (2020).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def condition_score(\n    self,\n    cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n    p_mean_var: dict[str, Tensor],\n    x: Tensor,\n    t: Tensor,\n    model_kwargs: dict[str, Any] | None = None,\n) -&gt; dict[str, Tensor]:\n    \"\"\"\n    Compute what the p_mean_variance output would have been, should the\n    model's score function be conditioned by cond_fn.\n\n    See condition_mean() for details on cond_fn.\n\n    Unlike condition_mean(), this instead uses the conditioning strategy\n    from Song et al (2020).\n    \"\"\"\n    if model_kwargs is None:\n        model_kwargs = {}\n\n    alpha_bar = extract(self.alphas_cumprod, t, x.shape)\n\n    eps = self._predict_eps_from_xstart(x, t, p_mean_var[\"pred_xstart\"])\n    eps = eps - (1 - alpha_bar).sqrt() * cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n\n    out = p_mean_var.copy()\n    out[\"pred_xstart\"] = self._predict_xstart_from_eps(x, t, eps)\n    out[\"mean\"], _, _ = self.q_posterior_mean_variance(x_start=out[\"pred_xstart\"], x_t=x, t=t)\n    return out\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.get_named_beta_schedule","title":"get_named_beta_schedule","text":"<pre><code>get_named_beta_schedule(\n    schedule_name, num_diffusion_timesteps\n)\n</code></pre> <p>Get a pre-defined beta schedule for the given name. The beta schedule library consists of beta schedules which remain similar in the limit of num_diffusion_timesteps. Beta schedules may be added, but should not be removed or changed once they are committed to maintain backwards compatibility.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def get_named_beta_schedule(schedule_name: str, num_diffusion_timesteps: int) -&gt; np.ndarray:\n    \"\"\"\n    Get a pre-defined beta schedule for the given name.\n    The beta schedule library consists of beta schedules which remain similar\n    in the limit of num_diffusion_timesteps.\n    Beta schedules may be added, but should not be removed or changed once\n    they are committed to maintain backwards compatibility.\n    \"\"\"\n    if schedule_name == \"linear\":\n        # Linear schedule from Ho et al, extended to work for any number of\n        # diffusion steps.\n        scale = 1000 / num_diffusion_timesteps\n        beta_start = scale * 0.0001\n        beta_end = scale * 0.02\n        return np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64)\n    if schedule_name == \"cosine\":\n        return betas_for_alpha_bar(\n            num_diffusion_timesteps,\n            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n        )\n    raise NotImplementedError(f\"unknown beta schedule: {schedule_name}\")\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.betas_for_alpha_bar","title":"betas_for_alpha_bar","text":"<pre><code>betas_for_alpha_bar(\n    num_diffusion_timesteps, alpha_bar, max_beta=0.999\n)\n</code></pre> <p>Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of (1-beta) over time from t = [0,1]. :param num_diffusion_timesteps: the number of betas to produce. :param alpha_bar: a lambda that takes an argument t from 0 to 1 and                   produces the cumulative product of (1-beta) up to that                   part of the diffusion process. :param max_beta: the maximum beta to use; use values lower than 1 to                  prevent singularities.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def betas_for_alpha_bar(num_diffusion_timesteps: int, alpha_bar: Callable, max_beta: float = 0.999) -&gt; np.ndarray:\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function,\n    which defines the cumulative product of (1-beta) over time from t = [0,1].\n    :param num_diffusion_timesteps: the number of betas to produce.\n    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n                      produces the cumulative product of (1-beta) up to that\n                      part of the diffusion process.\n    :param max_beta: the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n    \"\"\"\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return np.array(betas)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model","title":"model","text":""},{"location":"api/#midst_toolkit.models.clavaddpm.model.ScheduleSampler","title":"ScheduleSampler","text":"<p>               Bases: <code>ABC</code></p> <p>A distribution over timesteps in the diffusion process, intended to reduce variance of the objective.</p> <p>By default, samplers perform unbiased importance sampling, in which the objective's mean is unchanged. However, subclasses may override sample() to change how the resampled terms are reweighted, allowing for actual changes in the objective.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class ScheduleSampler(ABC):\n    \"\"\"\n    A distribution over timesteps in the diffusion process, intended to reduce\n    variance of the objective.\n\n    By default, samplers perform unbiased importance sampling, in which the\n    objective's mean is unchanged.\n    However, subclasses may override sample() to change how the resampled\n    terms are reweighted, allowing for actual changes in the objective.\n    \"\"\"\n\n    @abstractmethod\n    def weights(self) -&gt; Tensor:\n        \"\"\"\n        Get a numpy array of weights, one per diffusion step.\n\n        The weights needn't be normalized, but must be positive.\n        \"\"\"\n\n    def sample(self, batch_size: int, device: str) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"\n        Importance-sample timesteps for a batch.\n\n        :param batch_size: the number of timesteps.\n        :param device: the torch device to save to.\n        :return: a tuple (timesteps, weights):\n                 - timesteps: a tensor of timestep indices.\n                 - weights: a tensor of weights to scale the resulting losses.\n        \"\"\"\n        w = self.weights().cpu().numpy()\n        p = w / np.sum(w)\n        indices_np = np.random.choice(len(p), size=(batch_size,), p=p)\n        indices = torch.from_numpy(indices_np).long().to(device)\n        weights_np = 1 / (len(p) * p[indices_np])\n        weights = torch.from_numpy(weights_np).float().to(device)\n        return indices, weights\n</code></pre> <code></code> weights <code>abstractmethod</code> \u00b6 <pre><code>weights()\n</code></pre> <p>Get a numpy array of weights, one per diffusion step.</p> <p>The weights needn't be normalized, but must be positive.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>@abstractmethod\ndef weights(self) -&gt; Tensor:\n    \"\"\"\n    Get a numpy array of weights, one per diffusion step.\n\n    The weights needn't be normalized, but must be positive.\n    \"\"\"\n</code></pre> <code></code> sample \u00b6 <pre><code>sample(batch_size, device)\n</code></pre> <p>Importance-sample timesteps for a batch.</p> <p>:param batch_size: the number of timesteps. :param device: the torch device to save to. :return: a tuple (timesteps, weights):          - timesteps: a tensor of timestep indices.          - weights: a tensor of weights to scale the resulting losses.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def sample(self, batch_size: int, device: str) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"\n    Importance-sample timesteps for a batch.\n\n    :param batch_size: the number of timesteps.\n    :param device: the torch device to save to.\n    :return: a tuple (timesteps, weights):\n             - timesteps: a tensor of timestep indices.\n             - weights: a tensor of weights to scale the resulting losses.\n    \"\"\"\n    w = self.weights().cpu().numpy()\n    p = w / np.sum(w)\n    indices_np = np.random.choice(len(p), size=(batch_size,), p=p)\n    indices = torch.from_numpy(indices_np).long().to(device)\n    weights_np = 1 / (len(p) * p[indices_np])\n    weights = torch.from_numpy(weights_np).float().to(device)\n    return indices, weights\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.LossAwareSampler","title":"LossAwareSampler","text":"<p>               Bases: <code>ScheduleSampler</code></p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class LossAwareSampler(ScheduleSampler):\n    def update_with_local_losses(self, local_ts: Tensor, local_losses: Tensor) -&gt; None:\n        \"\"\"\n        Update the reweighting using losses from a model.\n\n        Call this method from each rank with a batch of timesteps and the\n        corresponding losses for each of those timesteps.\n        This method will perform synchronization to make sure all of the ranks\n        maintain the exact same reweighting.\n\n        :param local_ts: an integer Tensor of timesteps.\n        :param local_losses: a 1D Tensor of losses.\n        \"\"\"\n        batch_sizes = [\n            torch.tensor([0], dtype=torch.int32, device=local_ts.device)\n            for _ in range(torch.distributed.get_world_size())\n        ]\n        torch.distributed.all_gather(\n            batch_sizes,\n            torch.tensor([len(local_ts)], dtype=torch.int32, device=local_ts.device),\n        )\n\n        # Pad all_gather batches to be the maximum batch size.\n        max_bs = max([int(x.item()) for x in batch_sizes])\n\n        timestep_batches = [torch.zeros(max_bs).to(local_ts) for bs in batch_sizes]\n        loss_batches = [torch.zeros(max_bs).to(local_losses) for bs in batch_sizes]\n        torch.distributed.all_gather(timestep_batches, local_ts)\n        torch.distributed.all_gather(loss_batches, local_losses)\n        timesteps = [x.item() for y, bs in zip(timestep_batches, batch_sizes) for x in y[:bs]]\n        losses = [x.item() for y, bs in zip(loss_batches, batch_sizes) for x in y[:bs]]\n        self.update_with_all_losses(timesteps, losses)\n\n    @abstractmethod\n    def update_with_all_losses(self, ts: list[int], losses: list[float]) -&gt; None:\n        \"\"\"\n        Update the reweighting using losses from a model.\n\n        Sub-classes should override this method to update the reweighting\n        using losses from the model.\n\n        This method directly updates the reweighting without synchronizing\n        between workers. It is called by update_with_local_losses from all\n        ranks with identical arguments. Thus, it should have deterministic\n        behavior to maintain state across workers.\n\n        :param ts: a list of int timesteps.\n        :param losses: a list of float losses, one per timestep.\n        \"\"\"\n</code></pre> <code></code> update_with_local_losses \u00b6 <pre><code>update_with_local_losses(local_ts, local_losses)\n</code></pre> <p>Update the reweighting using losses from a model.</p> <p>Call this method from each rank with a batch of timesteps and the corresponding losses for each of those timesteps. This method will perform synchronization to make sure all of the ranks maintain the exact same reweighting.</p> <p>:param local_ts: an integer Tensor of timesteps. :param local_losses: a 1D Tensor of losses.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def update_with_local_losses(self, local_ts: Tensor, local_losses: Tensor) -&gt; None:\n    \"\"\"\n    Update the reweighting using losses from a model.\n\n    Call this method from each rank with a batch of timesteps and the\n    corresponding losses for each of those timesteps.\n    This method will perform synchronization to make sure all of the ranks\n    maintain the exact same reweighting.\n\n    :param local_ts: an integer Tensor of timesteps.\n    :param local_losses: a 1D Tensor of losses.\n    \"\"\"\n    batch_sizes = [\n        torch.tensor([0], dtype=torch.int32, device=local_ts.device)\n        for _ in range(torch.distributed.get_world_size())\n    ]\n    torch.distributed.all_gather(\n        batch_sizes,\n        torch.tensor([len(local_ts)], dtype=torch.int32, device=local_ts.device),\n    )\n\n    # Pad all_gather batches to be the maximum batch size.\n    max_bs = max([int(x.item()) for x in batch_sizes])\n\n    timestep_batches = [torch.zeros(max_bs).to(local_ts) for bs in batch_sizes]\n    loss_batches = [torch.zeros(max_bs).to(local_losses) for bs in batch_sizes]\n    torch.distributed.all_gather(timestep_batches, local_ts)\n    torch.distributed.all_gather(loss_batches, local_losses)\n    timesteps = [x.item() for y, bs in zip(timestep_batches, batch_sizes) for x in y[:bs]]\n    losses = [x.item() for y, bs in zip(loss_batches, batch_sizes) for x in y[:bs]]\n    self.update_with_all_losses(timesteps, losses)\n</code></pre> <code></code> update_with_all_losses <code>abstractmethod</code> \u00b6 <pre><code>update_with_all_losses(ts, losses)\n</code></pre> <p>Update the reweighting using losses from a model.</p> <p>Sub-classes should override this method to update the reweighting using losses from the model.</p> <p>This method directly updates the reweighting without synchronizing between workers. It is called by update_with_local_losses from all ranks with identical arguments. Thus, it should have deterministic behavior to maintain state across workers.</p> <p>:param ts: a list of int timesteps. :param losses: a list of float losses, one per timestep.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>@abstractmethod\ndef update_with_all_losses(self, ts: list[int], losses: list[float]) -&gt; None:\n    \"\"\"\n    Update the reweighting using losses from a model.\n\n    Sub-classes should override this method to update the reweighting\n    using losses from the model.\n\n    This method directly updates the reweighting without synchronizing\n    between workers. It is called by update_with_local_losses from all\n    ranks with identical arguments. Thus, it should have deterministic\n    behavior to maintain state across workers.\n\n    :param ts: a list of int timesteps.\n    :param losses: a list of float losses, one per timestep.\n    \"\"\"\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.FastTensorDataLoader","title":"FastTensorDataLoader","text":"<p>Defines a faster dataloader for PyTorch tensors.</p> <p>A DataLoader-like object for a set of tensors that can be much faster than TensorDataset + DataLoader because dataloader grabs individual indices of the dataset and calls cat (slow). Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class FastTensorDataLoader:\n    \"\"\"\n    Defines a faster dataloader for PyTorch tensors.\n\n    A DataLoader-like object for a set of tensors that can be much faster than\n    TensorDataset + DataLoader because dataloader grabs individual indices of\n    the dataset and calls cat (slow).\n    Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6\n    \"\"\"\n\n    def __init__(self, *tensors: Tensor, batch_size: int = 32, shuffle: bool = False):\n        \"\"\"\n        Initialize a FastTensorDataLoader.\n        :param *tensors: tensors to store. Must have the same length @ dim 0.\n        :param batch_size: batch size to load.\n        :param shuffle: if True, shuffle the data *in-place* whenever an\n            iterator is created out of this object.\n        :returns: A FastTensorDataLoader.\n        \"\"\"\n        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n        self.tensors = tensors\n\n        self.dataset_len = self.tensors[0].shape[0]\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n\n        # Calculate # batches\n        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n        if remainder &gt; 0:\n            n_batches += 1\n        self.n_batches = n_batches\n\n    def __iter__(self):\n        # ruff: noqa: D105\n        if self.shuffle:\n            r = torch.randperm(self.dataset_len)\n            self.tensors = [t[r] for t in self.tensors]  # type: ignore[assignment]\n        self.i = 0\n        return self\n\n    def __next__(self):\n        # ruff: noqa: D105\n        if self.i &gt;= self.dataset_len:\n            raise StopIteration\n        batch = tuple(t[self.i : self.i + self.batch_size] for t in self.tensors)\n        self.i += self.batch_size\n        return batch\n\n    def __len__(self):\n        # ruff: noqa: D105\n        return self.n_batches\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(*tensors, batch_size=32, shuffle=False)\n</code></pre> <p>Initialize a FastTensorDataLoader. :param tensors: tensors to store. Must have the same length @ dim 0. :param batch_size: batch size to load. :param shuffle: if True, shuffle the data in-place* whenever an     iterator is created out of this object. :returns: A FastTensorDataLoader.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def __init__(self, *tensors: Tensor, batch_size: int = 32, shuffle: bool = False):\n    \"\"\"\n    Initialize a FastTensorDataLoader.\n    :param *tensors: tensors to store. Must have the same length @ dim 0.\n    :param batch_size: batch size to load.\n    :param shuffle: if True, shuffle the data *in-place* whenever an\n        iterator is created out of this object.\n    :returns: A FastTensorDataLoader.\n    \"\"\"\n    assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n    self.tensors = tensors\n\n    self.dataset_len = self.tensors[0].shape[0]\n    self.batch_size = batch_size\n    self.shuffle = shuffle\n\n    # Calculate # batches\n    n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n    if remainder &gt; 0:\n        n_batches += 1\n    self.n_batches = n_batches\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.MLP","title":"MLP","text":"<p>               Bases: <code>Module</code></p> <p>The MLP model used in [gorishniy2021revisiting].</p> <p>The following scheme describes the architecture:</p> <p>.. code-block:: text</p> <pre><code>  MLP: (in) -&gt; Block -&gt; ... -&gt; Block -&gt; Linear -&gt; (out)\nBlock: (in) -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; (out)\n</code></pre> <p>Examples:</p> <p>.. testcode::</p> <pre><code>x = torch.randn(4, 2)\nmodule = MLP.make_baseline(x.shape[1], [3, 5], 0.1, 1)\nassert module(x).shape == (len(x), 1)\n</code></pre> References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class MLP(nn.Module):\n    \"\"\"The MLP model used in [gorishniy2021revisiting].\n\n    The following scheme describes the architecture:\n\n    .. code-block:: text\n\n          MLP: (in) -&gt; Block -&gt; ... -&gt; Block -&gt; Linear -&gt; (out)\n        Block: (in) -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; (out)\n\n    Examples:\n        .. testcode::\n\n            x = torch.randn(4, 2)\n            module = MLP.make_baseline(x.shape[1], [3, 5], 0.1, 1)\n            assert module(x).shape == (len(x), 1)\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n\n    class Block(nn.Module):\n        \"\"\"The main building block of `MLP`.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            d_in: int,\n            d_out: int,\n            bias: bool,\n            activation: ModuleType,\n            dropout: float,\n        ) -&gt; None:\n            super().__init__()\n            self.linear = nn.Linear(d_in, d_out, bias)\n            self.activation = _make_nn_module(activation)\n            self.dropout = nn.Dropout(dropout)\n\n        def forward(self, x: Tensor) -&gt; Tensor:\n            return self.dropout(self.activation(self.linear(x)))\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        d_layers: list[int],\n        dropouts: float | list[float],\n        activation: str | Callable[[], nn.Module],\n        d_out: int,\n    ) -&gt; None:\n        \"\"\"\n        Note:\n            `make_baseline` is the recommended constructor.\n        \"\"\"\n        super().__init__()\n        if isinstance(dropouts, float):\n            dropouts = [dropouts] * len(d_layers)\n        assert len(d_layers) == len(dropouts)\n        assert activation not in [\"ReGLU\", \"GEGLU\"]\n\n        self.blocks = nn.ModuleList(\n            [\n                MLP.Block(\n                    d_in=d_layers[i - 1] if i else d_in,\n                    d_out=d,\n                    bias=True,\n                    activation=activation,\n                    dropout=dropout,\n                )\n                for i, (d, dropout) in enumerate(zip(d_layers, dropouts))\n            ]\n        )\n        self.head = nn.Linear(d_layers[-1] if d_layers else d_in, d_out)\n\n    @classmethod\n    def make_baseline(\n        cls,\n        d_in: int,\n        d_layers: list[int],\n        dropout: float,\n        d_out: int,\n    ) -&gt; Self:\n        \"\"\"Create a \"baseline\" `MLP`.\n\n        This variation of MLP was used in [gorishniy2021revisiting]. Features:\n\n        * :code:`Activation` = :code:`ReLU`\n        * all linear layers except for the first one and the last one are of the same dimension\n        * the dropout rate is the same for all dropout layers\n\n        Args:\n            d_in: the input size\n            d_layers: the dimensions of the linear layers. If there are more than two\n                layers, then all of them except for the first and the last ones must\n                have the same dimension. Valid examples: :code:`[]`, :code:`[8]`,\n                :code:`[8, 16]`, :code:`[2, 2, 2, 2]`, :code:`[1, 2, 2, 4]`. Invalid\n                example: :code:`[1, 2, 3, 4]`.\n            dropout: the dropout rate for all hidden layers\n            d_out: the output size\n        Returns:\n            MLP\n\n        References:\n            * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n            Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n        \"\"\"\n        assert isinstance(dropout, float)\n        if len(d_layers) &gt; 2:\n            assert len(set(d_layers[1:-1])) == 1, (\n                \"if d_layers contains more than two elements, then\"\n                \" all elements except for the first and the last ones must be equal.\"\n            )\n        return cls(\n            d_in=d_in,\n            d_layers=d_layers,\n            dropouts=dropout,\n            activation=\"ReLU\",\n            d_out=d_out,\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = x.float()\n        for block in self.blocks:\n            x = block(x)\n        return self.head(x)\n</code></pre> <code></code> Block \u00b6 <p>               Bases: <code>Module</code></p> <p>The main building block of <code>MLP</code>.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class Block(nn.Module):\n    \"\"\"The main building block of `MLP`.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        d_out: int,\n        bias: bool,\n        activation: ModuleType,\n        dropout: float,\n    ) -&gt; None:\n        super().__init__()\n        self.linear = nn.Linear(d_in, d_out, bias)\n        self.activation = _make_nn_module(activation)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        return self.dropout(self.activation(self.linear(x)))\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(*, d_in, d_layers, dropouts, activation, d_out)\n</code></pre> Note <p><code>make_baseline</code> is the recommended constructor.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def __init__(\n    self,\n    *,\n    d_in: int,\n    d_layers: list[int],\n    dropouts: float | list[float],\n    activation: str | Callable[[], nn.Module],\n    d_out: int,\n) -&gt; None:\n    \"\"\"\n    Note:\n        `make_baseline` is the recommended constructor.\n    \"\"\"\n    super().__init__()\n    if isinstance(dropouts, float):\n        dropouts = [dropouts] * len(d_layers)\n    assert len(d_layers) == len(dropouts)\n    assert activation not in [\"ReGLU\", \"GEGLU\"]\n\n    self.blocks = nn.ModuleList(\n        [\n            MLP.Block(\n                d_in=d_layers[i - 1] if i else d_in,\n                d_out=d,\n                bias=True,\n                activation=activation,\n                dropout=dropout,\n            )\n            for i, (d, dropout) in enumerate(zip(d_layers, dropouts))\n        ]\n    )\n    self.head = nn.Linear(d_layers[-1] if d_layers else d_in, d_out)\n</code></pre> <code></code> make_baseline <code>classmethod</code> \u00b6 <pre><code>make_baseline(d_in, d_layers, dropout, d_out)\n</code></pre> <p>Create a \"baseline\" <code>MLP</code>.</p> <p>This variation of MLP was used in [gorishniy2021revisiting]. Features:</p> <ul> <li>:code:<code>Activation</code> = :code:<code>ReLU</code></li> <li>all linear layers except for the first one and the last one are of the same dimension</li> <li>the dropout rate is the same for all dropout layers</li> </ul> <p>Parameters:</p> Name Type Description Default <code>d_in</code> <code>int</code> <p>the input size</p> required <code>d_layers</code> <code>list[int]</code> <p>the dimensions of the linear layers. If there are more than two layers, then all of them except for the first and the last ones must have the same dimension. Valid examples: :code:<code>[]</code>, :code:<code>[8]</code>, :code:<code>[8, 16]</code>, :code:<code>[2, 2, 2, 2]</code>, :code:<code>[1, 2, 2, 4]</code>. Invalid example: :code:<code>[1, 2, 3, 4]</code>.</p> required <code>dropout</code> <code>float</code> <p>the dropout rate for all hidden layers</p> required <code>d_out</code> <code>int</code> <p>the output size</p> required <p>Returns:     MLP</p> References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>@classmethod\ndef make_baseline(\n    cls,\n    d_in: int,\n    d_layers: list[int],\n    dropout: float,\n    d_out: int,\n) -&gt; Self:\n    \"\"\"Create a \"baseline\" `MLP`.\n\n    This variation of MLP was used in [gorishniy2021revisiting]. Features:\n\n    * :code:`Activation` = :code:`ReLU`\n    * all linear layers except for the first one and the last one are of the same dimension\n    * the dropout rate is the same for all dropout layers\n\n    Args:\n        d_in: the input size\n        d_layers: the dimensions of the linear layers. If there are more than two\n            layers, then all of them except for the first and the last ones must\n            have the same dimension. Valid examples: :code:`[]`, :code:`[8]`,\n            :code:`[8, 16]`, :code:`[2, 2, 2, 2]`, :code:`[1, 2, 2, 4]`. Invalid\n            example: :code:`[1, 2, 3, 4]`.\n        dropout: the dropout rate for all hidden layers\n        d_out: the output size\n    Returns:\n        MLP\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n    assert isinstance(dropout, float)\n    if len(d_layers) &gt; 2:\n        assert len(set(d_layers[1:-1])) == 1, (\n            \"if d_layers contains more than two elements, then\"\n            \" all elements except for the first and the last ones must be equal.\"\n        )\n    return cls(\n        d_in=d_in,\n        d_layers=d_layers,\n        dropouts=dropout,\n        activation=\"ReLU\",\n        d_out=d_out,\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ResNet","title":"ResNet","text":"<p>               Bases: <code>Module</code></p> <p>The ResNet model used in [gorishniy2021revisiting].</p> <p>The following scheme describes the architecture: .. code-block:: text     ResNet: (in) -&gt; Linear -&gt; Block -&gt; ... -&gt; Block -&gt; Head -&gt; (out)              |-&gt; Norm -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; Linear -&gt; Dropout -&gt;|              |                                                                  |      Block: (in) ------------------------------------------------------------&gt; Add -&gt; (out)       Head: (in) -&gt; Norm -&gt; Activation -&gt; Linear -&gt; (out)</p> <p>Examples:</p> <p>.. testcode::     x = torch.randn(4, 2)     module = ResNet.make_baseline(         d_in=x.shape[1],         n_blocks=2,         d_main=3,         d_hidden=4,         dropout_first=0.25,         dropout_second=0.0,         d_out=1     )     assert module(x).shape == (len(x), 1)</p> References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class ResNet(nn.Module):\n    \"\"\"\n    The ResNet model used in [gorishniy2021revisiting].\n\n    The following scheme describes the architecture:\n    .. code-block:: text\n        ResNet: (in) -&gt; Linear -&gt; Block -&gt; ... -&gt; Block -&gt; Head -&gt; (out)\n                 |-&gt; Norm -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; Linear -&gt; Dropout -&gt;|\n                 |                                                                  |\n         Block: (in) ------------------------------------------------------------&gt; Add -&gt; (out)\n          Head: (in) -&gt; Norm -&gt; Activation -&gt; Linear -&gt; (out)\n\n    Examples:\n        .. testcode::\n            x = torch.randn(4, 2)\n            module = ResNet.make_baseline(\n                d_in=x.shape[1],\n                n_blocks=2,\n                d_main=3,\n                d_hidden=4,\n                dropout_first=0.25,\n                dropout_second=0.0,\n                d_out=1\n            )\n            assert module(x).shape == (len(x), 1)\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n\n    class Block(nn.Module):\n        \"\"\"The main building block of `ResNet`.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            d_main: int,\n            d_hidden: int,\n            bias_first: bool,\n            bias_second: bool,\n            dropout_first: float,\n            dropout_second: float,\n            normalization: ModuleType,\n            activation: ModuleType,\n            skip_connection: bool,\n        ) -&gt; None:\n            super().__init__()\n            self.normalization = _make_nn_module(normalization, d_main)\n            self.linear_first = nn.Linear(d_main, d_hidden, bias_first)\n            self.activation = _make_nn_module(activation)\n            self.dropout_first = nn.Dropout(dropout_first)\n            self.linear_second = nn.Linear(d_hidden, d_main, bias_second)\n            self.dropout_second = nn.Dropout(dropout_second)\n            self.skip_connection = skip_connection\n\n        def forward(self, x: Tensor) -&gt; Tensor:\n            x_input = x\n            x = self.normalization(x)\n            x = self.linear_first(x)\n            x = self.activation(x)\n            x = self.dropout_first(x)\n            x = self.linear_second(x)\n            x = self.dropout_second(x)\n            if self.skip_connection:\n                x = x_input + x\n            return x\n\n    class Head(nn.Module):\n        \"\"\"The final module of `ResNet`.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            d_in: int,\n            d_out: int,\n            bias: bool,\n            normalization: ModuleType,\n            activation: ModuleType,\n        ) -&gt; None:\n            super().__init__()\n            self.normalization = _make_nn_module(normalization, d_in)\n            self.activation = _make_nn_module(activation)\n            self.linear = nn.Linear(d_in, d_out, bias)\n\n        def forward(self, x: Tensor) -&gt; Tensor:\n            if self.normalization is not None:\n                x = self.normalization(x)\n            x = self.activation(x)\n            return self.linear(x)\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        n_blocks: int,\n        d_main: int,\n        d_hidden: int,\n        dropout_first: float,\n        dropout_second: float,\n        normalization: ModuleType,\n        activation: ModuleType,\n        d_out: int,\n    ) -&gt; None:\n        \"\"\"\n        Note:\n            `make_baseline` is the recommended constructor.\n        \"\"\"\n        super().__init__()\n\n        self.first_layer = nn.Linear(d_in, d_main)\n        if d_main is None:\n            d_main = d_in\n        self.blocks = nn.Sequential(\n            *[\n                ResNet.Block(\n                    d_main=d_main,\n                    d_hidden=d_hidden,\n                    bias_first=True,\n                    bias_second=True,\n                    dropout_first=dropout_first,\n                    dropout_second=dropout_second,\n                    normalization=normalization,\n                    activation=activation,\n                    skip_connection=True,\n                )\n                for _ in range(n_blocks)\n            ]\n        )\n        self.head = ResNet.Head(\n            d_in=d_main,\n            d_out=d_out,\n            bias=True,\n            normalization=normalization,\n            activation=activation,\n        )\n\n    @classmethod\n    def make_baseline(\n        cls,\n        *,\n        d_in: int,\n        n_blocks: int,\n        d_main: int,\n        d_hidden: int,\n        dropout_first: float,\n        dropout_second: float,\n        d_out: int,\n    ) -&gt; Self:\n        \"\"\"Create a \"baseline\" `ResNet`.\n        This variation of ResNet was used in [gorishniy2021revisiting]. Features:\n        * :code:`Activation` = :code:`ReLU`\n        * :code:`Norm` = :code:`BatchNorm1d`\n        Args:\n            d_in: the input size\n            n_blocks: the number of Blocks\n            d_main: the input size (or, equivalently, the output size) of each Block\n            d_hidden: the output size of the first linear layer in each Block\n            dropout_first: the dropout rate of the first dropout layer in each Block.\n            dropout_second: the dropout rate of the second dropout layer in each Block.\n\n        References:\n            * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n            Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n        \"\"\"\n        return cls(\n            d_in=d_in,\n            n_blocks=n_blocks,\n            d_main=d_main,\n            d_hidden=d_hidden,\n            dropout_first=dropout_first,\n            dropout_second=dropout_second,\n            normalization=\"BatchNorm1d\",\n            activation=\"ReLU\",\n            d_out=d_out,\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = x.float()\n        x = self.first_layer(x)\n        x = self.blocks(x)\n        return self.head(x)\n</code></pre> <code></code> Block \u00b6 <p>               Bases: <code>Module</code></p> <p>The main building block of <code>ResNet</code>.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class Block(nn.Module):\n    \"\"\"The main building block of `ResNet`.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        d_main: int,\n        d_hidden: int,\n        bias_first: bool,\n        bias_second: bool,\n        dropout_first: float,\n        dropout_second: float,\n        normalization: ModuleType,\n        activation: ModuleType,\n        skip_connection: bool,\n    ) -&gt; None:\n        super().__init__()\n        self.normalization = _make_nn_module(normalization, d_main)\n        self.linear_first = nn.Linear(d_main, d_hidden, bias_first)\n        self.activation = _make_nn_module(activation)\n        self.dropout_first = nn.Dropout(dropout_first)\n        self.linear_second = nn.Linear(d_hidden, d_main, bias_second)\n        self.dropout_second = nn.Dropout(dropout_second)\n        self.skip_connection = skip_connection\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x_input = x\n        x = self.normalization(x)\n        x = self.linear_first(x)\n        x = self.activation(x)\n        x = self.dropout_first(x)\n        x = self.linear_second(x)\n        x = self.dropout_second(x)\n        if self.skip_connection:\n            x = x_input + x\n        return x\n</code></pre> <code></code> Head \u00b6 <p>               Bases: <code>Module</code></p> <p>The final module of <code>ResNet</code>.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class Head(nn.Module):\n    \"\"\"The final module of `ResNet`.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        d_out: int,\n        bias: bool,\n        normalization: ModuleType,\n        activation: ModuleType,\n    ) -&gt; None:\n        super().__init__()\n        self.normalization = _make_nn_module(normalization, d_in)\n        self.activation = _make_nn_module(activation)\n        self.linear = nn.Linear(d_in, d_out, bias)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        if self.normalization is not None:\n            x = self.normalization(x)\n        x = self.activation(x)\n        return self.linear(x)\n</code></pre> <code></code> __init__ \u00b6 <pre><code>__init__(\n    *,\n    d_in,\n    n_blocks,\n    d_main,\n    d_hidden,\n    dropout_first,\n    dropout_second,\n    normalization,\n    activation,\n    d_out,\n)\n</code></pre> Note <p><code>make_baseline</code> is the recommended constructor.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def __init__(\n    self,\n    *,\n    d_in: int,\n    n_blocks: int,\n    d_main: int,\n    d_hidden: int,\n    dropout_first: float,\n    dropout_second: float,\n    normalization: ModuleType,\n    activation: ModuleType,\n    d_out: int,\n) -&gt; None:\n    \"\"\"\n    Note:\n        `make_baseline` is the recommended constructor.\n    \"\"\"\n    super().__init__()\n\n    self.first_layer = nn.Linear(d_in, d_main)\n    if d_main is None:\n        d_main = d_in\n    self.blocks = nn.Sequential(\n        *[\n            ResNet.Block(\n                d_main=d_main,\n                d_hidden=d_hidden,\n                bias_first=True,\n                bias_second=True,\n                dropout_first=dropout_first,\n                dropout_second=dropout_second,\n                normalization=normalization,\n                activation=activation,\n                skip_connection=True,\n            )\n            for _ in range(n_blocks)\n        ]\n    )\n    self.head = ResNet.Head(\n        d_in=d_main,\n        d_out=d_out,\n        bias=True,\n        normalization=normalization,\n        activation=activation,\n    )\n</code></pre> <code></code> make_baseline <code>classmethod</code> \u00b6 <pre><code>make_baseline(\n    *,\n    d_in,\n    n_blocks,\n    d_main,\n    d_hidden,\n    dropout_first,\n    dropout_second,\n    d_out,\n)\n</code></pre> <p>Create a \"baseline\" <code>ResNet</code>. This variation of ResNet was used in [gorishniy2021revisiting]. Features: * :code:<code>Activation</code> = :code:<code>ReLU</code> * :code:<code>Norm</code> = :code:<code>BatchNorm1d</code> Args:     d_in: the input size     n_blocks: the number of Blocks     d_main: the input size (or, equivalently, the output size) of each Block     d_hidden: the output size of the first linear layer in each Block     dropout_first: the dropout rate of the first dropout layer in each Block.     dropout_second: the dropout rate of the second dropout layer in each Block.</p> References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>@classmethod\ndef make_baseline(\n    cls,\n    *,\n    d_in: int,\n    n_blocks: int,\n    d_main: int,\n    d_hidden: int,\n    dropout_first: float,\n    dropout_second: float,\n    d_out: int,\n) -&gt; Self:\n    \"\"\"Create a \"baseline\" `ResNet`.\n    This variation of ResNet was used in [gorishniy2021revisiting]. Features:\n    * :code:`Activation` = :code:`ReLU`\n    * :code:`Norm` = :code:`BatchNorm1d`\n    Args:\n        d_in: the input size\n        n_blocks: the number of Blocks\n        d_main: the input size (or, equivalently, the output size) of each Block\n        d_hidden: the output size of the first linear layer in each Block\n        dropout_first: the dropout rate of the first dropout layer in each Block.\n        dropout_second: the dropout rate of the second dropout layer in each Block.\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n    return cls(\n        d_in=d_in,\n        n_blocks=n_blocks,\n        d_main=d_main,\n        d_hidden=d_hidden,\n        dropout_first=dropout_first,\n        dropout_second=dropout_second,\n        normalization=\"BatchNorm1d\",\n        activation=\"ReLU\",\n        d_out=d_out,\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ReGLU","title":"ReGLU","text":"<p>               Bases: <code>Module</code></p> <p>The ReGLU activation function from [shazeer2020glu].</p> <p>Examples:</p> <p>.. testcode::</p> <pre><code>module = ReGLU()\nx = torch.randn(3, 4)\nassert module(x).shape == (3, 2)\n</code></pre> References <ul> <li>[shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class ReGLU(nn.Module):\n    \"\"\"The ReGLU activation function from [shazeer2020glu].\n\n    Examples:\n        .. testcode::\n\n            module = ReGLU()\n            x = torch.randn(3, 4)\n            assert module(x).shape == (3, 2)\n\n    References:\n        * [shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        # ruff: noqa: D102\n        return reglu(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.GEGLU","title":"GEGLU","text":"<p>               Bases: <code>Module</code></p> <p>The GEGLU activation function from [shazeer2020glu].</p> <p>Examples:</p> <p>.. testcode::</p> <pre><code>module = GEGLU()\nx = torch.randn(3, 4)\nassert module(x).shape == (3, 2)\n</code></pre> References <ul> <li>[shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class GEGLU(nn.Module):\n    \"\"\"The GEGLU activation function from [shazeer2020glu].\n\n    Examples:\n        .. testcode::\n\n            module = GEGLU()\n            x = torch.randn(3, 4)\n            assert module(x).shape == (3, 2)\n\n    References:\n        * [shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        # ruff: noqa: D102\n        return geglu(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.make_dataset_from_df","title":"make_dataset_from_df","text":"<pre><code>make_dataset_from_df(\n    df, T, is_y_cond, df_info, ratios=None, std=0\n)\n</code></pre> <p>The order of the generated dataset: (y, X_num, X_cat).</p> is_y_cond <p>concat: y is concatenated to X, the model learn a joint distribution of (y, X) embedding: y is not concatenated to X. During computations, y is embedded     and added to the latent vector of X none: y column is completely ignored</p> <p>How does is_y_cond affect the generation of y? is_y_cond:     concat: the model synthesizes (y, X) directly, so y is just the first column     embedding: y is first sampled using empirical distribution of y. The model only         synthesizes X. When returning the generated data, we return the generated X         and the sampled y. (y is sampled from empirical distribution, instead of being         generated by the model)         Note that in this way, y is still not independent of X, because the model has been         adding the embedding of y to the latent vector of X during computations.     none:         y is synthesized using y's empirical distribution. X is generated by the model.         In this case, y is completely independent of X.</p> <p>Note: For now, n_classes has to be set to 0. This is because our matrix is the concatenation of (X_num, X_cat). In this case, if we have is_y_cond == 'concat', we can guarantee that y is the first column of the matrix. However, if we have n_classes &gt; 0, then y is not the first column of the matrix.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def make_dataset_from_df(\n    # ruff: noqa: PLR0915, PLR0912\n    df: pd.DataFrame,\n    T: Transformations,\n    is_y_cond: str,\n    df_info: pd.DataFrame,\n    ratios: list[float] | None = None,\n    std: float = 0,\n) -&gt; tuple[Dataset, dict[int, LabelEncoder], list[int]]:\n    \"\"\"\n    The order of the generated dataset: (y, X_num, X_cat).\n\n    is_y_cond:\n        concat: y is concatenated to X, the model learn a joint distribution of (y, X)\n        embedding: y is not concatenated to X. During computations, y is embedded\n            and added to the latent vector of X\n        none: y column is completely ignored\n\n    How does is_y_cond affect the generation of y?\n    is_y_cond:\n        concat: the model synthesizes (y, X) directly, so y is just the first column\n        embedding: y is first sampled using empirical distribution of y. The model only\n            synthesizes X. When returning the generated data, we return the generated X\n            and the sampled y. (y is sampled from empirical distribution, instead of being\n            generated by the model)\n            Note that in this way, y is still not independent of X, because the model has been\n            adding the embedding of y to the latent vector of X during computations.\n        none:\n            y is synthesized using y's empirical distribution. X is generated by the model.\n            In this case, y is completely independent of X.\n\n    Note: For now, n_classes has to be set to 0. This is because our matrix is the concatenation\n    of (X_num, X_cat). In this case, if we have is_y_cond == 'concat', we can guarantee that y\n    is the first column of the matrix.\n    However, if we have n_classes &gt; 0, then y is not the first column of the matrix.\n    \"\"\"\n    if ratios is None:\n        ratios = [0.7, 0.2, 0.1]\n\n    train_val_df, test_df = train_test_split(df, test_size=ratios[2], random_state=42)\n    train_df, val_df = train_test_split(train_val_df, test_size=ratios[1] / (ratios[0] + ratios[1]), random_state=42)\n\n    cat_column_orders = []\n    num_column_orders = []\n    index_to_column = list(df.columns)\n    column_to_index = {col: i for i, col in enumerate(index_to_column)}\n\n    if df_info[\"n_classes\"] &gt; 0:\n        X_cat: dict[str, np.ndarray] | None = {} if df_info[\"cat_cols\"] is not None or is_y_cond == \"concat\" else None\n        X_num: dict[str, np.ndarray] | None = {} if df_info[\"num_cols\"] is not None else None\n        y = {}\n\n        cat_cols_with_y = []\n        if df_info[\"cat_cols\"] is not None:\n            cat_cols_with_y += df_info[\"cat_cols\"]\n        if is_y_cond == \"concat\":\n            cat_cols_with_y = [df_info[\"y_col\"]] + cat_cols_with_y\n\n        if len(cat_cols_with_y) &gt; 0:\n            X_cat[\"train\"] = train_df[cat_cols_with_y].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"val\"] = val_df[cat_cols_with_y].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"test\"] = test_df[cat_cols_with_y].to_numpy(dtype=np.str_)  # type: ignore[index]\n\n        y[\"train\"] = train_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"val\"] = val_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"test\"] = test_df[df_info[\"y_col\"]].values.astype(np.float32)\n\n        if df_info[\"num_cols\"] is not None:\n            X_num[\"train\"] = train_df[df_info[\"num_cols\"]].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"val\"] = val_df[df_info[\"num_cols\"]].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"test\"] = test_df[df_info[\"num_cols\"]].values.astype(np.float32)  # type: ignore[index]\n\n        cat_column_orders = [column_to_index[col] for col in cat_cols_with_y]\n        num_column_orders = [column_to_index[col] for col in df_info[\"num_cols\"]]\n\n    else:\n        X_cat = {} if df_info[\"cat_cols\"] is not None else None\n        X_num = {} if df_info[\"num_cols\"] is not None or is_y_cond == \"concat\" else None\n        y = {}\n\n        num_cols_with_y = []\n        if df_info[\"num_cols\"] is not None:\n            num_cols_with_y += df_info[\"num_cols\"]\n        if is_y_cond == \"concat\":\n            num_cols_with_y = [df_info[\"y_col\"]] + num_cols_with_y\n\n        if len(num_cols_with_y) &gt; 0:\n            X_num[\"train\"] = train_df[num_cols_with_y].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"val\"] = val_df[num_cols_with_y].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"test\"] = test_df[num_cols_with_y].values.astype(np.float32)  # type: ignore[index]\n\n        y[\"train\"] = train_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"val\"] = val_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"test\"] = test_df[df_info[\"y_col\"]].values.astype(np.float32)\n\n        if df_info[\"cat_cols\"] is not None:\n            X_cat[\"train\"] = train_df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"val\"] = val_df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"test\"] = test_df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)  # type: ignore[index]\n\n        cat_column_orders = [column_to_index[col] for col in df_info[\"cat_cols\"]]\n        num_column_orders = [column_to_index[col] for col in num_cols_with_y]\n\n    column_orders = num_column_orders + cat_column_orders\n    column_orders = [index_to_column[index] for index in column_orders]\n\n    label_encoders = {}\n    if X_cat is not None and len(df_info[\"cat_cols\"]) &gt; 0:\n        X_cat_all = np.vstack((X_cat[\"train\"], X_cat[\"val\"], X_cat[\"test\"]))\n        X_cat_converted = []\n        for col_index in range(X_cat_all.shape[1]):\n            label_encoder = LabelEncoder()\n            X_cat_converted.append(label_encoder.fit_transform(X_cat_all[:, col_index]).astype(float))\n            if std &gt; 0:\n                # add noise\n                X_cat_converted[-1] += np.random.normal(0, std, X_cat_converted[-1].shape)\n            label_encoders[col_index] = label_encoder\n\n        X_cat_converted = np.vstack(X_cat_converted).T  # type: ignore[assignment]\n\n        train_num = X_cat[\"train\"].shape[0]\n        val_num = X_cat[\"val\"].shape[0]\n        # test_num = X_cat[\"test\"].shape[0]\n\n        X_cat[\"train\"] = X_cat_converted[:train_num, :]  # type: ignore[call-overload]\n        X_cat[\"val\"] = X_cat_converted[train_num : train_num + val_num, :]  # type: ignore[call-overload]\n        X_cat[\"test\"] = X_cat_converted[train_num + val_num :, :]  # type: ignore[call-overload]\n\n        if X_num and len(X_num) &gt; 0:\n            X_num[\"train\"] = np.concatenate((X_num[\"train\"], X_cat[\"train\"]), axis=1)\n            X_num[\"val\"] = np.concatenate((X_num[\"val\"], X_cat[\"val\"]), axis=1)\n            X_num[\"test\"] = np.concatenate((X_num[\"test\"], X_cat[\"test\"]), axis=1)\n        else:\n            X_num = X_cat\n            X_cat = None\n\n    D = Dataset(\n        # ruff: noqa: N806\n        X_num,\n        None,\n        y,\n        y_info={},\n        task_type=TaskType(df_info[\"task_type\"]),\n        n_classes=df_info[\"n_classes\"],\n    )\n\n    return transform_dataset(D, T, None), label_encoders, column_orders\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.update_ema","title":"update_ema","text":"<pre><code>update_ema(target_params, source_params, rate=0.999)\n</code></pre> <p>Update target parameters to be closer to those of source parameters using an exponential moving average. :param target_params: the target parameter sequence. :param source_params: the source parameter sequence. :param rate: the EMA rate (closer to 1 means slower).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def update_ema(\n    target_params: Iterator[nn.Parameter],\n    source_params: Iterator[nn.Parameter],\n    rate: float = 0.999,\n) -&gt; None:\n    \"\"\"\n    Update target parameters to be closer to those of source parameters using\n    an exponential moving average.\n    :param target_params: the target parameter sequence.\n    :param source_params: the source parameter sequence.\n    :param rate: the EMA rate (closer to 1 means slower).\n    \"\"\"\n    for targ, src in zip(target_params, source_params):\n        targ.detach().mul_(rate).add_(src.detach(), alpha=1 - rate)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.create_named_schedule_sampler","title":"create_named_schedule_sampler","text":"<pre><code>create_named_schedule_sampler(name, diffusion)\n</code></pre> <p>Create a ScheduleSampler from a library of pre-defined samplers.</p> <p>:param name: the name of the sampler. :param diffusion: the diffusion object to sample for.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def create_named_schedule_sampler(name: str, diffusion: GaussianMultinomialDiffusion) -&gt; ScheduleSampler:\n    \"\"\"\n    Create a ScheduleSampler from a library of pre-defined samplers.\n\n    :param name: the name of the sampler.\n    :param diffusion: the diffusion object to sample for.\n    \"\"\"\n    if name == \"uniform\":\n        return UniformSampler(diffusion)\n    if name == \"loss-second-moment\":\n        return LossSecondMomentResampler(diffusion)\n    raise NotImplementedError(f\"unknown schedule sampler: {name}\")\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.timestep_embedding","title":"timestep_embedding","text":"<pre><code>timestep_embedding(timesteps, dim, max_period=10000)\n</code></pre> <p>Create sinusoidal timestep embeddings.</p> <p>:param timesteps: a 1-D Tensor of N indices, one per batch element.                   These may be fractional. :param dim: the dimension of the output. :param max_period: controls the minimum frequency of the embeddings. :return: an [N x dim] Tensor of positional embeddings.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def timestep_embedding(timesteps: Tensor, dim: int, max_period: int = 10000) -&gt; Tensor:\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2\n    freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(\n        device=timesteps.device\n    )\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.reglu","title":"reglu","text":"<pre><code>reglu(x)\n</code></pre> <p>The ReGLU activation function from [1].</p> References <p>[1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def reglu(x: Tensor) -&gt; Tensor:\n    \"\"\"The ReGLU activation function from [1].\n\n    References:\n        [1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n    assert x.shape[-1] % 2 == 0\n    a, b = x.chunk(2, dim=-1)\n    return a * F.relu(b)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.geglu","title":"geglu","text":"<pre><code>geglu(x)\n</code></pre> <p>The GEGLU activation function from [1].</p> References <p>[1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def geglu(x: Tensor) -&gt; Tensor:\n    \"\"\"The GEGLU activation function from [1].\n\n    References:\n        [1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n    assert x.shape[-1] % 2 == 0\n    a, b = x.chunk(2, dim=-1)\n    return a * F.gelu(b)\n</code></pre>"},{"location":"api/#data-loaders-module","title":"Data Loaders Module","text":""},{"location":"api/#midst_toolkit.core.data_loaders","title":"midst_toolkit.core.data_loaders","text":""},{"location":"api/#logger-module","title":"Logger Module","text":""},{"location":"api/#midst_toolkit.core.logger","title":"midst_toolkit.core.logger","text":"<p>Logger copied from OpenAI baselines to avoid extra RL-based dependencies.</p> <p>https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/logger.py</p>"},{"location":"api/#midst_toolkit.core.logger.TensorBoardOutputFormat","title":"TensorBoardOutputFormat","text":"<p>               Bases: <code>KVWriter</code></p> <p>Dumps key/value pairs into TensorBoard's numeric format.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>class TensorBoardOutputFormat(KVWriter):\n    \"\"\"Dumps key/value pairs into TensorBoard's numeric format.\"\"\"\n\n    def __init__(self, dir: str):\n        os.makedirs(dir, exist_ok=True)\n        self.dir = dir\n        self.step = 1\n        prefix = \"events\"\n        path = osp.join(osp.abspath(dir), prefix)\n        import tensorflow as tf\n        from tensorflow.core.util import event_pb2\n        from tensorflow.python import pywrap_tensorflow\n        from tensorflow.python.util import compat\n\n        self.tf = tf\n        self.event_pb2 = event_pb2\n        self.pywrap_tensorflow = pywrap_tensorflow\n        self.writer = pywrap_tensorflow.EventsWriter(compat.as_bytes(path))\n\n    def writekvs(self, kvs: dict[str, Any]) -&gt; None:\n        def summary_val(k: str, v: Any) -&gt; Any:\n            kwargs = {\"tag\": k, \"simple_value\": float(v)}\n            return self.tf.Summary.Value(**kwargs)\n\n        summary = self.tf.Summary(value=[summary_val(k, v) for k, v in kvs.items()])\n        event = self.event_pb2.Event(wall_time=time.time(), summary=summary)\n        event.step = self.step  # is there any reason why you'd want to specify the step?\n        self.writer.WriteEvent(event)\n        self.writer.Flush()\n        self.step += 1\n\n    def close(self) -&gt; None:\n        if self.writer:\n            self.writer.Close()\n            self.writer = None\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.logkv","title":"logkv","text":"<pre><code>logkv(key, val)\n</code></pre> <p>Log a value of some diagnostic.</p> <p>Call this once for each diagnostic quantity, each iteration If called many times, last value will be used.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def logkv(key: str, val: Any) -&gt; None:\n    \"\"\"\n    Log a value of some diagnostic.\n\n    Call this once for each diagnostic quantity, each iteration\n    If called many times, last value will be used.\n    \"\"\"\n    get_current().logkv(key, val)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.logkv_mean","title":"logkv_mean","text":"<pre><code>logkv_mean(key, val)\n</code></pre> <p>The same as logkv(), but if called many times, values averaged.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def logkv_mean(key: str, val: Any) -&gt; None:\n    \"\"\"The same as logkv(), but if called many times, values averaged.\"\"\"\n    get_current().logkv_mean(key, val)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.logkvs","title":"logkvs","text":"<pre><code>logkvs(d)\n</code></pre> <p>Log a dictionary of key-value pairs.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def logkvs(d: dict[str, Any]) -&gt; None:\n    \"\"\"Log a dictionary of key-value pairs.\"\"\"\n    for k, v in d.items():\n        logkv(k, v)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.dumpkvs","title":"dumpkvs","text":"<pre><code>dumpkvs()\n</code></pre> <p>Write all of the diagnostics from the current iteration.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def dumpkvs() -&gt; dict[str, Any]:\n    \"\"\"Write all of the diagnostics from the current iteration.\"\"\"\n    return get_current().dumpkvs()\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.log","title":"log","text":"<pre><code>log(*args, level=INFO)\n</code></pre> <p>Logs the args in the desired level.</p> <p>Write the sequence of args, with no separators, to the console and output files (if you've configured an output file).</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def log(*args: Iterable[Any], level: int = INFO) -&gt; None:\n    \"\"\"\n    Logs the args in the desired level.\n\n    Write the sequence of args, with no separators, to the console and output\n    files (if you've configured an output file).\n    \"\"\"\n    get_current().log(*args, level=level)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.set_level","title":"set_level","text":"<pre><code>set_level(level)\n</code></pre> <p>Set logging threshold on current logger.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def set_level(level: int) -&gt; None:\n    \"\"\"Set logging threshold on current logger.\"\"\"\n    get_current().set_level(level)\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.get_dir","title":"get_dir","text":"<pre><code>get_dir()\n</code></pre> <p>Get directory that log files are being written to.</p> <p>will be None if there is no output directory (i.e., if you didn't call start)</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def get_dir() -&gt; str:\n    \"\"\"\n    Get directory that log files are being written to.\n\n    will be None if there is no output directory (i.e., if you didn't call start)\n    \"\"\"\n    return get_current().get_dir()\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.profile","title":"profile","text":"<pre><code>profile(n)\n</code></pre> <p>Usage.</p> <p>@profile(\"my_func\") def my_func(): code</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def profile(n):\n    \"\"\"\n    Usage.\n\n    @profile(\"my_func\")\n    def my_func(): code\n    \"\"\"\n\n    def decorator_with_name(func):\n        def func_wrapper(*args, **kwargs):\n            with profile_kv(n):\n                return func(*args, **kwargs)\n\n        return func_wrapper\n\n    return decorator_with_name\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.mpi_weighted_mean","title":"mpi_weighted_mean","text":"<pre><code>mpi_weighted_mean(comm, local_name2valcount)\n</code></pre> <p>Copied from below.</p> <p>https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/common/mpi_util.py#L110 Perform a weighted average over dicts that are each on a different node Input: local_name2valcount: dict mapping key -&gt; (value, count) Returns: key -&gt; mean</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def mpi_weighted_mean(comm: Any, local_name2valcount: dict[str, tuple[float, float]]) -&gt; dict[str, float]:\n    \"\"\"\n    Copied from below.\n\n    https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/common/mpi_util.py#L110\n    Perform a weighted average over dicts that are each on a different node\n    Input: local_name2valcount: dict mapping key -&gt; (value, count)\n    Returns: key -&gt; mean\n    \"\"\"\n    all_name2valcount = comm.gather(local_name2valcount)\n    if comm.rank == 0:\n        name2sum: defaultdict[str, float] = defaultdict(float)\n        name2count: defaultdict[str, float] = defaultdict(float)\n        for n2vc in all_name2valcount:\n            for name, (val, count) in n2vc.items():\n                try:\n                    val = float(val)\n                except ValueError:\n                    if comm.rank == 0:\n                        warnings.warn(\"WARNING: tried to compute mean on non-float {}={}\".format(name, val))\n                        # ruff: noqa: B028\n                else:\n                    name2sum[name] += val * count\n                    name2count[name] += count\n        return {name: name2sum[name] / name2count[name] for name in name2sum}\n    return {}\n</code></pre>"},{"location":"api/#midst_toolkit.core.logger.configure","title":"configure","text":"<pre><code>configure(\n    dir=None, format_strs=None, comm=None, log_suffix=\"\"\n)\n</code></pre> <p>If comm is provided, average all numerical stats across that comm.</p> Source code in <code>src/midst_toolkit/core/logger.py</code> <pre><code>def configure(\n    dir: str | None = None,\n    format_strs: list[str] | None = None,\n    comm: Any | None = None,\n    log_suffix: str = \"\",\n) -&gt; None:\n    \"\"\"If comm is provided, average all numerical stats across that comm.\"\"\"\n    if dir is None:\n        dir = os.getenv(\"OPENAI_LOGDIR\")\n    if dir is None:\n        dir = osp.join(\n            tempfile.gettempdir(),\n            datetime.datetime.now().strftime(\"openai-%Y-%m-%d-%H-%M-%S-%f\"),\n        )\n    assert isinstance(dir, str)\n    dir = os.path.expanduser(dir)\n    os.makedirs(os.path.expanduser(dir), exist_ok=True)\n\n    rank = get_rank_without_mpi_import()\n    if rank &gt; 0:\n        log_suffix = log_suffix + \"-rank%03i\" % rank\n\n    if format_strs is None:\n        if rank == 0:\n            format_strs = os.getenv(\"OPENAI_LOG_FORMAT\", \"stdout,log,csv\").split(\",\")\n        else:\n            format_strs = os.getenv(\"OPENAI_LOG_FORMAT_MPI\", \"log\").split(\",\")\n    format_strs_filter = filter(None, format_strs)\n    output_formats = [make_output_format(f, dir, log_suffix) for f in format_strs_filter]\n\n    Logger.CURRENT = Logger(dir=dir, output_formats=output_formats, comm=comm)  # type: ignore[assignment]\n    if output_formats:\n        log(\"Logging to %s\" % dir)\n</code></pre>"},{"location":"api/#diffusion-utils-module","title":"Diffusion Utils Module","text":""},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils","title":"midst_toolkit.models.clavaddpm.diffusion_utils","text":"<p>PLACEHOLDER.</p>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.FoundNANsError","title":"FoundNANsError","text":"<p>               Bases: <code>BaseException</code></p> <p>Found NANs during sampling.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>class FoundNANsError(BaseException):\n    \"\"\"Found NANs during sampling.\"\"\"\n\n    def __init__(self, message=\"Found NANs during sampling.\"):\n        # ruff: noqa: D107\n        super(FoundNANsError, self).__init__(message)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.normal_kl","title":"normal_kl","text":"<pre><code>normal_kl(mean1, logvar1, mean2, logvar2)\n</code></pre> <p>Compute the KL divergence between two gaussians.</p> <p>Shapes are automatically broadcasted, so batches can be compared to scalars, among other use cases.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def normal_kl(\n    mean1: Tensor | float,\n    logvar1: Tensor | float,\n    mean2: Tensor | float,\n    logvar2: Tensor | float,\n) -&gt; Tensor:\n    \"\"\"\n    Compute the KL divergence between two gaussians.\n\n    Shapes are automatically broadcasted, so batches can be compared to\n    scalars, among other use cases.\n    \"\"\"\n    tensor = None\n    for obj in (mean1, logvar1, mean2, logvar2):\n        if isinstance(obj, torch.Tensor):\n            tensor = obj\n            break\n    assert tensor is not None, \"at least one argument must be a Tensor\"\n\n    # Force variances to be Tensors. Broadcasting helps convert scalars to\n    # Tensors, but it does not work for torch.exp().\n    logvar1, logvar2 = [x if isinstance(x, torch.Tensor) else torch.tensor(x).to(tensor) for x in (logvar1, logvar2)]\n\n    return 0.5 * (\n        -1.0 + logvar2 - logvar1 + torch.exp(logvar1 - logvar2) + ((mean1 - mean2) ** 2) * torch.exp(-logvar2)\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.approx_standard_normal_cdf","title":"approx_standard_normal_cdf","text":"<pre><code>approx_standard_normal_cdf(x)\n</code></pre> <p>A fast approximation of the cumulative distribution function of the standard normal.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def approx_standard_normal_cdf(x: Tensor) -&gt; Tensor:\n    \"\"\"\n    A fast approximation of the cumulative distribution function of the\n    standard normal.\n    \"\"\"\n    return 0.5 * (1.0 + torch.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * torch.pow(x, 3))))\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.discretized_gaussian_log_likelihood","title":"discretized_gaussian_log_likelihood","text":"<pre><code>discretized_gaussian_log_likelihood(\n    x, *, means, log_scales\n)\n</code></pre> <p>Compute the log-likelihood of a Gaussian distribution discretizing to a given image.</p> <p>:param x: the target images. It is assumed that this was uint8 values,           rescaled to the range [-1, 1]. :param means: the Gaussian mean Tensor. :param log_scales: the Gaussian log stddev Tensor. :return: a tensor like x of log probabilities (in nats).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def discretized_gaussian_log_likelihood(x: Tensor, *, means: Tensor, log_scales: Tensor) -&gt; Tensor:\n    \"\"\"\n    Compute the log-likelihood of a Gaussian distribution discretizing to a\n    given image.\n\n    :param x: the target images. It is assumed that this was uint8 values,\n              rescaled to the range [-1, 1].\n    :param means: the Gaussian mean Tensor.\n    :param log_scales: the Gaussian log stddev Tensor.\n    :return: a tensor like x of log probabilities (in nats).\n    \"\"\"\n    assert x.shape == means.shape == log_scales.shape\n    centered_x = x - means\n    inv_stdv = torch.exp(-log_scales)\n    plus_in = inv_stdv * (centered_x + 1.0 / 255.0)\n    cdf_plus = approx_standard_normal_cdf(plus_in)\n    min_in = inv_stdv * (centered_x - 1.0 / 255.0)\n    cdf_min = approx_standard_normal_cdf(min_in)\n    log_cdf_plus = torch.log(cdf_plus.clamp(min=1e-12))\n    log_one_minus_cdf_min = torch.log((1.0 - cdf_min).clamp(min=1e-12))\n    cdf_delta = cdf_plus - cdf_min\n    log_probs = torch.where(\n        x &lt; -0.999,\n        log_cdf_plus,\n        torch.where(x &gt; 0.999, log_one_minus_cdf_min, torch.log(cdf_delta.clamp(min=1e-12))),\n    )\n    assert log_probs.shape == x.shape\n    return log_probs\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.sum_except_batch","title":"sum_except_batch","text":"<pre><code>sum_except_batch(x, num_dims=1)\n</code></pre> <p>Sums all dimensions except the first.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Tensor, shape (batch_size, ...)</p> required <code>num_dims</code> <code>int</code> <p>int, number of batch dims (default=1)</p> <code>1</code> <p>Returns:</p> Name Type Description <code>x_sum</code> <code>Tensor</code> <p>Tensor, shape (batch_size,)</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def sum_except_batch(x: Tensor, num_dims: int = 1) -&gt; Tensor:\n    \"\"\"\n    Sums all dimensions except the first.\n\n    Args:\n        x: Tensor, shape (batch_size, ...)\n        num_dims: int, number of batch dims (default=1)\n\n    Returns:\n        x_sum: Tensor, shape (batch_size,)\n    \"\"\"\n    return x.reshape(*x.shape[:num_dims], -1).sum(-1)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.diffusion_utils.mean_flat","title":"mean_flat","text":"<pre><code>mean_flat(tensor)\n</code></pre> <p>Take the mean over all non-batch dimensions.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/diffusion_utils.py</code> <pre><code>def mean_flat(tensor: Tensor) -&gt; Tensor:\n    \"\"\"Take the mean over all non-batch dimensions.\"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n</code></pre>"},{"location":"api/#gaussian-multinomial-diffusion-module","title":"Gaussian Multinomial Diffusion Module","text":""},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion","title":"midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion","text":"<p>Based on the code below.</p> <p>https://github.com/openai/guided-diffusion/blob/main/guided_diffusion https://github.com/ehoogeboom/multinomial_diffusion</p>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.GaussianMultinomialDiffusion","title":"GaussianMultinomialDiffusion","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>class GaussianMultinomialDiffusion(torch.nn.Module):\n    def __init__(\n        # ruff: noqa: PLR0915\n        self,\n        num_classes: np.ndarray,\n        num_numerical_features: int,\n        denoise_fn: torch.nn.Module,\n        num_timesteps: int = 1000,\n        gaussian_loss_type: str = \"mse\",\n        gaussian_parametrization: str = \"eps\",\n        multinomial_loss_type: str = \"vb_stochastic\",\n        parametrization: str = \"x0\",\n        scheduler: str = \"cosine\",\n        device: torch.device | None = None,\n    ):\n        # ruff: noqa: D107\n        if device is None:\n            device = torch.device(\"cpu\")\n\n        super(GaussianMultinomialDiffusion, self).__init__()\n        assert multinomial_loss_type in (\"vb_stochastic\", \"vb_all\")\n        assert parametrization in (\"x0\", \"direct\")\n\n        if multinomial_loss_type == \"vb_all\":\n            print(\n                \"Computing the loss using the bound on _all_ timesteps.\"\n                \" This is expensive both in terms of memory and computation.\"\n            )\n\n        self.num_numerical_features = num_numerical_features\n        self.num_classes = num_classes  # it as a vector [K1, K2, ..., Km]\n        self.num_classes_expanded = torch.from_numpy(\n            np.concatenate([num_classes[i].repeat(num_classes[i]) for i in range(len(num_classes))])\n        ).to(device)\n\n        self.slices_for_classes = [np.arange(self.num_classes[0])]\n        offsets: np.ndarray = np.cumsum(self.num_classes)\n        for i in range(1, len(offsets)):\n            self.slices_for_classes.append(np.arange(offsets[i - 1], offsets[i]))\n        self.offsets = torch.from_numpy(np.append([0], offsets)).to(device)\n\n        self._denoise_fn = denoise_fn\n        self.gaussian_loss_type = gaussian_loss_type\n        self.gaussian_parametrization = gaussian_parametrization\n        self.multinomial_loss_type = multinomial_loss_type\n        self.num_timesteps = num_timesteps\n        self.parametrization = parametrization\n        self.scheduler = scheduler\n        self.device = device\n        self.alphas: Tensor\n        self.alphas_cumprod: Tensor\n        self.alphas_cumprod_next: Tensor\n        self.alphas_cumprod_prev: Tensor\n        self.sqrt_alphas_cumprod: Tensor\n        self.sqrt_one_minus_alphas_cumprod: Tensor\n        self.log_cumprod_alpha: Tensor\n        self.log_alpha: Tensor\n        self.log_1_min_alpha: Tensor\n        self.log_1_min_cumprod_alpha: Tensor\n        self.sqrt_recipm1_alphas_cumprod: Tensor\n        self.sqrt_recip_alphas_cumprod: Tensor\n        self.Lt_history: Tensor\n        self.Lt_count: Tensor\n\n        a = 1.0 - get_named_beta_schedule(scheduler, num_timesteps)\n        alphas = torch.tensor(a.astype(\"float64\"))\n        betas = 1.0 - alphas\n\n        log_alpha: Tensor = np.log(alphas)  # type: ignore[assignment]\n        log_cumprod_alpha: Tensor = np.cumsum(log_alpha)  # type: ignore[assignment]\n\n        log_1_min_alpha: Tensor = log_1_min_a(log_alpha)\n        log_1_min_cumprod_alpha: Tensor = log_1_min_a(log_cumprod_alpha)\n\n        alphas_cumprod: Tensor = np.cumprod(alphas, axis=0)  # type: ignore[assignment]\n        alphas_cumprod_prev = torch.tensor(np.append(1.0, alphas_cumprod[:-1]))\n        alphas_cumprod_next = torch.tensor(np.append(alphas_cumprod[1:], 0.0))\n        sqrt_alphas_cumprod: Tensor = np.sqrt(alphas_cumprod)  # type: ignore[assignment]\n        sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - alphas_cumprod)\n        sqrt_recip_alphas_cumprod: Tensor = np.sqrt(1.0 / alphas_cumprod)\n        sqrt_recipm1_alphas_cumprod: Tensor = np.sqrt(1.0 / alphas_cumprod - 1)\n\n        # Gaussian diffusion\n\n        self.posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n        self.posterior_log_variance_clipped = (\n            torch.from_numpy(np.log(np.append(self.posterior_variance[1], self.posterior_variance[1:])))\n            .float()\n            .to(device)\n        )\n        self.posterior_mean_coef1 = (betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)).float().to(device)\n        self.posterior_mean_coef2 = (\n            ((1.0 - alphas_cumprod_prev) * np.sqrt(alphas.numpy()) / (1.0 - alphas_cumprod)).float().to(device)\n        )\n\n        assert log_add_exp(log_alpha, log_1_min_alpha).abs().sum().item() &lt; 1.0e-5\n        assert log_add_exp(log_cumprod_alpha, log_1_min_cumprod_alpha).abs().sum().item() &lt; 1e-5\n        diff: Tensor = cast(Tensor, np.cumsum(log_alpha) - log_cumprod_alpha)\n        assert diff.abs().sum().item() &lt; 1.0e-5\n\n        # Convert to float32 and register buffers.\n        self.register_buffer(\"alphas\", alphas.float().to(device))\n        self.register_buffer(\"log_alpha\", log_alpha.float().to(device))\n        self.register_buffer(\"log_1_min_alpha\", log_1_min_alpha.float().to(device))\n        self.register_buffer(\"log_1_min_cumprod_alpha\", log_1_min_cumprod_alpha.float().to(device))\n        self.register_buffer(\"log_cumprod_alpha\", log_cumprod_alpha.float().to(device))\n        self.register_buffer(\"alphas_cumprod\", alphas_cumprod.float().to(device))\n        self.register_buffer(\"alphas_cumprod_prev\", alphas_cumprod_prev.float().to(device))\n        self.register_buffer(\"alphas_cumprod_next\", alphas_cumprod_next.float().to(device))\n        self.register_buffer(\"sqrt_alphas_cumprod\", sqrt_alphas_cumprod.float().to(device))\n        self.register_buffer(\n            \"sqrt_one_minus_alphas_cumprod\",\n            sqrt_one_minus_alphas_cumprod.float().to(device),\n        )\n        self.register_buffer(\"sqrt_recip_alphas_cumprod\", sqrt_recip_alphas_cumprod.float().to(device))\n        self.register_buffer(\n            \"sqrt_recipm1_alphas_cumprod\",\n            sqrt_recipm1_alphas_cumprod.float().to(device),\n        )\n\n        self.register_buffer(\"Lt_history\", torch.zeros(num_timesteps))\n        self.register_buffer(\"Lt_count\", torch.zeros(num_timesteps))\n\n    # Gaussian part\n    def gaussian_q_mean_variance(self, x_start: Tensor, t: Tensor) -&gt; tuple[Tensor, Tensor, Tensor]:\n        mean = extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n        variance = extract(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract(self.log_1_min_cumprod_alpha, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def gaussian_q_sample(self, x_start: Tensor, t: Tensor, noise: Tensor | None = None) -&gt; Tensor:\n        if noise is None:\n            noise = torch.randn_like(x_start)\n        assert noise.shape == x_start.shape\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n            + extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n        )\n\n    def gaussian_q_posterior_mean_variance(\n        self,\n        x_start: Tensor,\n        x_t: Tensor,\n        t: Tensor,\n    ) -&gt; tuple[Tensor, Tensor, Tensor]:\n        assert x_start.shape == x_t.shape\n        posterior_mean = (\n            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start\n            + extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n        assert (\n            posterior_mean.shape[0]\n            == posterior_variance.shape[0]\n            == posterior_log_variance_clipped.shape[0]\n            == x_start.shape[0]\n        )\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def gaussian_p_mean_variance(\n        self,\n        model_output: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        denoised_fn: Callable | None = None,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Tensor]:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        B, C = x.shape[:2]\n        assert t.shape == (B,)\n\n        model_variance = torch.cat(\n            [\n                self.posterior_variance[1].unsqueeze(0).to(x.device),\n                (1.0 - self.alphas)[1:],\n            ],\n            dim=0,\n        )\n        # model_variance = self.posterior_variance.to(x.device)\n        model_log_variance = torch.log(model_variance)\n\n        model_variance = extract(model_variance, t, x.shape)\n        model_log_variance = extract(model_log_variance, t, x.shape)\n\n        if self.gaussian_parametrization == \"eps\":\n            pred_xstart = self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n        elif self.gaussian_parametrization == \"x0\":\n            pred_xstart = model_output\n        else:\n            raise NotImplementedError\n\n        model_mean, _, _ = self.gaussian_q_posterior_mean_variance(x_start=pred_xstart, x_t=x, t=t)\n\n        assert model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape, (\n            f\"{model_mean.shape}, {model_log_variance.shape}, {pred_xstart.shape}, {x.shape}\"\n        )\n\n        return {\n            \"mean\": model_mean,\n            \"variance\": model_variance,\n            \"log_variance\": model_log_variance,\n            \"pred_xstart\": pred_xstart,\n        }\n\n    def _vb_terms_bpd(\n        self,\n        model_output: Tensor,\n        x_start: Tensor,\n        x_t: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Tensor]:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        (\n            true_mean,\n            _,\n            true_log_variance_clipped,\n        ) = self.gaussian_q_posterior_mean_variance(x_start=x_start, x_t=x_t, t=t)\n        out = self.gaussian_p_mean_variance(\n            model_output, x_t, t, clip_denoised=clip_denoised, model_kwargs=model_kwargs\n        )\n        kl = normal_kl(true_mean, true_log_variance_clipped, out[\"mean\"], out[\"log_variance\"])\n        kl = mean_flat(kl) / np.log(2.0)\n\n        decoder_nll = -discretized_gaussian_log_likelihood(\n            x_start, means=out[\"mean\"], log_scales=0.5 * out[\"log_variance\"]\n        )\n        assert decoder_nll.shape == x_start.shape\n        decoder_nll = mean_flat(decoder_nll) / np.log(2.0)\n\n        # At the first timestep return the decoder NLL,\n        # otherwise return KL(q(x_{t-1}|x_t,x_0) || p(x_{t-1}|x_t))\n        output = torch.where((t == 0), decoder_nll, kl)\n        return {\n            \"output\": output,\n            \"pred_xstart\": out[\"pred_xstart\"],\n            \"out_mean\": out[\"mean\"],\n            \"true_mean\": true_mean,\n        }\n\n    def _prior_gaussian(self, x_start: Tensor) -&gt; Tensor:\n        \"\"\"\n        Get the prior KL term for the variational lower-bound, measured in\n        bits-per-dim.\n\n        This term can't be optimized, as it only depends on the encoder.\n\n        :param x_start: the [N x C x ...] tensor of inputs.\n        :return: a batch of [N] KL values (in bits), one per batch element.\n        \"\"\"\n        batch_size = x_start.shape[0]\n        t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n        qt_mean, _, qt_log_variance = self.gaussian_q_mean_variance(x_start, t)\n        kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0)\n        return mean_flat(kl_prior) / np.log(2.0)\n\n    def _gaussian_loss(\n        self,\n        model_out: Tensor,\n        x_start: Tensor,\n        x_t: Tensor,\n        t: Tensor,\n        noise: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; Tensor:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        terms = {}\n        if self.gaussian_loss_type == \"mse\":\n            terms[\"loss\"] = mean_flat((noise - model_out) ** 2)\n        elif self.gaussian_loss_type == \"kl\":\n            terms[\"loss\"] = self._vb_terms_bpd(\n                model_output=model_out,\n                x_start=x_start,\n                x_t=x_t,\n                t=t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n            )[\"output\"]\n\n        return terms[\"loss\"]\n\n    def _predict_xstart_from_eps(self, x_t: Tensor, t: Tensor, eps: Tensor) -&gt; Tensor:\n        assert x_t.shape == eps.shape\n        return (\n            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n            - extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * eps\n        )\n\n    def _predict_eps_from_xstart(self, x_t: Tensor, t: Tensor, pred_xstart: Tensor) -&gt; Tensor:\n        return (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / extract(\n            self.sqrt_recipm1_alphas_cumprod, t, x_t.shape\n        )\n\n    def condition_mean(\n        self,\n        cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n        p_mean_var: dict[str, Tensor],\n        x: Tensor,\n        t: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; Tensor:\n        \"\"\"\n        Compute the mean for the previous step, given a function cond_fn that\n        computes the gradient of a conditional log probability with respect to\n        x. In particular, cond_fn computes grad(log(p(y|x))), and we want to\n        condition on y.\n\n        This uses the conditioning strategy from Sohl-Dickstein et al. (2015).\n        \"\"\"\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        gradient = cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n        return p_mean_var[\"mean\"].float() + p_mean_var[\"variance\"] * gradient.float()\n\n    def condition_score(\n        self,\n        cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n        p_mean_var: dict[str, Tensor],\n        x: Tensor,\n        t: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Tensor]:\n        \"\"\"\n        Compute what the p_mean_variance output would have been, should the\n        model's score function be conditioned by cond_fn.\n\n        See condition_mean() for details on cond_fn.\n\n        Unlike condition_mean(), this instead uses the conditioning strategy\n        from Song et al (2020).\n        \"\"\"\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        alpha_bar = extract(self.alphas_cumprod, t, x.shape)\n\n        eps = self._predict_eps_from_xstart(x, t, p_mean_var[\"pred_xstart\"])\n        eps = eps - (1 - alpha_bar).sqrt() * cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n\n        out = p_mean_var.copy()\n        out[\"pred_xstart\"] = self._predict_xstart_from_eps(x, t, eps)\n        out[\"mean\"], _, _ = self.q_posterior_mean_variance(x_start=out[\"pred_xstart\"], x_t=x, t=t)\n        return out\n\n    def gaussian_p_sample(\n        self,\n        model_out: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        denoised_fn: Callable | None = None,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; dict[str, Tensor]:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        out = self.gaussian_p_mean_variance(\n            model_out,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n        )\n        noise = torch.randn_like(x)\n        nonzero_mask = (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))  # no noise when t == 0\n\n        if cond_fn is not None:\n            out[\"mean\"] = self.condition_mean(cond_fn, out, x, t, model_kwargs=model_kwargs)\n\n        sample = out[\"mean\"] + nonzero_mask * torch.exp(0.5 * out[\"log_variance\"]) * noise\n        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    # Multinomial part\n\n    def multinomial_kl(self, log_prob1: Tensor, log_prob2: Tensor) -&gt; Tensor:\n        return (log_prob1.exp() * (log_prob1 - log_prob2)).sum(dim=1)\n\n    def q_pred_one_timestep(self, log_x_t: Tensor, t: Tensor) -&gt; Tensor:\n        log_alpha_t = extract(self.log_alpha, t, log_x_t.shape)\n        log_1_min_alpha_t = extract(self.log_1_min_alpha, t, log_x_t.shape)\n\n        # alpha_t * E[xt] + (1 - alpha_t) 1 / K\n        return log_add_exp(\n            log_x_t + log_alpha_t,\n            log_1_min_alpha_t - torch.log(self.num_classes_expanded),\n        )\n\n    def q_pred(self, log_x_start: Tensor, t: Tensor) -&gt; Tensor:\n        log_cumprod_alpha_t = extract(self.log_cumprod_alpha, t, log_x_start.shape)\n        log_1_min_cumprod_alpha = extract(self.log_1_min_cumprod_alpha, t, log_x_start.shape)\n\n        return log_add_exp(\n            log_x_start + log_cumprod_alpha_t,\n            log_1_min_cumprod_alpha - torch.log(self.num_classes_expanded),\n        )\n\n    def predict_start(self, model_out: Tensor, log_x_t: Tensor, t: Tensor, out_dict: dict[str, Tensor]) -&gt; Tensor:\n        # model_out = self._denoise_fn(x_t, t.to(x_t.device), **out_dict)\n\n        assert model_out.size(0) == log_x_t.size(0)\n        assert self.num_classes is not None\n        assert model_out.size(1) == self.num_classes.sum(), f\"{model_out.size()}\"\n\n        log_pred = torch.empty_like(model_out)\n        for ix in self.slices_for_classes:\n            log_pred[:, ix] = F.log_softmax(model_out[:, ix], dim=1)\n        return log_pred\n\n    def q_posterior(self, log_x_start: Tensor, log_x_t: Tensor, t: Tensor) -&gt; Tensor:\n        # q(xt-1 | xt, x0) = q(xt | xt-1, x0) * q(xt-1 | x0) / q(xt | x0)\n        # where q(xt | xt-1, x0) = q(xt | xt-1).\n\n        # EV_log_qxt_x0 = self.q_pred(log_x_start, t)\n\n        # print('sum exp', EV_log_qxt_x0.exp().sum(1).mean())\n        # assert False\n\n        # log_qxt_x0 = (log_x_t.exp() * EV_log_qxt_x0).sum(dim=1)\n        t_minus_1 = t - 1\n        # Remove negative values, will not be used anyway for final decoder\n        t_minus_1 = torch.where(t_minus_1 &lt; 0, torch.zeros_like(t_minus_1), t_minus_1)\n        log_EV_qxtmin_x0 = self.q_pred(log_x_start, t_minus_1)\n\n        num_axes = (1,) * (len(log_x_start.size()) - 1)\n        t_broadcast = t.to(log_x_start.device).view(-1, *num_axes) * torch.ones_like(log_x_start)\n        log_EV_qxtmin_x0 = torch.where(t_broadcast == 0, log_x_start, log_EV_qxtmin_x0.to(torch.float32))\n\n        # unnormed_logprobs = log_EV_qxtmin_x0 +\n        #                     log q_pred_one_timestep(x_t, t)\n        # Note: _NOT_ x_tmin1, which is how the formula is typically used!!!\n        # Not very easy to see why this is true. But it is :)\n        unnormed_logprobs = log_EV_qxtmin_x0 + self.q_pred_one_timestep(log_x_t, t)\n\n        return unnormed_logprobs - sliced_logsumexp(unnormed_logprobs, self.offsets)\n\n    def p_pred(self, model_out: Tensor, log_x: Tensor, t: Tensor, out_dict: dict[str, Tensor]) -&gt; Tensor:\n        if self.parametrization == \"x0\":\n            log_x_recon = self.predict_start(model_out, log_x, t=t, out_dict=out_dict)\n            log_model_pred = self.q_posterior(log_x_start=log_x_recon, log_x_t=log_x, t=t)\n        elif self.parametrization == \"direct\":\n            log_model_pred = self.predict_start(model_out, log_x, t=t, out_dict=out_dict)\n        else:\n            raise ValueError\n        return log_model_pred\n\n    @torch.no_grad()\n    def p_sample(self, model_out: Tensor, log_x: Tensor, t: Tensor, out_dict: dict[str, Tensor]) -&gt; Tensor:\n        model_log_prob = self.p_pred(model_out, log_x=log_x, t=t, out_dict=out_dict)\n        return self.log_sample_categorical(model_log_prob)\n\n    # Dead code\n    # @torch.no_grad()\n    # def p_sample_loop(self, shape, out_dict):\n    #     b = shape[0]\n    #     # start with random normal image.\n    #     img = torch.randn(shape, device=device)\n\n    #     for i in reversed(range(1, self.num_timesteps)):\n    #         img = self.p_sample(img, torch.full((b,), i, device=self.device, dtype=torch.long), out_dict)\n    #     return img\n\n    # @torch.no_grad()\n    # def _sample(self, image_size, out_dict, batch_size=16):\n    #     return self.p_sample_loop((batch_size, 3, image_size, image_size), out_dict)\n\n    # Dead code\n    # @torch.no_grad()\n    # def interpolate(self, x1: Tensor, x2: Tensor, t: Tensor | None = None, lam: float = 0.5) -&gt; Tensor:\n    #     b, *_, device = *x1.shape, x1.device\n    #     t = default(t, self.num_timesteps - 1)\n\n    #     assert x1.shape == x2.shape\n\n    #     t_batched = torch.stack([torch.tensor(t, device=device)] * b)\n    #     xt1, xt2 = map(lambda x: self.q_sample(x, t=t_batched), (x1, x2))\n\n    #     img = (1 - lam) * xt1 + lam * xt2\n    #     for i in reversed(range(0, t)):\n    #         img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long))\n\n    #     return img\n\n    def log_sample_categorical(self, logits: Tensor) -&gt; Tensor:\n        full_sample = []\n        for i in range(len(self.num_classes)):\n            one_class_logits = logits[:, self.slices_for_classes[i]]\n            uniform = torch.rand_like(one_class_logits)\n            gumbel_noise = -torch.log(-torch.log(uniform + 1e-30) + 1e-30)\n            sample = (gumbel_noise + one_class_logits).argmax(dim=1)\n            full_sample.append(sample.unsqueeze(1))\n        full_sample_tensor = torch.cat(full_sample, dim=1)\n        return index_to_log_onehot(full_sample_tensor, torch.from_numpy(self.num_classes))\n\n    def q_sample(self, log_x_start: Tensor, t: Tensor) -&gt; Tensor:\n        log_EV_qxt_x0 = self.q_pred(log_x_start, t)\n        # ruff: noqa: N806\n        return self.log_sample_categorical(log_EV_qxt_x0)\n\n    # Dead code\n    # def nll(self, log_x_start, out_dict):\n    #     b = log_x_start.size(0)\n    #     device = log_x_start.device\n    #     loss = 0\n    #     for t in range(0, self.num_timesteps):\n    #         t_array = (torch.ones(b, device=device) * t).long()\n\n    #         kl = self.compute_Lt(\n    #             log_x_start=log_x_start,\n    #             log_x_t=self.q_sample(log_x_start=log_x_start, t=t_array),\n    #             t=t_array,\n    #             out_dict=out_dict,\n    #         )\n\n    #         loss += kl\n\n    #     loss += self.kl_prior(log_x_start)\n\n    #     return loss\n\n    def kl_prior(self, log_x_start: Tensor) -&gt; Tensor:\n        b = log_x_start.size(0)\n        device = log_x_start.device\n        ones = torch.ones(b, device=device).long()\n\n        log_qxT_prob = self.q_pred(log_x_start, t=(self.num_timesteps - 1) * ones)\n        # ruff: noqa: N806\n        log_half_prob = -torch.log(self.num_classes_expanded * torch.ones_like(log_qxT_prob))\n\n        kl_prior = self.multinomial_kl(log_qxT_prob, log_half_prob)\n        return sum_except_batch(kl_prior)\n\n    def compute_Lt(\n        # ruff: noqa: N802\n        self,\n        model_out: Tensor,\n        log_x_start: Tensor,\n        log_x_t: Tensor,\n        t: Tensor,\n        out_dict: dict[str, Tensor],\n        detach_mean: bool = False,\n    ) -&gt; Tensor:\n        log_true_prob = self.q_posterior(log_x_start=log_x_start, log_x_t=log_x_t, t=t)\n        log_model_prob = self.p_pred(model_out, log_x=log_x_t, t=t, out_dict=out_dict)\n\n        if detach_mean:\n            log_model_prob = log_model_prob.detach()\n\n        kl = self.multinomial_kl(log_true_prob, log_model_prob)\n        kl = sum_except_batch(kl)\n\n        decoder_nll = -log_categorical(log_x_start, log_model_prob)\n        decoder_nll = sum_except_batch(decoder_nll)\n\n        mask = (t == torch.zeros_like(t)).float()\n        return mask * decoder_nll + (1.0 - mask) * kl\n\n    def sample_time(self, b: int, device: torch.device, method: str = \"uniform\") -&gt; tuple[Tensor, Tensor]:\n        if method == \"importance\":\n            if not (self.Lt_count &gt; 10).all():\n                return self.sample_time(b, device, method=\"uniform\")\n\n            Lt_sqrt = torch.sqrt(self.Lt_history + 1e-10) + 0.0001\n            # ruff: noqa: N806\n            Lt_sqrt[0] = Lt_sqrt[1]  # Overwrite decoder term with L1.\n            pt_all = (Lt_sqrt / Lt_sqrt.sum()).to(device)\n\n            t = torch.multinomial(pt_all, num_samples=b, replacement=True).to(device)\n\n            pt = pt_all.gather(dim=0, index=t)\n\n            return t, pt\n\n        if method == \"uniform\":\n            t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n\n            pt = torch.ones_like(t).float() / self.num_timesteps\n            return t, pt\n        raise ValueError\n\n    def _multinomial_loss(\n        self,\n        model_out: Tensor,\n        log_x_start: Tensor,\n        log_x_t: Tensor,\n        t: Tensor,\n        pt: Tensor,\n        out_dict: dict[str, Tensor],\n    ) -&gt; Tensor:\n        if self.multinomial_loss_type == \"vb_stochastic\":\n            kl = self.compute_Lt(model_out, log_x_start, log_x_t, t, out_dict)\n            kl_prior = self.kl_prior(log_x_start)\n            # Upweigh loss term of the kl\n            return kl / pt + kl_prior\n\n        if self.multinomial_loss_type == \"vb_all\":\n            # Expensive, dont do it ;).\n            # DEPRECATED\n            # return -self.nll(log_x_start)\n            raise ValueError(\"multinomial_loss_type == 'vb_all' is deprecated.\")\n        raise ValueError\n\n    # Dead code\n    # def log_prob(self, x, out_dict):\n    #     b, device = x.size(0), x.device\n    #     if self.training:\n    #         return self._multinomial_loss(x, out_dict)\n\n    #     log_x_start = index_to_log_onehot(x, self.num_classes)\n\n    #     t, pt = self.sample_time(b, device, \"importance\")\n\n    #     kl = self.compute_Lt(log_x_start, self.q_sample(log_x_start=log_x_start, t=t), t, out_dict)\n\n    #     kl_prior = self.kl_prior(log_x_start)\n\n    #     # Upweigh loss term of the kl\n    #     loss = kl / pt + kl_prior\n\n    #     return -loss\n\n    def mixed_loss(self, x: Tensor, out_dict: dict[str, Tensor]) -&gt; tuple[Tensor, Tensor]:\n        b = x.shape[0]\n        device = x.device\n        t, pt = self.sample_time(b, device, \"uniform\")\n\n        x_num = x[:, : self.num_numerical_features]\n        x_cat = x[:, self.num_numerical_features :]\n\n        x_num_t = x_num\n        log_x_cat_t = x_cat\n        if x_num.shape[1] &gt; 0:\n            noise = torch.randn_like(x_num)\n            x_num_t = self.gaussian_q_sample(x_num, t, noise=noise)\n        if x_cat.shape[1] &gt; 0:\n            log_x_cat = index_to_log_onehot(x_cat.long(), torch.from_numpy(self.num_classes))\n            log_x_cat_t = self.q_sample(log_x_start=log_x_cat, t=t)\n\n        x_in = torch.cat([x_num_t, log_x_cat_t], dim=1)\n\n        model_out = self._denoise_fn(x_in, t, **out_dict)\n\n        model_out_num = model_out[:, : self.num_numerical_features]\n        model_out_cat = model_out[:, self.num_numerical_features :]\n\n        loss_multi = torch.zeros((1,)).float()\n        loss_gauss = torch.zeros((1,)).float()\n        if x_cat.shape[1] &gt; 0:\n            loss_multi = self._multinomial_loss(model_out_cat, log_x_cat, log_x_cat_t, t, pt, out_dict) / len(\n                self.num_classes\n            )\n\n        if x_num.shape[1] &gt; 0:\n            loss_gauss = self._gaussian_loss(model_out_num, x_num, x_num_t, t, noise)\n\n        # loss_multi = torch.where(out_dict['y'] == 1, loss_multi, 2 * loss_multi)\n        # loss_gauss = torch.where(out_dict['y'] == 1, loss_gauss, 2 * loss_gauss)\n\n        return loss_multi.mean(), loss_gauss.mean()\n\n    @torch.no_grad()\n    def mixed_elbo(self, x0, out_dict):\n        b = x0.size(0)\n        device = x0.device\n\n        x_num = x0[:, : self.num_numerical_features]\n        x_cat = x0[:, self.num_numerical_features :]\n        has_cat = x_cat.shape[1] &gt; 0\n        if has_cat:\n            log_x_cat = index_to_log_onehot(x_cat.long(), torch.from_numpy(self.num_classes)).to(device)\n\n        gaussian_loss = []\n        xstart_mse = []\n        mse = []\n        # mu_mse = []\n        out_mean = []\n        true_mean = []\n        multinomial_loss = []\n        for t in range(self.num_timesteps):\n            t_array = (torch.ones(b, device=device) * t).long()\n            noise = torch.randn_like(x_num)\n\n            x_num_t = self.gaussian_q_sample(x_start=x_num, t=t_array, noise=noise)\n            log_x_cat_t = self.q_sample(log_x_start=log_x_cat, t=t_array) if has_cat else x_cat\n\n            model_out = self._denoise_fn(torch.cat([x_num_t, log_x_cat_t], dim=1), t_array, **out_dict)\n\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n\n            kl = torch.tensor([0.0])\n            if has_cat:\n                kl = self.compute_Lt(\n                    model_out=model_out_cat,\n                    log_x_start=log_x_cat,\n                    log_x_t=log_x_cat_t,\n                    t=t_array,\n                    out_dict=out_dict,\n                )\n\n            out = self._vb_terms_bpd(\n                model_out_num,\n                x_start=x_num,\n                x_t=x_num_t,\n                t=t_array,\n                clip_denoised=False,\n            )\n\n            multinomial_loss.append(kl)\n            gaussian_loss.append(out[\"output\"])\n            xstart_mse.append(mean_flat((out[\"pred_xstart\"] - x_num) ** 2))\n            # mu_mse.append(mean_flat(out[\"mean_mse\"]))\n            out_mean.append(mean_flat(out[\"out_mean\"]))\n            true_mean.append(mean_flat(out[\"true_mean\"]))\n\n            eps = self._predict_eps_from_xstart(x_num_t, t_array, out[\"pred_xstart\"])\n            mse.append(mean_flat((eps - noise) ** 2))\n\n        gaussian_loss_tensor = torch.stack(gaussian_loss, dim=1)\n        multinomial_loss_tensor = torch.stack(multinomial_loss, dim=1)\n        xstart_mse_tensor = torch.stack(xstart_mse, dim=1)\n        mse_tensor = torch.stack(mse, dim=1)\n        # mu_mse = torch.stack(mu_mse, dim=1)\n        out_mean_tensor = torch.stack(out_mean, dim=1)\n        true_mean_tensor = torch.stack(true_mean, dim=1)\n\n        prior_gauss = self._prior_gaussian(x_num)\n\n        prior_multin = torch.tensor([0.0])\n        if has_cat:\n            prior_multin = self.kl_prior(log_x_cat)\n\n        total_gauss = gaussian_loss_tensor.sum(dim=1) + prior_gauss\n        total_multin = multinomial_loss_tensor.sum(dim=1) + prior_multin\n        return {\n            \"total_gaussian\": total_gauss,\n            \"total_multinomial\": total_multin,\n            \"losses_gaussian\": gaussian_loss_tensor,\n            \"losses_multinimial\": multinomial_loss_tensor,\n            \"xstart_mse\": xstart_mse_tensor,\n            \"mse\": mse_tensor,\n            # \"mu_mse\": mu_mse\n            \"out_mean\": out_mean_tensor,\n            \"true_mean\": true_mean_tensor,\n        }\n\n    @torch.no_grad()\n    def gaussian_ddim_step(\n        self,\n        model_out_num: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        denoised_fn: Callable | None = None,\n        eta: float = 0.0,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; Tensor:\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        out = self.gaussian_p_mean_variance(\n            model_out_num,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=None,\n        )\n\n        if cond_fn is not None:\n            out = self.condition_score(cond_fn, out, x, t, model_kwargs=model_kwargs)\n\n        eps = self._predict_eps_from_xstart(x, t, out[\"pred_xstart\"])\n\n        alpha_bar = extract(self.alphas_cumprod, t, x.shape)\n        alpha_bar_prev = extract(self.alphas_cumprod_prev, t, x.shape)\n        sigma = eta * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar)) * torch.sqrt(1 - alpha_bar / alpha_bar_prev)\n\n        noise = torch.randn_like(x)\n        mean_pred = out[\"pred_xstart\"] * torch.sqrt(alpha_bar_prev) + torch.sqrt(1 - alpha_bar_prev - sigma**2) * eps\n        nonzero_mask = (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))  # no noise when t == 0\n        return mean_pred + nonzero_mask * sigma * noise\n\n    @torch.no_grad()\n    def gaussian_ddim_sample(self, noise, T, out_dict, eta=0.0, model_kwargs=None, cond_fn=None):\n        # ruff: noqa: D102, N803\n        x = noise\n        b = x.shape[0]\n        device = x.device\n        for t in reversed(range(T)):\n            print(f\"Sample timestep {t:4d}\", end=\"\\r\")\n            t_array = (torch.ones(b, device=device) * t).long()\n            out_num = self._denoise_fn(x, t_array, **out_dict)\n            x = self.gaussian_ddim_step(out_num, x, t_array, model_kwargs=model_kwargs, cond_fn=cond_fn)\n        print()\n        return x\n\n    @torch.no_grad()\n    def gaussian_ddim_reverse_step(\n        self,\n        model_out_num: Tensor,\n        x: Tensor,\n        t: Tensor,\n        clip_denoised: bool = False,\n        eta: float = 0.0,\n    ) -&gt; Tensor:\n        # ruff: noqa: D102\n        assert eta == 0.0, \"Eta must be zero.\"\n        out = self.gaussian_p_mean_variance(\n            model_out_num,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=None,\n            model_kwargs=None,\n        )\n\n        eps = (extract(self.sqrt_recip_alphas_cumprod, t, x.shape) * x - out[\"pred_xstart\"]) / extract(\n            self.sqrt_recipm1_alphas_cumprod, t, x.shape\n        )\n        alpha_bar_next = extract(self.alphas_cumprod_next, t, x.shape)\n\n        return out[\"pred_xstart\"] * torch.sqrt(alpha_bar_next) + torch.sqrt(1 - alpha_bar_next) * eps\n\n    @torch.no_grad()\n    def gaussian_ddim_reverse_sample(\n        self,\n        x,\n        T,\n        # ruff: noqa: N803\n        out_dict,\n    ):\n        # ruff: noqa: D102\n        b = x.shape[0]\n        device = x.device\n        for t in range(T):\n            print(f\"Reverse timestep {t:4d}\", end=\"\\r\")\n            t_array = (torch.ones(b, device=device) * t).long()\n            out_num = self._denoise_fn(x, t_array, **out_dict)\n            x = self.gaussian_ddim_reverse_step(out_num, x, t_array, eta=0.0)\n        print()\n\n        return x\n\n    @torch.no_grad()\n    def multinomial_ddim_step(\n        self,\n        model_out_cat: Tensor,\n        log_x_t: Tensor,\n        t: Tensor,\n        out_dict: dict[str, Tensor],\n        eta: float = 0.0,\n    ) -&gt; Tensor:\n        # ruff: noqa: D102\n        # not ddim, essentially\n        log_x0 = self.predict_start(model_out_cat, log_x_t=log_x_t, t=t, out_dict=out_dict)\n\n        alpha_bar = extract(self.alphas_cumprod, t, log_x_t.shape)\n        alpha_bar_prev = extract(self.alphas_cumprod_prev, t, log_x_t.shape)\n        sigma = eta * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar)) * torch.sqrt(1 - alpha_bar / alpha_bar_prev)\n\n        coef1 = sigma\n        coef2 = alpha_bar_prev - sigma * alpha_bar\n        coef3 = 1 - coef1 - coef2\n\n        log_ps = torch.stack(\n            [\n                torch.log(coef1) + log_x_t,\n                torch.log(coef2) + log_x0,\n                torch.log(coef3) - torch.log(self.num_classes_expanded),\n            ],\n            dim=2,\n        )\n\n        log_prob = torch.logsumexp(log_ps, dim=2)\n\n        return self.log_sample_categorical(log_prob)\n\n    @torch.no_grad()\n    def sample_ddim(\n        self,\n        num_samples: int,\n        y_dist: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; tuple[Tensor, dict[str, Tensor]]:\n        # ruff: noqa: D102\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        b = num_samples\n        z_norm = torch.randn((b, self.num_numerical_features), device=self.device)\n\n        assert self.num_classes is not None\n        has_cat = self.num_classes[0] != 0\n        log_z = torch.zeros((b, 0), device=self.device).float()\n        if has_cat:\n            uniform_logits = torch.zeros((b, len(self.num_classes_expanded)), device=self.device)\n            log_z = self.log_sample_categorical(uniform_logits)\n\n        y = torch.multinomial(y_dist, num_samples=b, replacement=True)\n        out_dict = {\"y\": y.long().to(self.device)}\n        for i in reversed(range(0, self.num_timesteps)):\n            print(f\"Sample timestep {i:4d}\", end=\"\\r\")\n            t = torch.full((b,), i, device=self.device, dtype=torch.long)\n            model_out = self._denoise_fn(torch.cat([z_norm, log_z], dim=1).float(), t, **out_dict)\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n            z_norm = self.gaussian_ddim_step(\n                model_out_num,\n                z_norm,\n                t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n                cond_fn=cond_fn,\n            )\n            if has_cat:\n                log_z = self.multinomial_ddim_step(model_out_cat, log_z, t, out_dict)\n\n        print()\n        z_ohe = torch.exp(log_z).round()\n        z_cat = log_z\n        if has_cat:\n            z_cat = ohe_to_categories(z_ohe, torch.from_numpy(self.num_classes))\n        sample = torch.cat([z_norm, z_cat], dim=1).cpu()\n        return sample, out_dict\n\n    @torch.no_grad()\n    def conditional_sample(\n        self,\n        ys: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; tuple[Tensor, dict[str, Tensor]]:\n        # ruff: noqa: D102\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        b = len(ys)\n        z_norm = torch.randn((b, self.num_numerical_features), device=self.device)\n        assert self.num_classes is not None\n        has_cat = self.num_classes[0] != 0\n        log_z = torch.zeros((b, 0), device=self.device).float()\n\n        out_dict = {\"y\": ys.long().to(self.device)}\n        for i in reversed(range(0, self.num_timesteps)):\n            print(f\"Sample timestep {i:4d}\", end=\"\\r\")\n            t = torch.full((b,), i, device=self.device, dtype=torch.long)\n            model_out = self._denoise_fn(torch.cat([z_norm, log_z], dim=1).float(), t, **out_dict)\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n            z_norm = self.gaussian_p_sample(\n                model_out_num,\n                z_norm,\n                t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n                cond_fn=cond_fn,\n            )[\"sample\"]\n            if has_cat:\n                log_z = self.p_sample(model_out_cat, log_z, t, out_dict)\n\n        print()\n        z_ohe = torch.exp(log_z).round()\n        z_cat = log_z\n        if has_cat:\n            z_cat = ohe_to_categories(z_ohe, torch.from_numpy(self.num_classes))\n        sample = torch.cat([z_norm, z_cat], dim=1).cpu()\n        return sample, out_dict\n\n    @torch.no_grad()\n    def sample(\n        self,\n        num_samples: int,\n        y_dist: Tensor,\n        model_kwargs: dict[str, Any] | None = None,\n        cond_fn: Callable | None = None,\n    ) -&gt; tuple[Tensor, dict[str, Tensor]]:\n        # ruff: noqa: D102\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        b = num_samples\n        z_norm = torch.randn((b, self.num_numerical_features), device=self.device)\n\n        assert self.num_classes is not None\n        has_cat = self.num_classes[0] != 0\n        log_z = torch.zeros((b, 0), device=self.device).float()\n        if has_cat:\n            uniform_logits = torch.zeros((b, len(self.num_classes_expanded)), device=self.device)\n            log_z = self.log_sample_categorical(uniform_logits)\n\n        y = torch.multinomial(y_dist, num_samples=b, replacement=True)\n        out_dict = {\"y\": y.long().to(self.device)}\n        for i in reversed(range(0, self.num_timesteps)):\n            print(f\"Sample timestep {i:4d}\", end=\"\\r\")\n            t = torch.full((b,), i, device=self.device, dtype=torch.long)\n            model_out = self._denoise_fn(torch.cat([z_norm, log_z], dim=1).float(), t, **out_dict)\n            model_out_num = model_out[:, : self.num_numerical_features]\n            model_out_cat = model_out[:, self.num_numerical_features :]\n            z_norm = self.gaussian_p_sample(\n                model_out_num,\n                z_norm,\n                t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n                cond_fn=cond_fn,\n            )[\"sample\"]\n            if has_cat:\n                log_z = self.p_sample(model_out_cat, log_z, t, out_dict)\n\n        print()\n        z_ohe = torch.exp(log_z).round()\n        z_cat = log_z\n        if has_cat:\n            z_cat = ohe_to_categories(z_ohe, torch.from_numpy(self.num_classes))\n        sample = torch.cat([z_norm, z_cat], dim=1).cpu()\n        return sample, out_dict\n\n    def sample_all(\n        self,\n        num_samples,\n        batch_size,\n        y_dist,\n        ddim=False,\n        model_kwargs=None,\n        cond_fn=None,\n    ):\n        # ruff: noqa: D102\n        if ddim:\n            print(\"Sample using DDIM.\")\n            sample_fn = self.sample_ddim\n        else:\n            sample_fn = self.sample\n\n        b = batch_size\n\n        all_y = []\n        all_samples = []\n        num_generated = 0\n        while num_generated &lt; num_samples:\n            sample, out_dict = sample_fn(b, y_dist, model_kwargs=model_kwargs, cond_fn=cond_fn)\n            mask_nan = torch.any(sample.isnan(), dim=1)\n            sample = sample[~mask_nan]\n            out_dict[\"y\"] = out_dict[\"y\"][~mask_nan]\n\n            all_samples.append(sample)\n            all_y.append(out_dict[\"y\"].cpu())\n            if sample.shape[0] != b:\n                raise FoundNANsError\n            num_generated += sample.shape[0]\n\n        x_gen = torch.cat(all_samples, dim=0)[:num_samples]\n        y_gen = torch.cat(all_y, dim=0)[:num_samples]\n\n        return x_gen, y_gen\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.GaussianMultinomialDiffusion.condition_mean","title":"condition_mean","text":"<pre><code>condition_mean(\n    cond_fn, p_mean_var, x, t, model_kwargs=None\n)\n</code></pre> <p>Compute the mean for the previous step, given a function cond_fn that computes the gradient of a conditional log probability with respect to x. In particular, cond_fn computes grad(log(p(y|x))), and we want to condition on y.</p> <p>This uses the conditioning strategy from Sohl-Dickstein et al. (2015).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def condition_mean(\n    self,\n    cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n    p_mean_var: dict[str, Tensor],\n    x: Tensor,\n    t: Tensor,\n    model_kwargs: dict[str, Any] | None = None,\n) -&gt; Tensor:\n    \"\"\"\n    Compute the mean for the previous step, given a function cond_fn that\n    computes the gradient of a conditional log probability with respect to\n    x. In particular, cond_fn computes grad(log(p(y|x))), and we want to\n    condition on y.\n\n    This uses the conditioning strategy from Sohl-Dickstein et al. (2015).\n    \"\"\"\n    if model_kwargs is None:\n        model_kwargs = {}\n\n    gradient = cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n    return p_mean_var[\"mean\"].float() + p_mean_var[\"variance\"] * gradient.float()\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.GaussianMultinomialDiffusion.condition_score","title":"condition_score","text":"<pre><code>condition_score(\n    cond_fn, p_mean_var, x, t, model_kwargs=None\n)\n</code></pre> <p>Compute what the p_mean_variance output would have been, should the model's score function be conditioned by cond_fn.</p> <p>See condition_mean() for details on cond_fn.</p> <p>Unlike condition_mean(), this instead uses the conditioning strategy from Song et al (2020).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def condition_score(\n    self,\n    cond_fn: Callable[[Tensor, Tensor, Any], Tensor],\n    p_mean_var: dict[str, Tensor],\n    x: Tensor,\n    t: Tensor,\n    model_kwargs: dict[str, Any] | None = None,\n) -&gt; dict[str, Tensor]:\n    \"\"\"\n    Compute what the p_mean_variance output would have been, should the\n    model's score function be conditioned by cond_fn.\n\n    See condition_mean() for details on cond_fn.\n\n    Unlike condition_mean(), this instead uses the conditioning strategy\n    from Song et al (2020).\n    \"\"\"\n    if model_kwargs is None:\n        model_kwargs = {}\n\n    alpha_bar = extract(self.alphas_cumprod, t, x.shape)\n\n    eps = self._predict_eps_from_xstart(x, t, p_mean_var[\"pred_xstart\"])\n    eps = eps - (1 - alpha_bar).sqrt() * cond_fn(x, t, **model_kwargs)  # type: ignore[call-arg]\n\n    out = p_mean_var.copy()\n    out[\"pred_xstart\"] = self._predict_xstart_from_eps(x, t, eps)\n    out[\"mean\"], _, _ = self.q_posterior_mean_variance(x_start=out[\"pred_xstart\"], x_t=x, t=t)\n    return out\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.get_named_beta_schedule","title":"get_named_beta_schedule","text":"<pre><code>get_named_beta_schedule(\n    schedule_name, num_diffusion_timesteps\n)\n</code></pre> <p>Get a pre-defined beta schedule for the given name. The beta schedule library consists of beta schedules which remain similar in the limit of num_diffusion_timesteps. Beta schedules may be added, but should not be removed or changed once they are committed to maintain backwards compatibility.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def get_named_beta_schedule(schedule_name: str, num_diffusion_timesteps: int) -&gt; np.ndarray:\n    \"\"\"\n    Get a pre-defined beta schedule for the given name.\n    The beta schedule library consists of beta schedules which remain similar\n    in the limit of num_diffusion_timesteps.\n    Beta schedules may be added, but should not be removed or changed once\n    they are committed to maintain backwards compatibility.\n    \"\"\"\n    if schedule_name == \"linear\":\n        # Linear schedule from Ho et al, extended to work for any number of\n        # diffusion steps.\n        scale = 1000 / num_diffusion_timesteps\n        beta_start = scale * 0.0001\n        beta_end = scale * 0.02\n        return np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64)\n    if schedule_name == \"cosine\":\n        return betas_for_alpha_bar(\n            num_diffusion_timesteps,\n            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n        )\n    raise NotImplementedError(f\"unknown beta schedule: {schedule_name}\")\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.gaussian_multinomial_diffusion.betas_for_alpha_bar","title":"betas_for_alpha_bar","text":"<pre><code>betas_for_alpha_bar(\n    num_diffusion_timesteps, alpha_bar, max_beta=0.999\n)\n</code></pre> <p>Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of (1-beta) over time from t = [0,1]. :param num_diffusion_timesteps: the number of betas to produce. :param alpha_bar: a lambda that takes an argument t from 0 to 1 and                   produces the cumulative product of (1-beta) up to that                   part of the diffusion process. :param max_beta: the maximum beta to use; use values lower than 1 to                  prevent singularities.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/gaussian_multinomial_diffusion.py</code> <pre><code>def betas_for_alpha_bar(num_diffusion_timesteps: int, alpha_bar: Callable, max_beta: float = 0.999) -&gt; np.ndarray:\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function,\n    which defines the cumulative product of (1-beta) over time from t = [0,1].\n    :param num_diffusion_timesteps: the number of betas to produce.\n    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n                      produces the cumulative product of (1-beta) up to that\n                      part of the diffusion process.\n    :param max_beta: the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n    \"\"\"\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return np.array(betas)\n</code></pre>"},{"location":"api/#clavaddpm-model-module","title":"ClavaDDPM Model Module","text":""},{"location":"api/#midst_toolkit.models.clavaddpm.model","title":"midst_toolkit.models.clavaddpm.model","text":""},{"location":"api/#midst_toolkit.models.clavaddpm.model.ScheduleSampler","title":"ScheduleSampler","text":"<p>               Bases: <code>ABC</code></p> <p>A distribution over timesteps in the diffusion process, intended to reduce variance of the objective.</p> <p>By default, samplers perform unbiased importance sampling, in which the objective's mean is unchanged. However, subclasses may override sample() to change how the resampled terms are reweighted, allowing for actual changes in the objective.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class ScheduleSampler(ABC):\n    \"\"\"\n    A distribution over timesteps in the diffusion process, intended to reduce\n    variance of the objective.\n\n    By default, samplers perform unbiased importance sampling, in which the\n    objective's mean is unchanged.\n    However, subclasses may override sample() to change how the resampled\n    terms are reweighted, allowing for actual changes in the objective.\n    \"\"\"\n\n    @abstractmethod\n    def weights(self) -&gt; Tensor:\n        \"\"\"\n        Get a numpy array of weights, one per diffusion step.\n\n        The weights needn't be normalized, but must be positive.\n        \"\"\"\n\n    def sample(self, batch_size: int, device: str) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"\n        Importance-sample timesteps for a batch.\n\n        :param batch_size: the number of timesteps.\n        :param device: the torch device to save to.\n        :return: a tuple (timesteps, weights):\n                 - timesteps: a tensor of timestep indices.\n                 - weights: a tensor of weights to scale the resulting losses.\n        \"\"\"\n        w = self.weights().cpu().numpy()\n        p = w / np.sum(w)\n        indices_np = np.random.choice(len(p), size=(batch_size,), p=p)\n        indices = torch.from_numpy(indices_np).long().to(device)\n        weights_np = 1 / (len(p) * p[indices_np])\n        weights = torch.from_numpy(weights_np).float().to(device)\n        return indices, weights\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ScheduleSampler.weights","title":"weights  <code>abstractmethod</code>","text":"<pre><code>weights()\n</code></pre> <p>Get a numpy array of weights, one per diffusion step.</p> <p>The weights needn't be normalized, but must be positive.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>@abstractmethod\ndef weights(self) -&gt; Tensor:\n    \"\"\"\n    Get a numpy array of weights, one per diffusion step.\n\n    The weights needn't be normalized, but must be positive.\n    \"\"\"\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ScheduleSampler.sample","title":"sample","text":"<pre><code>sample(batch_size, device)\n</code></pre> <p>Importance-sample timesteps for a batch.</p> <p>:param batch_size: the number of timesteps. :param device: the torch device to save to. :return: a tuple (timesteps, weights):          - timesteps: a tensor of timestep indices.          - weights: a tensor of weights to scale the resulting losses.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def sample(self, batch_size: int, device: str) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"\n    Importance-sample timesteps for a batch.\n\n    :param batch_size: the number of timesteps.\n    :param device: the torch device to save to.\n    :return: a tuple (timesteps, weights):\n             - timesteps: a tensor of timestep indices.\n             - weights: a tensor of weights to scale the resulting losses.\n    \"\"\"\n    w = self.weights().cpu().numpy()\n    p = w / np.sum(w)\n    indices_np = np.random.choice(len(p), size=(batch_size,), p=p)\n    indices = torch.from_numpy(indices_np).long().to(device)\n    weights_np = 1 / (len(p) * p[indices_np])\n    weights = torch.from_numpy(weights_np).float().to(device)\n    return indices, weights\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.LossAwareSampler","title":"LossAwareSampler","text":"<p>               Bases: <code>ScheduleSampler</code></p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class LossAwareSampler(ScheduleSampler):\n    def update_with_local_losses(self, local_ts: Tensor, local_losses: Tensor) -&gt; None:\n        \"\"\"\n        Update the reweighting using losses from a model.\n\n        Call this method from each rank with a batch of timesteps and the\n        corresponding losses for each of those timesteps.\n        This method will perform synchronization to make sure all of the ranks\n        maintain the exact same reweighting.\n\n        :param local_ts: an integer Tensor of timesteps.\n        :param local_losses: a 1D Tensor of losses.\n        \"\"\"\n        batch_sizes = [\n            torch.tensor([0], dtype=torch.int32, device=local_ts.device)\n            for _ in range(torch.distributed.get_world_size())\n        ]\n        torch.distributed.all_gather(\n            batch_sizes,\n            torch.tensor([len(local_ts)], dtype=torch.int32, device=local_ts.device),\n        )\n\n        # Pad all_gather batches to be the maximum batch size.\n        max_bs = max([int(x.item()) for x in batch_sizes])\n\n        timestep_batches = [torch.zeros(max_bs).to(local_ts) for bs in batch_sizes]\n        loss_batches = [torch.zeros(max_bs).to(local_losses) for bs in batch_sizes]\n        torch.distributed.all_gather(timestep_batches, local_ts)\n        torch.distributed.all_gather(loss_batches, local_losses)\n        timesteps = [x.item() for y, bs in zip(timestep_batches, batch_sizes) for x in y[:bs]]\n        losses = [x.item() for y, bs in zip(loss_batches, batch_sizes) for x in y[:bs]]\n        self.update_with_all_losses(timesteps, losses)\n\n    @abstractmethod\n    def update_with_all_losses(self, ts: list[int], losses: list[float]) -&gt; None:\n        \"\"\"\n        Update the reweighting using losses from a model.\n\n        Sub-classes should override this method to update the reweighting\n        using losses from the model.\n\n        This method directly updates the reweighting without synchronizing\n        between workers. It is called by update_with_local_losses from all\n        ranks with identical arguments. Thus, it should have deterministic\n        behavior to maintain state across workers.\n\n        :param ts: a list of int timesteps.\n        :param losses: a list of float losses, one per timestep.\n        \"\"\"\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.LossAwareSampler.update_with_local_losses","title":"update_with_local_losses","text":"<pre><code>update_with_local_losses(local_ts, local_losses)\n</code></pre> <p>Update the reweighting using losses from a model.</p> <p>Call this method from each rank with a batch of timesteps and the corresponding losses for each of those timesteps. This method will perform synchronization to make sure all of the ranks maintain the exact same reweighting.</p> <p>:param local_ts: an integer Tensor of timesteps. :param local_losses: a 1D Tensor of losses.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def update_with_local_losses(self, local_ts: Tensor, local_losses: Tensor) -&gt; None:\n    \"\"\"\n    Update the reweighting using losses from a model.\n\n    Call this method from each rank with a batch of timesteps and the\n    corresponding losses for each of those timesteps.\n    This method will perform synchronization to make sure all of the ranks\n    maintain the exact same reweighting.\n\n    :param local_ts: an integer Tensor of timesteps.\n    :param local_losses: a 1D Tensor of losses.\n    \"\"\"\n    batch_sizes = [\n        torch.tensor([0], dtype=torch.int32, device=local_ts.device)\n        for _ in range(torch.distributed.get_world_size())\n    ]\n    torch.distributed.all_gather(\n        batch_sizes,\n        torch.tensor([len(local_ts)], dtype=torch.int32, device=local_ts.device),\n    )\n\n    # Pad all_gather batches to be the maximum batch size.\n    max_bs = max([int(x.item()) for x in batch_sizes])\n\n    timestep_batches = [torch.zeros(max_bs).to(local_ts) for bs in batch_sizes]\n    loss_batches = [torch.zeros(max_bs).to(local_losses) for bs in batch_sizes]\n    torch.distributed.all_gather(timestep_batches, local_ts)\n    torch.distributed.all_gather(loss_batches, local_losses)\n    timesteps = [x.item() for y, bs in zip(timestep_batches, batch_sizes) for x in y[:bs]]\n    losses = [x.item() for y, bs in zip(loss_batches, batch_sizes) for x in y[:bs]]\n    self.update_with_all_losses(timesteps, losses)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.LossAwareSampler.update_with_all_losses","title":"update_with_all_losses  <code>abstractmethod</code>","text":"<pre><code>update_with_all_losses(ts, losses)\n</code></pre> <p>Update the reweighting using losses from a model.</p> <p>Sub-classes should override this method to update the reweighting using losses from the model.</p> <p>This method directly updates the reweighting without synchronizing between workers. It is called by update_with_local_losses from all ranks with identical arguments. Thus, it should have deterministic behavior to maintain state across workers.</p> <p>:param ts: a list of int timesteps. :param losses: a list of float losses, one per timestep.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>@abstractmethod\ndef update_with_all_losses(self, ts: list[int], losses: list[float]) -&gt; None:\n    \"\"\"\n    Update the reweighting using losses from a model.\n\n    Sub-classes should override this method to update the reweighting\n    using losses from the model.\n\n    This method directly updates the reweighting without synchronizing\n    between workers. It is called by update_with_local_losses from all\n    ranks with identical arguments. Thus, it should have deterministic\n    behavior to maintain state across workers.\n\n    :param ts: a list of int timesteps.\n    :param losses: a list of float losses, one per timestep.\n    \"\"\"\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.FastTensorDataLoader","title":"FastTensorDataLoader","text":"<p>Defines a faster dataloader for PyTorch tensors.</p> <p>A DataLoader-like object for a set of tensors that can be much faster than TensorDataset + DataLoader because dataloader grabs individual indices of the dataset and calls cat (slow). Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class FastTensorDataLoader:\n    \"\"\"\n    Defines a faster dataloader for PyTorch tensors.\n\n    A DataLoader-like object for a set of tensors that can be much faster than\n    TensorDataset + DataLoader because dataloader grabs individual indices of\n    the dataset and calls cat (slow).\n    Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6\n    \"\"\"\n\n    def __init__(self, *tensors: Tensor, batch_size: int = 32, shuffle: bool = False):\n        \"\"\"\n        Initialize a FastTensorDataLoader.\n        :param *tensors: tensors to store. Must have the same length @ dim 0.\n        :param batch_size: batch size to load.\n        :param shuffle: if True, shuffle the data *in-place* whenever an\n            iterator is created out of this object.\n        :returns: A FastTensorDataLoader.\n        \"\"\"\n        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n        self.tensors = tensors\n\n        self.dataset_len = self.tensors[0].shape[0]\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n\n        # Calculate # batches\n        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n        if remainder &gt; 0:\n            n_batches += 1\n        self.n_batches = n_batches\n\n    def __iter__(self):\n        # ruff: noqa: D105\n        if self.shuffle:\n            r = torch.randperm(self.dataset_len)\n            self.tensors = [t[r] for t in self.tensors]  # type: ignore[assignment]\n        self.i = 0\n        return self\n\n    def __next__(self):\n        # ruff: noqa: D105\n        if self.i &gt;= self.dataset_len:\n            raise StopIteration\n        batch = tuple(t[self.i : self.i + self.batch_size] for t in self.tensors)\n        self.i += self.batch_size\n        return batch\n\n    def __len__(self):\n        # ruff: noqa: D105\n        return self.n_batches\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.FastTensorDataLoader.__init__","title":"__init__","text":"<pre><code>__init__(*tensors, batch_size=32, shuffle=False)\n</code></pre> <p>Initialize a FastTensorDataLoader. :param tensors: tensors to store. Must have the same length @ dim 0. :param batch_size: batch size to load. :param shuffle: if True, shuffle the data in-place* whenever an     iterator is created out of this object. :returns: A FastTensorDataLoader.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def __init__(self, *tensors: Tensor, batch_size: int = 32, shuffle: bool = False):\n    \"\"\"\n    Initialize a FastTensorDataLoader.\n    :param *tensors: tensors to store. Must have the same length @ dim 0.\n    :param batch_size: batch size to load.\n    :param shuffle: if True, shuffle the data *in-place* whenever an\n        iterator is created out of this object.\n    :returns: A FastTensorDataLoader.\n    \"\"\"\n    assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n    self.tensors = tensors\n\n    self.dataset_len = self.tensors[0].shape[0]\n    self.batch_size = batch_size\n    self.shuffle = shuffle\n\n    # Calculate # batches\n    n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n    if remainder &gt; 0:\n        n_batches += 1\n    self.n_batches = n_batches\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.MLP","title":"MLP","text":"<p>               Bases: <code>Module</code></p> <p>The MLP model used in [gorishniy2021revisiting].</p> <p>The following scheme describes the architecture:</p> <p>.. code-block:: text</p> <pre><code>  MLP: (in) -&gt; Block -&gt; ... -&gt; Block -&gt; Linear -&gt; (out)\nBlock: (in) -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; (out)\n</code></pre> <p>Examples:</p> <p>.. testcode::</p> <pre><code>x = torch.randn(4, 2)\nmodule = MLP.make_baseline(x.shape[1], [3, 5], 0.1, 1)\nassert module(x).shape == (len(x), 1)\n</code></pre> References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class MLP(nn.Module):\n    \"\"\"The MLP model used in [gorishniy2021revisiting].\n\n    The following scheme describes the architecture:\n\n    .. code-block:: text\n\n          MLP: (in) -&gt; Block -&gt; ... -&gt; Block -&gt; Linear -&gt; (out)\n        Block: (in) -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; (out)\n\n    Examples:\n        .. testcode::\n\n            x = torch.randn(4, 2)\n            module = MLP.make_baseline(x.shape[1], [3, 5], 0.1, 1)\n            assert module(x).shape == (len(x), 1)\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n\n    class Block(nn.Module):\n        \"\"\"The main building block of `MLP`.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            d_in: int,\n            d_out: int,\n            bias: bool,\n            activation: ModuleType,\n            dropout: float,\n        ) -&gt; None:\n            super().__init__()\n            self.linear = nn.Linear(d_in, d_out, bias)\n            self.activation = _make_nn_module(activation)\n            self.dropout = nn.Dropout(dropout)\n\n        def forward(self, x: Tensor) -&gt; Tensor:\n            return self.dropout(self.activation(self.linear(x)))\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        d_layers: list[int],\n        dropouts: float | list[float],\n        activation: str | Callable[[], nn.Module],\n        d_out: int,\n    ) -&gt; None:\n        \"\"\"\n        Note:\n            `make_baseline` is the recommended constructor.\n        \"\"\"\n        super().__init__()\n        if isinstance(dropouts, float):\n            dropouts = [dropouts] * len(d_layers)\n        assert len(d_layers) == len(dropouts)\n        assert activation not in [\"ReGLU\", \"GEGLU\"]\n\n        self.blocks = nn.ModuleList(\n            [\n                MLP.Block(\n                    d_in=d_layers[i - 1] if i else d_in,\n                    d_out=d,\n                    bias=True,\n                    activation=activation,\n                    dropout=dropout,\n                )\n                for i, (d, dropout) in enumerate(zip(d_layers, dropouts))\n            ]\n        )\n        self.head = nn.Linear(d_layers[-1] if d_layers else d_in, d_out)\n\n    @classmethod\n    def make_baseline(\n        cls,\n        d_in: int,\n        d_layers: list[int],\n        dropout: float,\n        d_out: int,\n    ) -&gt; Self:\n        \"\"\"Create a \"baseline\" `MLP`.\n\n        This variation of MLP was used in [gorishniy2021revisiting]. Features:\n\n        * :code:`Activation` = :code:`ReLU`\n        * all linear layers except for the first one and the last one are of the same dimension\n        * the dropout rate is the same for all dropout layers\n\n        Args:\n            d_in: the input size\n            d_layers: the dimensions of the linear layers. If there are more than two\n                layers, then all of them except for the first and the last ones must\n                have the same dimension. Valid examples: :code:`[]`, :code:`[8]`,\n                :code:`[8, 16]`, :code:`[2, 2, 2, 2]`, :code:`[1, 2, 2, 4]`. Invalid\n                example: :code:`[1, 2, 3, 4]`.\n            dropout: the dropout rate for all hidden layers\n            d_out: the output size\n        Returns:\n            MLP\n\n        References:\n            * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n            Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n        \"\"\"\n        assert isinstance(dropout, float)\n        if len(d_layers) &gt; 2:\n            assert len(set(d_layers[1:-1])) == 1, (\n                \"if d_layers contains more than two elements, then\"\n                \" all elements except for the first and the last ones must be equal.\"\n            )\n        return cls(\n            d_in=d_in,\n            d_layers=d_layers,\n            dropouts=dropout,\n            activation=\"ReLU\",\n            d_out=d_out,\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = x.float()\n        for block in self.blocks:\n            x = block(x)\n        return self.head(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.MLP.Block","title":"Block","text":"<p>               Bases: <code>Module</code></p> <p>The main building block of <code>MLP</code>.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class Block(nn.Module):\n    \"\"\"The main building block of `MLP`.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        d_out: int,\n        bias: bool,\n        activation: ModuleType,\n        dropout: float,\n    ) -&gt; None:\n        super().__init__()\n        self.linear = nn.Linear(d_in, d_out, bias)\n        self.activation = _make_nn_module(activation)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        return self.dropout(self.activation(self.linear(x)))\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.MLP.__init__","title":"__init__","text":"<pre><code>__init__(*, d_in, d_layers, dropouts, activation, d_out)\n</code></pre> Note <p><code>make_baseline</code> is the recommended constructor.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def __init__(\n    self,\n    *,\n    d_in: int,\n    d_layers: list[int],\n    dropouts: float | list[float],\n    activation: str | Callable[[], nn.Module],\n    d_out: int,\n) -&gt; None:\n    \"\"\"\n    Note:\n        `make_baseline` is the recommended constructor.\n    \"\"\"\n    super().__init__()\n    if isinstance(dropouts, float):\n        dropouts = [dropouts] * len(d_layers)\n    assert len(d_layers) == len(dropouts)\n    assert activation not in [\"ReGLU\", \"GEGLU\"]\n\n    self.blocks = nn.ModuleList(\n        [\n            MLP.Block(\n                d_in=d_layers[i - 1] if i else d_in,\n                d_out=d,\n                bias=True,\n                activation=activation,\n                dropout=dropout,\n            )\n            for i, (d, dropout) in enumerate(zip(d_layers, dropouts))\n        ]\n    )\n    self.head = nn.Linear(d_layers[-1] if d_layers else d_in, d_out)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.MLP.make_baseline","title":"make_baseline  <code>classmethod</code>","text":"<pre><code>make_baseline(d_in, d_layers, dropout, d_out)\n</code></pre> <p>Create a \"baseline\" <code>MLP</code>.</p> <p>This variation of MLP was used in [gorishniy2021revisiting]. Features:</p> <ul> <li>:code:<code>Activation</code> = :code:<code>ReLU</code></li> <li>all linear layers except for the first one and the last one are of the same dimension</li> <li>the dropout rate is the same for all dropout layers</li> </ul> <p>Parameters:</p> Name Type Description Default <code>d_in</code> <code>int</code> <p>the input size</p> required <code>d_layers</code> <code>list[int]</code> <p>the dimensions of the linear layers. If there are more than two layers, then all of them except for the first and the last ones must have the same dimension. Valid examples: :code:<code>[]</code>, :code:<code>[8]</code>, :code:<code>[8, 16]</code>, :code:<code>[2, 2, 2, 2]</code>, :code:<code>[1, 2, 2, 4]</code>. Invalid example: :code:<code>[1, 2, 3, 4]</code>.</p> required <code>dropout</code> <code>float</code> <p>the dropout rate for all hidden layers</p> required <code>d_out</code> <code>int</code> <p>the output size</p> required <p>Returns:     MLP</p> References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>@classmethod\ndef make_baseline(\n    cls,\n    d_in: int,\n    d_layers: list[int],\n    dropout: float,\n    d_out: int,\n) -&gt; Self:\n    \"\"\"Create a \"baseline\" `MLP`.\n\n    This variation of MLP was used in [gorishniy2021revisiting]. Features:\n\n    * :code:`Activation` = :code:`ReLU`\n    * all linear layers except for the first one and the last one are of the same dimension\n    * the dropout rate is the same for all dropout layers\n\n    Args:\n        d_in: the input size\n        d_layers: the dimensions of the linear layers. If there are more than two\n            layers, then all of them except for the first and the last ones must\n            have the same dimension. Valid examples: :code:`[]`, :code:`[8]`,\n            :code:`[8, 16]`, :code:`[2, 2, 2, 2]`, :code:`[1, 2, 2, 4]`. Invalid\n            example: :code:`[1, 2, 3, 4]`.\n        dropout: the dropout rate for all hidden layers\n        d_out: the output size\n    Returns:\n        MLP\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n    assert isinstance(dropout, float)\n    if len(d_layers) &gt; 2:\n        assert len(set(d_layers[1:-1])) == 1, (\n            \"if d_layers contains more than two elements, then\"\n            \" all elements except for the first and the last ones must be equal.\"\n        )\n    return cls(\n        d_in=d_in,\n        d_layers=d_layers,\n        dropouts=dropout,\n        activation=\"ReLU\",\n        d_out=d_out,\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ResNet","title":"ResNet","text":"<p>               Bases: <code>Module</code></p> <p>The ResNet model used in [gorishniy2021revisiting].</p> <p>The following scheme describes the architecture: .. code-block:: text     ResNet: (in) -&gt; Linear -&gt; Block -&gt; ... -&gt; Block -&gt; Head -&gt; (out)              |-&gt; Norm -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; Linear -&gt; Dropout -&gt;|              |                                                                  |      Block: (in) ------------------------------------------------------------&gt; Add -&gt; (out)       Head: (in) -&gt; Norm -&gt; Activation -&gt; Linear -&gt; (out)</p> <p>Examples:</p> <p>.. testcode::     x = torch.randn(4, 2)     module = ResNet.make_baseline(         d_in=x.shape[1],         n_blocks=2,         d_main=3,         d_hidden=4,         dropout_first=0.25,         dropout_second=0.0,         d_out=1     )     assert module(x).shape == (len(x), 1)</p> References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class ResNet(nn.Module):\n    \"\"\"\n    The ResNet model used in [gorishniy2021revisiting].\n\n    The following scheme describes the architecture:\n    .. code-block:: text\n        ResNet: (in) -&gt; Linear -&gt; Block -&gt; ... -&gt; Block -&gt; Head -&gt; (out)\n                 |-&gt; Norm -&gt; Linear -&gt; Activation -&gt; Dropout -&gt; Linear -&gt; Dropout -&gt;|\n                 |                                                                  |\n         Block: (in) ------------------------------------------------------------&gt; Add -&gt; (out)\n          Head: (in) -&gt; Norm -&gt; Activation -&gt; Linear -&gt; (out)\n\n    Examples:\n        .. testcode::\n            x = torch.randn(4, 2)\n            module = ResNet.make_baseline(\n                d_in=x.shape[1],\n                n_blocks=2,\n                d_main=3,\n                d_hidden=4,\n                dropout_first=0.25,\n                dropout_second=0.0,\n                d_out=1\n            )\n            assert module(x).shape == (len(x), 1)\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n\n    class Block(nn.Module):\n        \"\"\"The main building block of `ResNet`.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            d_main: int,\n            d_hidden: int,\n            bias_first: bool,\n            bias_second: bool,\n            dropout_first: float,\n            dropout_second: float,\n            normalization: ModuleType,\n            activation: ModuleType,\n            skip_connection: bool,\n        ) -&gt; None:\n            super().__init__()\n            self.normalization = _make_nn_module(normalization, d_main)\n            self.linear_first = nn.Linear(d_main, d_hidden, bias_first)\n            self.activation = _make_nn_module(activation)\n            self.dropout_first = nn.Dropout(dropout_first)\n            self.linear_second = nn.Linear(d_hidden, d_main, bias_second)\n            self.dropout_second = nn.Dropout(dropout_second)\n            self.skip_connection = skip_connection\n\n        def forward(self, x: Tensor) -&gt; Tensor:\n            x_input = x\n            x = self.normalization(x)\n            x = self.linear_first(x)\n            x = self.activation(x)\n            x = self.dropout_first(x)\n            x = self.linear_second(x)\n            x = self.dropout_second(x)\n            if self.skip_connection:\n                x = x_input + x\n            return x\n\n    class Head(nn.Module):\n        \"\"\"The final module of `ResNet`.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            d_in: int,\n            d_out: int,\n            bias: bool,\n            normalization: ModuleType,\n            activation: ModuleType,\n        ) -&gt; None:\n            super().__init__()\n            self.normalization = _make_nn_module(normalization, d_in)\n            self.activation = _make_nn_module(activation)\n            self.linear = nn.Linear(d_in, d_out, bias)\n\n        def forward(self, x: Tensor) -&gt; Tensor:\n            if self.normalization is not None:\n                x = self.normalization(x)\n            x = self.activation(x)\n            return self.linear(x)\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        n_blocks: int,\n        d_main: int,\n        d_hidden: int,\n        dropout_first: float,\n        dropout_second: float,\n        normalization: ModuleType,\n        activation: ModuleType,\n        d_out: int,\n    ) -&gt; None:\n        \"\"\"\n        Note:\n            `make_baseline` is the recommended constructor.\n        \"\"\"\n        super().__init__()\n\n        self.first_layer = nn.Linear(d_in, d_main)\n        if d_main is None:\n            d_main = d_in\n        self.blocks = nn.Sequential(\n            *[\n                ResNet.Block(\n                    d_main=d_main,\n                    d_hidden=d_hidden,\n                    bias_first=True,\n                    bias_second=True,\n                    dropout_first=dropout_first,\n                    dropout_second=dropout_second,\n                    normalization=normalization,\n                    activation=activation,\n                    skip_connection=True,\n                )\n                for _ in range(n_blocks)\n            ]\n        )\n        self.head = ResNet.Head(\n            d_in=d_main,\n            d_out=d_out,\n            bias=True,\n            normalization=normalization,\n            activation=activation,\n        )\n\n    @classmethod\n    def make_baseline(\n        cls,\n        *,\n        d_in: int,\n        n_blocks: int,\n        d_main: int,\n        d_hidden: int,\n        dropout_first: float,\n        dropout_second: float,\n        d_out: int,\n    ) -&gt; Self:\n        \"\"\"Create a \"baseline\" `ResNet`.\n        This variation of ResNet was used in [gorishniy2021revisiting]. Features:\n        * :code:`Activation` = :code:`ReLU`\n        * :code:`Norm` = :code:`BatchNorm1d`\n        Args:\n            d_in: the input size\n            n_blocks: the number of Blocks\n            d_main: the input size (or, equivalently, the output size) of each Block\n            d_hidden: the output size of the first linear layer in each Block\n            dropout_first: the dropout rate of the first dropout layer in each Block.\n            dropout_second: the dropout rate of the second dropout layer in each Block.\n\n        References:\n            * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n            Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n        \"\"\"\n        return cls(\n            d_in=d_in,\n            n_blocks=n_blocks,\n            d_main=d_main,\n            d_hidden=d_hidden,\n            dropout_first=dropout_first,\n            dropout_second=dropout_second,\n            normalization=\"BatchNorm1d\",\n            activation=\"ReLU\",\n            d_out=d_out,\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = x.float()\n        x = self.first_layer(x)\n        x = self.blocks(x)\n        return self.head(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ResNet.Block","title":"Block","text":"<p>               Bases: <code>Module</code></p> <p>The main building block of <code>ResNet</code>.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class Block(nn.Module):\n    \"\"\"The main building block of `ResNet`.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        d_main: int,\n        d_hidden: int,\n        bias_first: bool,\n        bias_second: bool,\n        dropout_first: float,\n        dropout_second: float,\n        normalization: ModuleType,\n        activation: ModuleType,\n        skip_connection: bool,\n    ) -&gt; None:\n        super().__init__()\n        self.normalization = _make_nn_module(normalization, d_main)\n        self.linear_first = nn.Linear(d_main, d_hidden, bias_first)\n        self.activation = _make_nn_module(activation)\n        self.dropout_first = nn.Dropout(dropout_first)\n        self.linear_second = nn.Linear(d_hidden, d_main, bias_second)\n        self.dropout_second = nn.Dropout(dropout_second)\n        self.skip_connection = skip_connection\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x_input = x\n        x = self.normalization(x)\n        x = self.linear_first(x)\n        x = self.activation(x)\n        x = self.dropout_first(x)\n        x = self.linear_second(x)\n        x = self.dropout_second(x)\n        if self.skip_connection:\n            x = x_input + x\n        return x\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ResNet.Head","title":"Head","text":"<p>               Bases: <code>Module</code></p> <p>The final module of <code>ResNet</code>.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class Head(nn.Module):\n    \"\"\"The final module of `ResNet`.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        d_in: int,\n        d_out: int,\n        bias: bool,\n        normalization: ModuleType,\n        activation: ModuleType,\n    ) -&gt; None:\n        super().__init__()\n        self.normalization = _make_nn_module(normalization, d_in)\n        self.activation = _make_nn_module(activation)\n        self.linear = nn.Linear(d_in, d_out, bias)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        if self.normalization is not None:\n            x = self.normalization(x)\n        x = self.activation(x)\n        return self.linear(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ResNet.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    d_in,\n    n_blocks,\n    d_main,\n    d_hidden,\n    dropout_first,\n    dropout_second,\n    normalization,\n    activation,\n    d_out,\n)\n</code></pre> Note <p><code>make_baseline</code> is the recommended constructor.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def __init__(\n    self,\n    *,\n    d_in: int,\n    n_blocks: int,\n    d_main: int,\n    d_hidden: int,\n    dropout_first: float,\n    dropout_second: float,\n    normalization: ModuleType,\n    activation: ModuleType,\n    d_out: int,\n) -&gt; None:\n    \"\"\"\n    Note:\n        `make_baseline` is the recommended constructor.\n    \"\"\"\n    super().__init__()\n\n    self.first_layer = nn.Linear(d_in, d_main)\n    if d_main is None:\n        d_main = d_in\n    self.blocks = nn.Sequential(\n        *[\n            ResNet.Block(\n                d_main=d_main,\n                d_hidden=d_hidden,\n                bias_first=True,\n                bias_second=True,\n                dropout_first=dropout_first,\n                dropout_second=dropout_second,\n                normalization=normalization,\n                activation=activation,\n                skip_connection=True,\n            )\n            for _ in range(n_blocks)\n        ]\n    )\n    self.head = ResNet.Head(\n        d_in=d_main,\n        d_out=d_out,\n        bias=True,\n        normalization=normalization,\n        activation=activation,\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ResNet.make_baseline","title":"make_baseline  <code>classmethod</code>","text":"<pre><code>make_baseline(\n    *,\n    d_in,\n    n_blocks,\n    d_main,\n    d_hidden,\n    dropout_first,\n    dropout_second,\n    d_out,\n)\n</code></pre> <p>Create a \"baseline\" <code>ResNet</code>. This variation of ResNet was used in [gorishniy2021revisiting]. Features: * :code:<code>Activation</code> = :code:<code>ReLU</code> * :code:<code>Norm</code> = :code:<code>BatchNorm1d</code> Args:     d_in: the input size     n_blocks: the number of Blocks     d_main: the input size (or, equivalently, the output size) of each Block     d_hidden: the output size of the first linear layer in each Block     dropout_first: the dropout rate of the first dropout layer in each Block.     dropout_second: the dropout rate of the second dropout layer in each Block.</p> References <ul> <li>[gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>@classmethod\ndef make_baseline(\n    cls,\n    *,\n    d_in: int,\n    n_blocks: int,\n    d_main: int,\n    d_hidden: int,\n    dropout_first: float,\n    dropout_second: float,\n    d_out: int,\n) -&gt; Self:\n    \"\"\"Create a \"baseline\" `ResNet`.\n    This variation of ResNet was used in [gorishniy2021revisiting]. Features:\n    * :code:`Activation` = :code:`ReLU`\n    * :code:`Norm` = :code:`BatchNorm1d`\n    Args:\n        d_in: the input size\n        n_blocks: the number of Blocks\n        d_main: the input size (or, equivalently, the output size) of each Block\n        d_hidden: the output size of the first linear layer in each Block\n        dropout_first: the dropout rate of the first dropout layer in each Block.\n        dropout_second: the dropout rate of the second dropout layer in each Block.\n\n    References:\n        * [gorishniy2021revisiting] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\n        Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data\", 2021\n    \"\"\"\n    return cls(\n        d_in=d_in,\n        n_blocks=n_blocks,\n        d_main=d_main,\n        d_hidden=d_hidden,\n        dropout_first=dropout_first,\n        dropout_second=dropout_second,\n        normalization=\"BatchNorm1d\",\n        activation=\"ReLU\",\n        d_out=d_out,\n    )\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.ReGLU","title":"ReGLU","text":"<p>               Bases: <code>Module</code></p> <p>The ReGLU activation function from [shazeer2020glu].</p> <p>Examples:</p> <p>.. testcode::</p> <pre><code>module = ReGLU()\nx = torch.randn(3, 4)\nassert module(x).shape == (3, 2)\n</code></pre> References <ul> <li>[shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class ReGLU(nn.Module):\n    \"\"\"The ReGLU activation function from [shazeer2020glu].\n\n    Examples:\n        .. testcode::\n\n            module = ReGLU()\n            x = torch.randn(3, 4)\n            assert module(x).shape == (3, 2)\n\n    References:\n        * [shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        # ruff: noqa: D102\n        return reglu(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.GEGLU","title":"GEGLU","text":"<p>               Bases: <code>Module</code></p> <p>The GEGLU activation function from [shazeer2020glu].</p> <p>Examples:</p> <p>.. testcode::</p> <pre><code>module = GEGLU()\nx = torch.randn(3, 4)\nassert module(x).shape == (3, 2)\n</code></pre> References <ul> <li>[shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</li> </ul> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>class GEGLU(nn.Module):\n    \"\"\"The GEGLU activation function from [shazeer2020glu].\n\n    Examples:\n        .. testcode::\n\n            module = GEGLU()\n            x = torch.randn(3, 4)\n            assert module(x).shape == (3, 2)\n\n    References:\n        * [shazeer2020glu] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        # ruff: noqa: D102\n        return geglu(x)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.make_dataset_from_df","title":"make_dataset_from_df","text":"<pre><code>make_dataset_from_df(\n    df, T, is_y_cond, df_info, ratios=None, std=0\n)\n</code></pre> <p>The order of the generated dataset: (y, X_num, X_cat).</p> is_y_cond <p>concat: y is concatenated to X, the model learn a joint distribution of (y, X) embedding: y is not concatenated to X. During computations, y is embedded     and added to the latent vector of X none: y column is completely ignored</p> <p>How does is_y_cond affect the generation of y? is_y_cond:     concat: the model synthesizes (y, X) directly, so y is just the first column     embedding: y is first sampled using empirical distribution of y. The model only         synthesizes X. When returning the generated data, we return the generated X         and the sampled y. (y is sampled from empirical distribution, instead of being         generated by the model)         Note that in this way, y is still not independent of X, because the model has been         adding the embedding of y to the latent vector of X during computations.     none:         y is synthesized using y's empirical distribution. X is generated by the model.         In this case, y is completely independent of X.</p> <p>Note: For now, n_classes has to be set to 0. This is because our matrix is the concatenation of (X_num, X_cat). In this case, if we have is_y_cond == 'concat', we can guarantee that y is the first column of the matrix. However, if we have n_classes &gt; 0, then y is not the first column of the matrix.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def make_dataset_from_df(\n    # ruff: noqa: PLR0915, PLR0912\n    df: pd.DataFrame,\n    T: Transformations,\n    is_y_cond: str,\n    df_info: pd.DataFrame,\n    ratios: list[float] | None = None,\n    std: float = 0,\n) -&gt; tuple[Dataset, dict[int, LabelEncoder], list[int]]:\n    \"\"\"\n    The order of the generated dataset: (y, X_num, X_cat).\n\n    is_y_cond:\n        concat: y is concatenated to X, the model learn a joint distribution of (y, X)\n        embedding: y is not concatenated to X. During computations, y is embedded\n            and added to the latent vector of X\n        none: y column is completely ignored\n\n    How does is_y_cond affect the generation of y?\n    is_y_cond:\n        concat: the model synthesizes (y, X) directly, so y is just the first column\n        embedding: y is first sampled using empirical distribution of y. The model only\n            synthesizes X. When returning the generated data, we return the generated X\n            and the sampled y. (y is sampled from empirical distribution, instead of being\n            generated by the model)\n            Note that in this way, y is still not independent of X, because the model has been\n            adding the embedding of y to the latent vector of X during computations.\n        none:\n            y is synthesized using y's empirical distribution. X is generated by the model.\n            In this case, y is completely independent of X.\n\n    Note: For now, n_classes has to be set to 0. This is because our matrix is the concatenation\n    of (X_num, X_cat). In this case, if we have is_y_cond == 'concat', we can guarantee that y\n    is the first column of the matrix.\n    However, if we have n_classes &gt; 0, then y is not the first column of the matrix.\n    \"\"\"\n    if ratios is None:\n        ratios = [0.7, 0.2, 0.1]\n\n    train_val_df, test_df = train_test_split(df, test_size=ratios[2], random_state=42)\n    train_df, val_df = train_test_split(train_val_df, test_size=ratios[1] / (ratios[0] + ratios[1]), random_state=42)\n\n    cat_column_orders = []\n    num_column_orders = []\n    index_to_column = list(df.columns)\n    column_to_index = {col: i for i, col in enumerate(index_to_column)}\n\n    if df_info[\"n_classes\"] &gt; 0:\n        X_cat: dict[str, np.ndarray] | None = {} if df_info[\"cat_cols\"] is not None or is_y_cond == \"concat\" else None\n        X_num: dict[str, np.ndarray] | None = {} if df_info[\"num_cols\"] is not None else None\n        y = {}\n\n        cat_cols_with_y = []\n        if df_info[\"cat_cols\"] is not None:\n            cat_cols_with_y += df_info[\"cat_cols\"]\n        if is_y_cond == \"concat\":\n            cat_cols_with_y = [df_info[\"y_col\"]] + cat_cols_with_y\n\n        if len(cat_cols_with_y) &gt; 0:\n            X_cat[\"train\"] = train_df[cat_cols_with_y].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"val\"] = val_df[cat_cols_with_y].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"test\"] = test_df[cat_cols_with_y].to_numpy(dtype=np.str_)  # type: ignore[index]\n\n        y[\"train\"] = train_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"val\"] = val_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"test\"] = test_df[df_info[\"y_col\"]].values.astype(np.float32)\n\n        if df_info[\"num_cols\"] is not None:\n            X_num[\"train\"] = train_df[df_info[\"num_cols\"]].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"val\"] = val_df[df_info[\"num_cols\"]].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"test\"] = test_df[df_info[\"num_cols\"]].values.astype(np.float32)  # type: ignore[index]\n\n        cat_column_orders = [column_to_index[col] for col in cat_cols_with_y]\n        num_column_orders = [column_to_index[col] for col in df_info[\"num_cols\"]]\n\n    else:\n        X_cat = {} if df_info[\"cat_cols\"] is not None else None\n        X_num = {} if df_info[\"num_cols\"] is not None or is_y_cond == \"concat\" else None\n        y = {}\n\n        num_cols_with_y = []\n        if df_info[\"num_cols\"] is not None:\n            num_cols_with_y += df_info[\"num_cols\"]\n        if is_y_cond == \"concat\":\n            num_cols_with_y = [df_info[\"y_col\"]] + num_cols_with_y\n\n        if len(num_cols_with_y) &gt; 0:\n            X_num[\"train\"] = train_df[num_cols_with_y].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"val\"] = val_df[num_cols_with_y].values.astype(np.float32)  # type: ignore[index]\n            X_num[\"test\"] = test_df[num_cols_with_y].values.astype(np.float32)  # type: ignore[index]\n\n        y[\"train\"] = train_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"val\"] = val_df[df_info[\"y_col\"]].values.astype(np.float32)\n        y[\"test\"] = test_df[df_info[\"y_col\"]].values.astype(np.float32)\n\n        if df_info[\"cat_cols\"] is not None:\n            X_cat[\"train\"] = train_df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"val\"] = val_df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)  # type: ignore[index]\n            X_cat[\"test\"] = test_df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)  # type: ignore[index]\n\n        cat_column_orders = [column_to_index[col] for col in df_info[\"cat_cols\"]]\n        num_column_orders = [column_to_index[col] for col in num_cols_with_y]\n\n    column_orders = num_column_orders + cat_column_orders\n    column_orders = [index_to_column[index] for index in column_orders]\n\n    label_encoders = {}\n    if X_cat is not None and len(df_info[\"cat_cols\"]) &gt; 0:\n        X_cat_all = np.vstack((X_cat[\"train\"], X_cat[\"val\"], X_cat[\"test\"]))\n        X_cat_converted = []\n        for col_index in range(X_cat_all.shape[1]):\n            label_encoder = LabelEncoder()\n            X_cat_converted.append(label_encoder.fit_transform(X_cat_all[:, col_index]).astype(float))\n            if std &gt; 0:\n                # add noise\n                X_cat_converted[-1] += np.random.normal(0, std, X_cat_converted[-1].shape)\n            label_encoders[col_index] = label_encoder\n\n        X_cat_converted = np.vstack(X_cat_converted).T  # type: ignore[assignment]\n\n        train_num = X_cat[\"train\"].shape[0]\n        val_num = X_cat[\"val\"].shape[0]\n        # test_num = X_cat[\"test\"].shape[0]\n\n        X_cat[\"train\"] = X_cat_converted[:train_num, :]  # type: ignore[call-overload]\n        X_cat[\"val\"] = X_cat_converted[train_num : train_num + val_num, :]  # type: ignore[call-overload]\n        X_cat[\"test\"] = X_cat_converted[train_num + val_num :, :]  # type: ignore[call-overload]\n\n        if X_num and len(X_num) &gt; 0:\n            X_num[\"train\"] = np.concatenate((X_num[\"train\"], X_cat[\"train\"]), axis=1)\n            X_num[\"val\"] = np.concatenate((X_num[\"val\"], X_cat[\"val\"]), axis=1)\n            X_num[\"test\"] = np.concatenate((X_num[\"test\"], X_cat[\"test\"]), axis=1)\n        else:\n            X_num = X_cat\n            X_cat = None\n\n    D = Dataset(\n        # ruff: noqa: N806\n        X_num,\n        None,\n        y,\n        y_info={},\n        task_type=TaskType(df_info[\"task_type\"]),\n        n_classes=df_info[\"n_classes\"],\n    )\n\n    return transform_dataset(D, T, None), label_encoders, column_orders\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.update_ema","title":"update_ema","text":"<pre><code>update_ema(target_params, source_params, rate=0.999)\n</code></pre> <p>Update target parameters to be closer to those of source parameters using an exponential moving average. :param target_params: the target parameter sequence. :param source_params: the source parameter sequence. :param rate: the EMA rate (closer to 1 means slower).</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def update_ema(\n    target_params: Iterator[nn.Parameter],\n    source_params: Iterator[nn.Parameter],\n    rate: float = 0.999,\n) -&gt; None:\n    \"\"\"\n    Update target parameters to be closer to those of source parameters using\n    an exponential moving average.\n    :param target_params: the target parameter sequence.\n    :param source_params: the source parameter sequence.\n    :param rate: the EMA rate (closer to 1 means slower).\n    \"\"\"\n    for targ, src in zip(target_params, source_params):\n        targ.detach().mul_(rate).add_(src.detach(), alpha=1 - rate)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.create_named_schedule_sampler","title":"create_named_schedule_sampler","text":"<pre><code>create_named_schedule_sampler(name, diffusion)\n</code></pre> <p>Create a ScheduleSampler from a library of pre-defined samplers.</p> <p>:param name: the name of the sampler. :param diffusion: the diffusion object to sample for.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def create_named_schedule_sampler(name: str, diffusion: GaussianMultinomialDiffusion) -&gt; ScheduleSampler:\n    \"\"\"\n    Create a ScheduleSampler from a library of pre-defined samplers.\n\n    :param name: the name of the sampler.\n    :param diffusion: the diffusion object to sample for.\n    \"\"\"\n    if name == \"uniform\":\n        return UniformSampler(diffusion)\n    if name == \"loss-second-moment\":\n        return LossSecondMomentResampler(diffusion)\n    raise NotImplementedError(f\"unknown schedule sampler: {name}\")\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.timestep_embedding","title":"timestep_embedding","text":"<pre><code>timestep_embedding(timesteps, dim, max_period=10000)\n</code></pre> <p>Create sinusoidal timestep embeddings.</p> <p>:param timesteps: a 1-D Tensor of N indices, one per batch element.                   These may be fractional. :param dim: the dimension of the output. :param max_period: controls the minimum frequency of the embeddings. :return: an [N x dim] Tensor of positional embeddings.</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def timestep_embedding(timesteps: Tensor, dim: int, max_period: int = 10000) -&gt; Tensor:\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2\n    freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(\n        device=timesteps.device\n    )\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.reglu","title":"reglu","text":"<pre><code>reglu(x)\n</code></pre> <p>The ReGLU activation function from [1].</p> References <p>[1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def reglu(x: Tensor) -&gt; Tensor:\n    \"\"\"The ReGLU activation function from [1].\n\n    References:\n        [1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n    assert x.shape[-1] % 2 == 0\n    a, b = x.chunk(2, dim=-1)\n    return a * F.relu(b)\n</code></pre>"},{"location":"api/#midst_toolkit.models.clavaddpm.model.geglu","title":"geglu","text":"<pre><code>geglu(x)\n</code></pre> <p>The GEGLU activation function from [1].</p> References <p>[1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020</p> Source code in <code>src/midst_toolkit/models/clavaddpm/model.py</code> <pre><code>def geglu(x: Tensor) -&gt; Tensor:\n    \"\"\"The GEGLU activation function from [1].\n\n    References:\n        [1] Noam Shazeer, \"GLU Variants Improve Transformer\", 2020\n    \"\"\"\n    assert x.shape[-1] % 2 == 0\n    a, b = x.chunk(2, dim=-1)\n    return a * F.gelu(b)\n</code></pre>"},{"location":"user_guide/","title":"User Guide","text":""},{"location":"user_guide/#pyprojecttoml-file-and-dependency-management","title":"pyproject.toml file and dependency management","text":"<p>If your project doesn't have a pyproject.toml file, simply copy the one from the template and update file according to your project.</p> <p>For managing dependencies, this template makes use of uv, which according to some benchmarks is faster than alternative like Poetry (which our original AI Engineering Template makes use of).</p> <p>Hence, be sure to install uv in order to to setup the development virtual environment. Instructions for installing uv can be found here. Note that uv supports optional dependency groups which helps to manage dependencies for different parts of development such as <code>documentation</code>, <code>testing</code>, etc. The core dependencies are installed using the command:</p> <pre><code>uv sync\n</code></pre> <p>Additional dependency groups can be installed using the <code>--group</code> flag followed by the group name. For example:</p> <pre><code>uv sync --all-extras --group docs --group test\n</code></pre> <p>mypy configuration options</p> <p>By default, the <code>mypy</code> configuration in the <code>pyproject.toml</code> disallows subclassing the <code>Any</code> type - <code>allow_subclassing_any = false</code>. In cases where the type checker is not able to determine the types of objects in some external library (e.g. <code>PyTorch</code>), it will treat them as <code>Any</code> and raise errors. If your codebase has many of such cases, you can set <code>allow_subclassing_any = true</code> in the <code>mypy</code> configuration or remove it entirely to use the default value (which is <code>true</code>). For example, in a <code>PyTorch</code> project, subclassing <code>nn.Module</code> will raise errors if <code>allow_subclassing_any</code> is set to <code>false</code>.</p>"},{"location":"user_guide/#pre-commit","title":"pre-commit","text":"<p>You can use pre-commit to run pre-commit hooks (code checks, liniting, etc.) when you run <code>git commit</code> and commit your code. Simply copy the <code>.pre-commit-config.yaml</code> file to the root of the repository and install the test dependencies which installs pre-commit. Then run:</p> <pre><code>pre-commit install\n</code></pre> <p>If you prefer to not enforce using pre-commit every time you run <code>git commit</code>, you will have to run <code>pre-commit run --all-files</code> from the command line before you commit your code.</p> <p>hook configuration</p> <p>Some of the pre-commit hooks use supported hooks from the web.</p> <p>For some others, they are locally installed and hence use the python virtual environment locally. If <code>language</code> is set to <code>python</code>, each time the hook is installed, a separate python virtual environment is created and you can specify dependencies needed using <code>additional_dependencies</code>.</p> <p>If <code>language</code> is set to <code>system</code>, the activated python virtual environment is used and and hence you have to ensure that the required dependencies and their versions are correctly installed.</p> <pre><code>  - repo: local\n    hooks:\n    - id: pytest\n      name: pytest\n      entry: python3 -m pytest -m \"not integration_test\"\n      language: python/system # set according to your project needs\n</code></pre> <p>typos</p> <p>The typos pre-commit hook is used to check for common spelling mistakes in the codebase. While useful, it may require some configuration to ignore certain words or phrases that are not typos. You can configure the typos hook in the <code>pyproject.toml</code> file. In a large codebase, it may be useful to disable the typos hook and only run it occasionally on the entire codebase.</p>"},{"location":"user_guide/#pre-commit-ci","title":"pre-commit ci","text":"<p>Instead of fixing pre-commit errors manually, a CI to fix them as well as update pre-commit hooks periodically can be enabled for your repository. Please check pre-commit.ci and add your repository. The configuration for <code>pre-commit.ci</code> can be added to the <code>.pre-commit-config.yaml</code> file.</p>"},{"location":"user_guide/#documentation","title":"documentation","text":"<p>If your project doesn't have documentation, copy the directory named <code>docs</code> to the root directory of your repository. This template uses MkDocs with the Material for MkDocs theme.</p> <p>In order to build the documentation, install the documentation dependencies as mentioned in the previous section, then run the command:</p> <pre><code>mkdocs build\n</code></pre> <p>If you're making changes to the docs, and want to serve them locally on your machine, then you can use this command instead:</p> <pre><code>mkdocs serve\n</code></pre> <p>The above will launch the docs locally on <code>http://127.0.0.1:8000</code>, which you can enter into your browser of choice. Conveniently, this process also watches for any changes you make to the docs and will update them as they occur.</p> <p>You can configure the documentation by updating the <code>mkdocs.yml</code> file at the root of your repository. The markdown files in the <code>docs</code> directory can be updated to reflect the project's documentation.</p>"},{"location":"user_guide/#github-actions","title":"github actions","text":"<p>The template consists of some github action continuous integration workflows that you can add to your repository.</p> <p>The available workflows are:</p> <ul> <li>code checks: Static code analysis, code formatting and unit tests</li> <li>documentation: Project documentation including example API reference</li> <li>integration tests: Integration tests</li> <li>publish: Publishing python package to PyPI. Create a <code>PYPI_API_TOKEN</code> and add it to the repository's actions secret variables in order to publish PyPI packages when new software releases are created on Github.</li> </ul>"}]}